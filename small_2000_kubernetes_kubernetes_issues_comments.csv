	issue	user_login	created_at	author_association	comment
0	2	jbeda	2014-06-06 23:58:07	CONTRIBUTOR	"@brendandburns original filed.
"
1	1	jbeda	2014-06-06 23:58:24	CONTRIBUTOR	"@brendandburns originally filed.
"
2	2	brendandburns	2014-06-07 06:43:24	CONTRIBUTOR	"Added a check for go being installed.
"
3	9	proppy	2014-06-07 09:25:19	CONTRIBUTOR	"Yes, currently it's always:

```
            ""currentState"": {
                ""manifest"": {
                    ""version"": """",
                    ""volumes"": null,
                    ""containers"": null
                },
                ""host"": ""kubernetes-minion-...""
            }
```
"
4	17	brendandburns	2014-06-08 05:04:22	CONTRIBUTOR	"build/test/integration-test/e2e-test all pass.
"
5	19	proppy	2014-06-08 06:04:40	CONTRIBUTOR	"On Sat, Jun 7, 2014 at 10:35 PM, Joe Beda notifications@github.com wrote:

> Right now we use salt to distribute and start most of the server
> components for the master. As we support build and deployment from a local
> Mac, we don't pre-compile the go scripts but instead ship the source code
> to the nodes (with salt) and compile at install time.
> 
> Instead, we should do the following:
> - Only support building on a linux machine with docker installed.
>   Perhaps support local development on a mac with a local linux VM
> 
> We could link to docker instruction about boot2docker:
> http://docs.docker.io/installation/mac/
> - Package each server component up as a Docker image, built with a
>   Dockerfile
>   - We should support uploading these Docker images to either the
>     public index or a GCS backed index with google/docker-registry
>     https://index.docker.io/u/google/docker-registry/.
>     - Use the kubelet to run/health check the components. This means the
>       kubelet will manage a set of static tasks on each machine (including the
>       master) and a set of dynamic tasks.
> - The only task that shouldn't run under the docker should be the
>   kubelet itself. We may have to hack in something for (network mode = host)
>   for the proxy.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes-new/issues/19.

## 

Johan Euphrosine (proppy)
Developer Programs Engineer
Google Developer Relations
"
6	8	jbeda	2014-06-08 15:05:39	CONTRIBUTOR	"@brendandburns put in a 2 min wait here and that may be good enough for now.  See: #18.  But we should really do something more robust so leaving this bug open.
"
7	26	jbeda	2014-06-09 04:23:16	CONTRIBUTOR	"Looks reasonable to me. My guess is that unit test coverage here gets lower :/
"
8	30	brendandburns	2014-06-09 06:01:13	CONTRIBUTOR	"build/unit test/integration test/e2e tests pass.
"
9	43	lavalamp	2014-06-10 19:03:25	MEMBER	"Thank you for this fix! Can you please sign a CLA so we can accept it? Instructions are in CONTRIB.md. Thanks!
"
10	45	brendandburns	2014-06-10 19:54:22	CONTRIBUTOR	"Thank you for this fix! Can you please sign a CLA so we can accept it? Instructions are in CONTRIB.md. Thanks!
"
11	45	whiteinge	2014-06-10 19:56:32	CONTRIBUTOR	"Ah, I missed that. Done and done.
"
12	45	lavalamp	2014-06-10 20:01:01	MEMBER	"Thanks! I see your CLA in the list now.
"
13	48	lavalamp	2014-06-10 21:09:02	MEMBER	"Thank you for this fix! Can you please sign a CLA so we can accept it? Instructions are in CONTRIB.md. Thanks!
"
14	50	brendandburns	2014-06-10 22:04:32	CONTRIBUTOR	"The integration test binary is pretty close.  It'd be pretty simple to glue
in the kubelet to that binary.

Brendan
On Jun 10, 2014 2:53 PM, ""Daniel Smith"" notifications@github.com wrote:

> It would be great to have an easy way to run a kubernetes master/minion
> locally. Currently, it's not that easy to set up. We should have a
> convenient script to make it work.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/50.
"
15	49	lavalamp	2014-06-10 22:07:18	MEMBER	"Thanks for the change!
"
16	48	hmrm	2014-06-10 23:28:42	CONTRIBUTOR	"Done!
"
17	53	asm89	2014-06-11 01:53:40	CONTRIBUTOR	"I signed the CLA with the same mail address used in the commit. I guess that works?
"
18	52	proppy	2014-06-11 01:55:28	CONTRIBUTOR	"Thanks for the fix, can you sign the CLA as described in https://github.com/GoogleCloudPlatform/kubernetes/blob/master/CONTRIB.md ?
"
19	44	proppy	2014-06-11 01:56:03	CONTRIBUTOR	"Thanks for the fix, can you sign the CLA as described in https://github.com/GoogleCloudPlatform/kubernetes/blob/master/CONTRIB.md ?
"
20	53	lavalamp	2014-06-11 03:09:27	MEMBER	"Thanks for the fix!
"
21	48	lavalamp	2014-06-11 03:10:46	MEMBER	"Thanks!
"
22	54	lavalamp	2014-06-11 03:13:47	MEMBER	"Thank you for this fix! Can you please sign a CLA so we can accept it? Instructions are in CONTRIB.md. Thanks!
"
23	55	jkassemi	2014-06-11 05:08:01	NONE	"Woot - n/m already corrected elsewhere. 
"
24	56	proppy	2014-06-11 05:39:01	CONTRIBUTOR	"Looks like a duplicate of #52 
"
25	56	inthecloud247	2014-06-11 05:41:59	NONE	"lol gotta sign the CLA to merge the change? 
"
26	52	inthecloud247	2014-06-11 05:42:46	NONE	"sign and fax it in? :-)
"
27	56	proppy	2014-06-11 05:44:26	CONTRIBUTOR	"Yes, whoever sign first: gets merged :)
"
28	57	proppy	2014-06-11 05:46:42	CONTRIBUTOR	"/cc @bgrant0607 
"
29	56	inthecloud247	2014-06-11 05:50:56	NONE	"Will this suffice?

June 10, 2014
I hereby grant Google Inc. a perpetual worldwide license for my proprietary, patented implementation of the word update. 

Sincerely,
-John Albietz
"
30	56	proppy	2014-06-11 05:52:56	CONTRIBUTOR	"@inthecloud247 unfortunately no, our lawyers insist on every contributions (even small documentation update) to go through:
https://github.com/GoogleCloudPlatform/kubernetes/blob/master/CONTRIB.md
"
31	52	adnanh	2014-06-11 05:56:43	CONTRIBUTOR	"Signed it electronically, says it will be processed shortly?
"
32	56	inthecloud247	2014-06-11 06:01:39	NONE	"so that probably also means that you can't go and fix it yourself now, since that would possibly mean that you're incorporating proprietary code into your project. a pickle.
"
33	56	adnanh	2014-06-11 06:04:28	CONTRIBUTOR	"Lol, what a mess we've made.
"
34	52	proppy	2014-06-11 06:04:49	CONTRIBUTOR	"@adnanh yes, it's already processed, now we just need a maintainer to press the merge button :)
"
35	52	adnanh	2014-06-11 06:05:12	CONTRIBUTOR	"Excellent. :-)
"
36	56	inthecloud247	2014-06-11 06:05:17	NONE	"how about if I branch out upstream, re-implement (in a clean room), then patch back in?
"
37	56	proppy	2014-06-11 06:06:20	CONTRIBUTOR	"@inthecloud247 you'd have to reverse engineer the typo first.
"
38	19	monnand	2014-06-11 06:07:14	CONTRIBUTOR	"I have some questions just out of curious:
- Does it mean that salt will not be used in the future?
- About kubenetes docker containers, does it only contain compiled binary files, or contain source code? More precisely, are you going to put build process into the Dockerfile?

Thank you!
"
39	57	usarid	2014-06-11 06:21:52	CONTRIBUTOR	"I also accepted (electronically) the CLA. ""Thank you.  Your CLA submission will be processed shortly.""
"
40	54	klrkdekira	2014-06-11 06:31:16	NONE	"Hi, I've signed it electronically :)
"
41	60	travisby	2014-06-11 12:23:22	CONTRIBUTOR	"Hello,

I took a look at CONTRIB.MD, and have signed the CLA.  I noticed that normally an issue should be filed before a pull request, but I had a feeling something like this would be just as easy to commit :P
"
42	60	brendandburns	2014-06-11 14:03:03	CONTRIBUTOR	"Thanks for the fix!
"
43	52	jbeda	2014-06-11 16:19:38	CONTRIBUTOR	"Thanks for the fix.  Merging.
"
44	61	proppy	2014-06-11 16:21:57	CONTRIBUTOR	"Looks like a duplicate of #52
"
45	54	proppy	2014-06-11 16:22:56	CONTRIBUTOR	"Looks like a duplicate of #52
"
46	61	stvnrhodes	2014-06-11 16:23:28	NONE	"Okay, discarding
"
47	54	jbeda	2014-06-11 16:23:41	CONTRIBUTOR	"Thanks for the fix.  Already merged another patch with to fix my typo.  Please keep playing with Kubernetes and let us know what you think.
"
48	56	jbeda	2014-06-11 16:25:06	CONTRIBUTOR	"Moot point now as another PR has been merged to fix my stupid typo.  Thanks for helping out though.

Keep playing with Kubernetes and let us know what you think.
"
49	61	jbeda	2014-06-11 16:25:41	CONTRIBUTOR	"Thanks for helping out though!  Please keep playing with Kubernetes and let us know what you think.
"
50	19	jbeda	2014-06-11 17:24:17	CONTRIBUTOR	"@monnand I imagine that we will continue to use salt to bootstrap stuff.  But we'll be able to reduce some of the more complex salt config.

For example, we currently ship and compile the source everywhere where it is run.  If we start building docker images, we can precompile the binaries before they are run.

I'm thinking that we'll follow the example of Docker itself and do the build process in docker containers.  This, with boot2docker, could lead to a good dev flow for Mac OS X.
"
51	62	lavalamp	2014-06-11 18:42:35	MEMBER	"@brendandburns, any reason not to add source for those directly to the guestbook directory?
"
52	50	lavalamp	2014-06-11 18:55:26	MEMBER	"I'll take a look.
"
53	57	bgrant0607	2014-06-11 19:07:03	MEMBER	"LGTM. Thanks
"
54	62	brendandburns	2014-06-11 19:37:41	CONTRIBUTOR	"See:

https://github.com/GoogleCloudPlatform/kubernetes/pull/63
"
55	19	monnand	2014-06-11 20:48:12	CONTRIBUTOR	"@jbeda Thank you!

You also mentioned that kubelet should not run under the docker. Is it for technical reason, or other? I do not see any technical difficulties to run kubelet in docker. Or did I miss something?
"
56	19	jbeda	2014-06-11 22:01:13	CONTRIBUTOR	"We may be able to run the kubelet under docker, but most likely we'll want it to have a whole machine view and expanded privs.  Running it under a cgroup container is totally doable.  namespaces?  I'm not so sure if we can make that happen.

Another way of looking at this is that I think of the kubelet as operating at the same level as Docker itself (and perhaps merging with Docker at some point?) and so it should run outside of Docker.
"
57	65	jbeda	2014-06-11 22:18:54	CONTRIBUTOR	"Thanks for reading the fine print @inthecloud247.  Let me clear this with our lawyers and figure out the what they think the right thing to do here is.
"
58	65	inthecloud247	2014-06-11 22:37:09	NONE	"@bcantrill from @Joyent just wrote a great post discussing node.js removing
their CLA requirement:
http://dtrace.org/blogs/bmc/2014/06/11/broadening-nodejs/

http://www.infoworld.com/t/javascript/joyent-makes-it-easier-contribute-code-nodejs-244152

I'm heavily involved in both Saltstack and Docker, and I definitely want to get involved in your project here, but the CLA definitely has a chilling effect.

If you need to stick with the CLA route, why not use the standard Apache CLA: http://www.apache.org/licenses/icla.txt ?
"
59	50	drewcsillag	2014-06-11 22:50:36	CONTRIBUTOR	"+1 for this -- especially in script form rather than a go executable.
"
60	19	monnand	2014-06-11 23:03:35	CONTRIBUTOR	"@jbeda Correct me if I'm wrong. (I'm not saying that kubelet should run inside a docker container. I'm just trying to see what are the technical difficulties here.)

As far as I know, kubelet only needs to communicate with docker through docker remote api, which is either trhough a unix socket or a remote IP/port pair. Does it need to read/write cgroup's filesystem? In either case, it seems that we could mount the /var/run onto the container and run kubelet inside that containers.

We are currently doing this in [cadvisor](http://github.com/google/cadvisor), which runs inside a docker container but can communicate with docker daemon and read information from the cgroup filesystem. The container could still run inside its own namespace, but communicate with docker daemon through the mounted volume. We use the following command to run cadvisor inside a docker container:

```
sudo docker run \
  --volume=/var/run:/var/run:rw \
  --volume=/sys/fs/cgroup/:/sys/fs/cgroup:ro \
  --volume=/var/lib/docker/:/var/lib/docker:ro \
  --publish=8080:8080 \
  --detach=true \
  google/cadvisor
```
"
61	64	jbeda	2014-06-11 23:12:40	CONTRIBUTOR	"Yeah -- I'd love to see this happen.  Metadata is from API->VM, not the other way around though.  GCS would be good but we'd have to give GCS write permissions to the master node -- something we don't do right now.

Another option would be to generate the cert on the client as the cluster is launched but that'd mean you'd have to have openssl installed.  Looks like it is pre-installed on OSX so perhaps this is doable.  We'd probably put the private cert in the metadata for the master.  Or we could ssh in and push it after the cluster is booted.  Not as big a fan of that.
"
62	68	jbeda	2014-06-11 23:26:08	CONTRIBUTOR	"Good catch!

Can we perhaps move this into the e2e-test script instead?  I don't want to open up more ports than necessary for the regular non-test bring up.

Also, we'll need you to sign our ""Contributor Licensing Agreement"" to accept this.  Lawyers.  See CONTRIB.md.
"
63	19	jbeda	2014-06-11 23:33:06	CONTRIBUTOR	"@monnand Nice!  We should try and make that work.  

One thing I worry about is things like driving iptables rules.  To solve #15 we'll have to be able to either muck with iptable rules or get a new networking mode into Docker proper.
"
64	65	jbeda	2014-06-11 23:40:53	CONTRIBUTOR	"@inthecloud247 I'll point this stuff out to the Google Open Source team.  I totally hear what you are saying.

As for modifying the LICENSE file, our lawyer got back to us on this:

> The apache license is just giving an example.  The license itself
> should be left unmodified. Modifying the example text in it will simply cause our LICENSE file to
> not be recognized as Apache by a large number of license file identifiers out there. There is no benefit or reason to do this.

Based on advice of counsel I'm going to pass on this PR.  Thanks for the interest though.  I'll follow up on the CLA.
"
65	19	brendanburns	2014-06-11 23:49:19	CONTRIBUTOR	"There was just a discussion of this in the plumbers meeting.  The union bay
networks folks want the ability to muck with the network from a container
too.

Brendan
On Jun 11, 2014 4:33 PM, ""Joe Beda"" notifications@github.com wrote:

> @monnand https://github.com/monnand Nice! We should try and make that
> work.
> 
> One thing I worry about is things like driving iptables rules. To solve
> #15 https://github.com/GoogleCloudPlatform/kubernetes/issues/15 we'll
> have to be able to either muck with iptable rules or get a new networking
> mode into Docker proper.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/19#issuecomment-45813777
> .
"
66	64	brendanburns	2014-06-11 23:51:11	CONTRIBUTOR	"We already generate the htpasswd file locally, so we could just follow that
pattern.

Brendan
On Jun 11, 2014 4:12 PM, ""Joe Beda"" notifications@github.com wrote:

> Yeah -- I'd love to see this happen. Metadata is from API->VM, not the
> other way around though. GCS would be good but we'd have to give GCS write
> permissions to the master node -- something we don't do right now.
> 
> Another option would be to generate the cert on the client as the cluster
> is launched but that'd mean you'd have to have openssl installed. Looks
> like it is pre-installed on OSX so perhaps this is doable. We'd probably
> put the private cert in the metadata for the master. Or we could ssh in and
> push it after the cluster is booted. Not as big a fan of that.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/64#issuecomment-45812294
> .
"
67	64	jbeda	2014-06-11 23:57:54	CONTRIBUTOR	"@brendanburns Generating the cert locally and putting it in metadata would work, but we'd start putting sensitive key material in metadata.  The htpasswd stuff is hashed and so it isn't sensitive.  The metadata for the instance is readable by anyone with API read access after launch.  Perhaps this is okay...
"
68	64	mmdriley	2014-06-12 00:04:11	CONTRIBUTOR	"As Joe says, htpasswd is less sensitive than the server's SSL private key. Having it available to all project readers still isn't great, but at least it's a salted hash of a strong password.

Metadata would be secure enough to ship around the certificate fingerprint or public key, but not the private key.
"
69	64	jbeda	2014-06-12 00:06:00	CONTRIBUTOR	"That being said, it is strictly better than just trusting any ol key.
"
70	68	danielnorberg	2014-06-12 00:10:01	CONTRIBUTOR	"Sure, i'll move it into e2e-test.sh.

I've submitted a CLA electronically. 
"
71	68	brendandburns	2014-06-12 00:47:44	CONTRIBUTOR	"Merged, thanks!

--brendan
"
72	69	brendandburns	2014-06-12 01:01:36	CONTRIBUTOR	"build/unit test/integration test/e2e test/ all pass.
"
73	65	inthecloud247	2014-06-12 01:13:03	NONE	"uh... so from the apache license docs, that really seems incorrect. :-)
check out the way the docker team did it:
https://github.com/dotcloud/docker/blob/master/LICENSE

_http://httpd.apache.org/docs/2.0/license.html
http://httpd.apache.org/docs/2.0/license.html_

To apply the Apache License to your work, attach the following boilerplate
notice, with the fields enclosed by brackets ""[]"" replaced with your own
identifying information. _(Don't include the brackets!)_

John Albietz
m: 516-592-2372
e: inthecloud247@gmail.com
l'in: linkedin.com/in/ydavid

On Wed, Jun 11, 2014 at 4:55 PM, Joe Beda notifications@github.com wrote:

> Closed #65 https://github.com/GoogleCloudPlatform/kubernetes/pull/65.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/65#event-130507323
> .
"
74	65	brendandburns	2014-06-12 01:40:50	CONTRIBUTOR	"If you look at the individual source files that's what we did.  As I
understand the advice from counsel, they believe the license file should be
included verbatim.

Brendan
On Jun 11, 2014 6:13 PM, ""John Albietz"" notifications@github.com wrote:

> uh... so from the apache license docs, that really seems incorrect. :-)
> check out the way the docker team did it:
> https://github.com/dotcloud/docker/blob/master/LICENSE
> 
> _http://httpd.apache.org/docs/2.0/license.html
> http://httpd.apache.org/docs/2.0/license.html_
> 
> To apply the Apache License to your work, attach the following boilerplate
> notice, with the fields enclosed by brackets ""[]"" replaced with your own
> identifying information. _(Don't include the brackets!)_
> 
> John Albietz
> m: 516-592-2372
> e: inthecloud247@gmail.com
> l'in: linkedin.com/in/ydavid
> 
> On Wed, Jun 11, 2014 at 4:55 PM, Joe Beda notifications@github.com
> wrote:
> 
> > Closed #65 https://github.com/GoogleCloudPlatform/kubernetes/pull/65.
> > 
> > —
> > Reply to this email directly or view it on GitHub
> > <
> > https://github.com/GoogleCloudPlatform/kubernetes/pull/65#event-130507323>
> > 
> > .
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/65#issuecomment-45819700
> .
"
75	65	dberlin	2014-06-12 02:52:56	NONE	"As Brendan says, what the license says to do is actually quite clear (and you are welcome to ask the ASF, they will tell you the same :P):

""To apply the Apache License to your work, attach the following boilerplate notice, with the fields enclosed by brackets ""[]"" replaced with your own identifying information. (Don't include the brackets!) The text should be enclosed in the appropriate comment syntax for the file format. ""

IE The license says to attach the notice to the source file, replacing the [] with your own info,  _not_ modify the Apache LICENSE.

You can see what the ASF has done for years, which is exactly to leave the _LICENSE_ alone, and modify the source files, here:

http://svn.apache.org/repos/asf/httpd/httpd/trunk/LICENSE (note the LICENSE is included verbatim).
http://svn.apache.org/repos/asf/hive/trunk/LICENSE
http://svn.apache.org/repos/asf/cassandra/trunk/LICENSE.txt
etc
"
76	65	dberlin	2014-06-12 02:57:17	NONE	"@inthecloud247 Google's CLA is identical to Apache's CLA already. There used to be differences in the corporate versions, but even those don't exist any longer.

The only wording differences between the two should be the textual replacement of ""The Foundation"" with ""Google"".
"
77	72	lavalamp	2014-06-12 05:28:58	MEMBER	"PTAL
"
78	72	brendandburns	2014-06-12 05:36:21	CONTRIBUTOR	"LGTM, but looks like it needs a rebase before I can merge it cleanly.
"
79	72	lavalamp	2014-06-12 16:35:57	MEMBER	"Rebased.
"
80	76	brendandburns	2014-06-12 18:21:46	CONTRIBUTOR	"Thanks for the PR.  Have you signed our CLA?  Instructions are in CONTRIB.md.

(also, this needs to be rebased)

Thanks again!
--brendan
"
81	73	jbeda	2014-06-12 18:38:12	CONTRIBUTOR	"Thanks for this PR.  The new flags are going to land at some point, but the version isn't known yet.  I have another PR to fix up with the known good version but I'm going to switch back to our non-prerelease tool (`gcutil`) in a future PR.

I'm accepting this in spirit but will push the right fix in quickly to stop breakage.  Thanks!
"
82	73	proppy	2014-06-12 18:41:20	CONTRIBUTOR	"FYI, the PR @jbeda is referring to is #78 
"
83	62	brendandburns	2014-06-12 18:41:45	CONTRIBUTOR	"This has been submitted.
"
84	6	brendandburns	2014-06-12 18:42:16	CONTRIBUTOR	"This is now fixed.
"
85	19	vmarmol	2014-06-12 19:50:43	CONTRIBUTOR	"Today you should be able to get the host's network. +1 to @brendanburns's comment.
"
86	19	proppy	2014-06-12 19:54:14	CONTRIBUTOR	"yes `--net host` should do the trick.

Another interesting thing to do is `-v /var/run/docker.sock:/var/run/docker.sock` to access the docker daemon from the container. (or just having the docker daemon listen on localhost w/ `--net host`)
"
87	79	proppy	2014-06-12 20:02:41	CONTRIBUTOR	"Note that if you need custom marshaling for type you can implement the http://golang.org/pkg/encoding/json/#Marshaler interface
"
88	81	danielnorberg	2014-06-12 20:23:52	CONTRIBUTOR	":+1:
"
89	81	lavalamp	2014-06-12 20:24:57	MEMBER	"Whoops, thanks for catching!
"
90	82	brendandburns	2014-06-12 20:37:06	CONTRIBUTOR	"Thanks!
"
91	85	lavalamp	2014-06-12 21:14:49	MEMBER	"Thanks! But needs rebase/merge.
"
92	85	danielnorberg	2014-06-12 21:15:52	CONTRIBUTOR	"This has already been fixed: https://github.com/GoogleCloudPlatform/kubernetes/commit/75957dc5b9cea2a0fa5170851051cbf328aa21cb
"
93	85	proppy	2014-06-12 21:17:12	CONTRIBUTOR	"Yes, duplicate of #82
"
94	84	proppy	2014-06-12 21:17:48	CONTRIBUTOR	"rebased
"
95	85	lavalamp	2014-06-12 21:20:38	MEMBER	"Explains why it wasn't happening to me :)
"
96	85	proppy	2014-06-12 21:23:21	CONTRIBUTOR	"@lavalamp your go tool chain is drunk
"
97	83	lavalamp	2014-06-12 21:29:49	MEMBER	"Looks like a few additional changes are needed before travis will build?
"
98	79	lavalamp	2014-06-12 22:29:47	MEMBER	"I've fixed the json parsing. Still working on yaml parsing.
"
99	79	lavalamp	2014-06-12 23:54:46	MEMBER	"OK. This works and is ready to be merged.
"
100	76	gleamglom	2014-06-13 01:10:32	NONE	"I've signed the CLA and rebased!
"
101	67	lavalamp	2014-06-13 02:59:44	MEMBER	"I think #79 fixes this. We now support yaml input and catch syntax errors.
"
102	50	lavalamp	2014-06-13 03:02:50	MEMBER	"@drewcsillag, what exactly do you mean by ""script form""? Were you thinking of something like a version of cloudcfg that can be run without needing to have started up a daemon first? 
"
103	8	lavalamp	2014-06-13 03:04:46	MEMBER	"This seems like a good excuse to make cloudcfg list pods return more info about the status of containers. Then we could poll that instead of waiting 2 minutes.
"
104	76	lavalamp	2014-06-13 03:10:46	MEMBER	"This is a great PR, but it looks like we didn't merge fast enough and it needs another rebase.
"
105	86	lavalamp	2014-06-13 03:58:20	MEMBER	"Thanks for doing this!
"
106	76	jerome22	2014-06-13 08:33:40	NONE	"$ ./output/go/cloudcfg
usage: cloudcfg -h <host> [-c config/file.json] [-p <hostPort>:<containerPort>,..., <hostPort-n>:<containerPort-n>] <method> <methodArgs>

  Kubernetes REST API:
  cloudcfg [OPTIONS] get|list|create|delete|update <url>

  Run an image:
  cloudcfg [OPTIONS] run <image> <replicas> <name>

  Manage replication controllers:
  cloudcfg [OPTIONS] stop|rm|rollingupdate <controller>

  Options:
  -auth=""/Users/tlow/.kubernetes_auth"": Path to the auth info file.  If missing, prompt the user
  -c="""": Path to the config file.
  -h="""": The host to connect to.
  -json=false: If true, print raw JSON for responses
  -l="""": Label query to use for listing
  -p="""": The port spec, comma-separated list of <external>:<internal>,...
  -s=-1: If positive, create and run a corresponding service on this port, only used with 'run'
  -u=1m0s: Update interarrival period
  -v=false: Print the version number.
  -yaml=false: If true, print raw YAML for responses

$ ./output/go/cloudcfg rollingupdate
2014/06/12 11:13:18 usage: cloudcfg [OPTIONS] stop|rm|rollingupdate <controller>
$ ./output/go/cloudcfg get
2014/06/12 11:13:22 usage: cloudcfg [OPTIONS] get|list|create|update|delete <url>
$ ./output/go/cloudcfg run
2014/06/12 11:13:25 usage: cloudcfg [OPTIONS] run <image> <replicas> <name>
"
107	76	gleamglom	2014-06-13 17:35:49	NONE	"I find it useful that `./cloudcfg run` returns the expected number of arguments and what they mean. Otherwise, users have to go back to source to figure out that 3 arguments are required.

Updated messages:

```
$ ./output/go/cloudcfg
usage: cloudcfg -h [-c config/file.json] [-p :,..., :]

  Kubernetes REST API:
  cloudcfg [OPTIONS] get|list|create|delete|update <url>

  Manage replication controllers:
  cloudcfg [OPTIONS] stop|rm|rollingupdate <controller>
  cloudcfg [OPTIONS] run <image> <replicas> <controller>
  cloudcfg [OPTIONS] resize <controller> <replicas>

  Options:
  -auth=""~/.kubernetes_auth"": Path to the auth info file.  If missing, prompt the user.  Only used if doing https.
  -c="""": Path to the config file.
  -h="""": The host to connect to.
  -json=false: If true, print raw JSON for responses
  -l="""": Label query to use for listing
  -p="""": The port spec, comma-separated list of <external>:<internal>,...
  -s=-1: If positive, create and run a corresponding service on this port, only used with 'run'
  -u=1m0s: Update interarrival period
  -v=false: Print the version number.
  -yaml=false: If true, print raw YAML for responses
$ ./output/go/cloudcfg rollingupdate
2014/06/13 10:37:59 usage: cloudcfg [OPTIONS] stop|rm|rollingupdate <controller>
$ ./output/go/cloudcfg get
2014/06/13 10:38:02 usage: cloudcfg [OPTIONS] get|list|create|update|delete <url>
$ ./output/go/cloudcfg resize
2014/06/13 10:38:04 usage: cloudcfg resize <controller> <replicas>
$ ./output/go/cloudcfg run
2014/06/13 10:38:06 usage: cloudcfg [OPTIONS] run <image> <replicas> <controller>
```
"
108	50	drewcsillag	2014-06-13 18:31:45	CONTRIBUTOR	"@lavalamp Basically, what I had in mind, is a shell script that looks something like [this](https://gist.github.com/drewcsillag/1d7f51ec0c17f7237f86)
"
109	50	lavalamp	2014-06-13 18:54:35	MEMBER	"Ah, ok. That's very similar to local-up.sh; the only difference is that everything is packaged into a single go binary. We probably will switch to using individual binaries for this when we start running our components inside docker containers.
"
110	32	brendandburns	2014-06-13 18:55:13	CONTRIBUTOR	"This is now done in https://github.com/GoogleCloudPlatform/kubernetes/pull/80
"
111	76	brendandburns	2014-06-13 19:03:23	CONTRIBUTOR	"Sorry, what is your real name?  I need it to look it up in the CLA signers...

Thanks!
--brendan
"
112	50	drewcsillag	2014-06-13 19:05:06	CONTRIBUTOR	"The script just makes things more obvious how to run things for those who are looking to integrate them into non-GCE environments.
"
113	50	lavalamp	2014-06-13 19:09:37	MEMBER	"That's a good point. I'll look into this some more later.
"
114	98	lavalamp	2014-06-13 19:17:32	MEMBER	"LGTM
"
115	76	gleamglom	2014-06-13 20:19:11	NONE	"@brendandburns Tiffany L
"
116	4	brendandburns	2014-06-13 21:47:32	CONTRIBUTOR	"Handled by #98 for now.  We might re-open later.
"
117	76	brendandburns	2014-06-13 22:48:40	CONTRIBUTOR	"Merged.  Thanks for the contribution.
"
118	105	lavalamp	2014-06-14 00:38:48	MEMBER	"Hm, looks like the integration test flaked out in travis.
"
119	105	jbeda	2014-06-14 14:38:35	CONTRIBUTOR	"Other than the `--allowed` message this LGTM so feel free to merge.
"
120	19	jbeda	2014-06-14 15:48:42	CONTRIBUTOR	"Notes of work in progress:
- I'm starting out by moving our build process into Docker.  A snapshot of a Dockerfile and a Makefile to automate some common stuff is here: https://github.com/jbeda/kubernetes/commit/6c4a6a8fa7794874862cadaf31948bdb9235f51a
- If we say everyone has to build on Linux, this is much easier.  With the boot2docker on a Mac, you essentially have a remote machine that you are talking to through a local TCP pipe.  That means with any `-v local-path:container-path` the `local-path` is really local to the boot2docker VM and not the local workstation.

This leaves us with 2 choices:
1. Build in the boot2docker VM and copy results back out, either through stdout from the docker run or via `boot2docker ssh`
2. Build the final docker images inside the boot2docker VM inside of a docker container.  Yup, this means running docker in docker.  This is supported with `dind` but it gets complicated.  There is a repo (https://github.com/jpetazzo/dind) with support but...
   - There is no license in that repo.  Issue: https://github.com/jpetazzo/dind/issues/21
   - The Docker build process itself uses dind but has a forked script checked in to the Docker repo.  These have diverged and I worry about continued support for the separate dind repo.  Issue to harmonize these: https://github.com/jpetazzo/dind/issues/22

Right now I'm leaning toward copying stuff in and out (option 1).
"
121	108	jbeda	2014-06-14 16:00:05	CONTRIBUTOR	"Note that this relates to #19 as it would (a) make it less painful to copy stuff in and out of a boot2docker VM during build and (b) would make the resultant container images smaller as they'd all share a base with the single binary.
"
122	108	brendandburns	2014-06-14 16:25:20	CONTRIBUTOR	"SGTM.

Brendan

On Sat, Jun 14, 2014, 9:00 AM, Joe Beda notifications@github.com wrote:

> Note that this relates to #4
> https://github.com/GoogleCloudPlatform/kubernetes/issues/4 as it would
> (a) make it less painful to copy stuff in and out of a boot2docker VM
> during build and (b) would make the resultant container images smaller as
> they'd all share a base with the single binary.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/108#issuecomment-46091743
> .
"
123	103	jbeda	2014-06-14 16:32:08	CONTRIBUTOR	"I actually think that this is by design.  The contract for the replication controller is that it drives to a number of replicas.  If you remove the controller, the things that it controls shouldn't necessarily be removed.

I could see us having a mode for a clean delete.  On implication of this is the DELETE of the controller is now an async operation that returns an HTTP `202 accepted` with some way to track the process of the DELETE.
"
124	108	lavalamp	2014-06-14 16:39:21	MEMBER	"The localkube thing I made is most of the way there already.
On Jun 14, 2014 9:25 AM, ""brendandburns"" notifications@github.com wrote:

> SGTM.
> 
> Brendan
> 
> On Sat, Jun 14, 2014, 9:00 AM, Joe Beda notifications@github.com wrote:
> 
> > Note that this relates to #4
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/4 as it
> > would
> > (a) make it less painful to copy stuff in and out of a boot2docker VM
> > during build and (b) would make the resultant container images smaller
> > as
> > they'd all share a base with the single binary.
> > 
> > —
> > Reply to this email directly or view it on GitHub
> > <
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/108#issuecomment-46091743>
> > 
> > .
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/108#issuecomment-46092426
> .
"
125	105	lavalamp	2014-06-14 16:59:55	MEMBER	"Hm. I'm pretty sure I didn't change anything in this to cause the travis failure-- is this the only thing it's flaking for?
"
126	19	proppy	2014-06-14 17:00:57	CONTRIBUTOR	"You could have Dockerfile for individual binaries and have the resulting
container image launch it has its ENTRYPOINT.

That way you could leverage the fact sources are sent from your workstation
to the VM as the /build payload (context) over the remote API (no need to
copy to the host) and the docker image is your artefact.
On Jun 14, 2014 8:48 AM, ""Joe Beda"" notifications@github.com wrote:

> Notes of work in progress:
> - I'm starting out by moving our build process into Docker. A snapshot
>   of a Dockerfile and a Makefile to automate some common stuff is here:
>   jbeda@6c4a6a8
>   https://github.com/jbeda/kubernetes/commit/6c4a6a8fa7794874862cadaf31948bdb9235f51a
> - If we say everyone has to build on Linux, this is much easier. With
>   the boot2docker on a Mac, you essentially have a remote machine that you
>   are talking to through a local TCP pipe. That means with any -v
>   local-path:container-path the local-path is really local to the
>   boot2docker VM and not the local workstation.
> 
> This leaves us with 2 choices:
> 1. Build in the boot2docker VM and copy results back out, either
>    through stdout from the docker run or via boot2docker ssh
> 2. Build the final docker images inside the boot2docker VM inside of a
>    docker container. Yup, this means running docker in docker. This is
>    supported with dind but it gets complicated. There is a repo (
>    https://github.com/jpetazzo/dind) with support but...
>    - There is no license in that repo. Issue: jpetazzo/dind#21
>      https://github.com/jpetazzo/dind/issues/21
>    - The Docker build process itself uses dind but has a forked script
>      checked in to the Docker repo. These have diverged and I worry about
>      continued support for the separate dind repo. Issue to harmonize these:
>      jpetazzo/dind#22 https://github.com/jpetazzo/dind/issues/22
> 
> Right now I'm leaning toward copying stuff in and out (option 1).
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/19#issuecomment-46091452
> .
"
127	19	proppy	2014-06-14 17:11:39	CONTRIBUTOR	"Note of you just want to have a container to build the projects and get
binaries out.

You can also set the ENTRYPOINT to the build command.

docker build ; docker run # to build
docker cp # to get the file out of the container
On Jun 14, 2014 10:00 AM, ""Johan Euphrosine"" proppy@google.com wrote:

> You could have Dockerfile for individual binaries and have the resulting
> container image launch it has its ENTRYPOINT.
> 
> That way you could leverage the fact sources are sent from your
> workstation to the VM as the /build payload (context) over the remote API
> (no need to copy to the host) and the docker image is your artefact.
> On Jun 14, 2014 8:48 AM, ""Joe Beda"" notifications@github.com wrote:
> 
> > Notes of work in progress:
> > - I'm starting out by moving our build process into Docker. A
> >   snapshot of a Dockerfile and a Makefile to automate some common stuff is
> >   here: jbeda@6c4a6a8
> >   https://github.com/jbeda/kubernetes/commit/6c4a6a8fa7794874862cadaf31948bdb9235f51a
> > - If we say everyone has to build on Linux, this is much easier. With
> >   the boot2docker on a Mac, you essentially have a remote machine that you
> >   are talking to through a local TCP pipe. That means with any -v
> >   local-path:container-path the local-path is really local to the
> >   boot2docker VM and not the local workstation.
> > 
> > This leaves us with 2 choices:
> > 1. Build in the boot2docker VM and copy results back out, either
> >    through stdout from the docker run or via boot2docker ssh
> > 2. Build the final docker images inside the boot2docker VM inside of
> >    a docker container. Yup, this means running docker in docker. This is
> >    supported with dind but it gets complicated. There is a repo (
> >    https://github.com/jpetazzo/dind) with support but...
> >    - There is no license in that repo. Issue: jpetazzo/dind#21
> >      https://github.com/jpetazzo/dind/issues/21
> >    - The Docker build process itself uses dind but has a forked
> >      script checked in to the Docker repo. These have diverged and I worry about
> >      continued support for the separate dind repo. Issue to harmonize these:
> >      jpetazzo/dind#22 https://github.com/jpetazzo/dind/issues/22
> > 
> > Right now I'm leaning toward copying stuff in and out (option 1).
> > 
> > —
> > Reply to this email directly or view it on GitHub
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/19#issuecomment-46091452
> > .
"
128	108	proppy	2014-06-14 17:13:39	CONTRIBUTOR	"Consider using subcommand instead of flags for defining which component to
launch, ex:
kube agent -some-agent-flag
 On Jun 14, 2014 9:39 AM, ""Daniel Smith"" notifications@github.com wrote:

> The localkube thing I made is most of the way there already.
> On Jun 14, 2014 9:25 AM, ""brendandburns"" notifications@github.com
> wrote:
> 
> > SGTM.
> > 
> > Brendan
> > 
> > On Sat, Jun 14, 2014, 9:00 AM, Joe Beda notifications@github.com
> > wrote:
> > 
> > > Note that this relates to #4
> > > https://github.com/GoogleCloudPlatform/kubernetes/issues/4 as it
> > > would
> > > (a) make it less painful to copy stuff in and out of a boot2docker VM
> > > during build and (b) would make the resultant container images smaller
> > > as
> > > they'd all share a base with the single binary.
> > > 
> > > —
> > > Reply to this email directly or view it on GitHub
> > > <
> > 
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/108#issuecomment-46091743>
> > 
> > > .
> > 
> > —
> > Reply to this email directly or view it on GitHub
> > <
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/108#issuecomment-46092426>
> > 
> > .
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/108#issuecomment-46092804
> .
"
129	19	proppy	2014-06-14 17:25:48	CONTRIBUTOR	"You could also have a combination of the two.

Build and run kube on top of google/golang for development; for production
if the size of google/debian base bother you: rebase the binary on top of
busybox image.
On Jun 14, 2014 10:11 AM, ""Johan Euphrosine"" proppy@google.com wrote:

> Note of you just want to have a container to build the projects and get
> binaries out.
> 
> You can also set the ENTRYPOINT to the build command.
> 
> docker build ; docker run # to build
> docker cp # to get the file out of the container
> On Jun 14, 2014 10:00 AM, ""Johan Euphrosine"" proppy@google.com wrote:
> 
> > You could have Dockerfile for individual binaries and have the resulting
> > container image launch it has its ENTRYPOINT.
> > 
> > That way you could leverage the fact sources are sent from your
> > workstation to the VM as the /build payload (context) over the remote API
> > (no need to copy to the host) and the docker image is your artefact.
> > On Jun 14, 2014 8:48 AM, ""Joe Beda"" notifications@github.com wrote:
> > 
> > > Notes of work in progress:
> > > - I'm starting out by moving our build process into Docker. A
> > >   snapshot of a Dockerfile and a Makefile to automate some common stuff is
> > >   here: jbeda@6c4a6a8
> > >   https://github.com/jbeda/kubernetes/commit/6c4a6a8fa7794874862cadaf31948bdb9235f51a
> > > - If we say everyone has to build on Linux, this is much easier.
> > >   With the boot2docker on a Mac, you essentially have a remote machine that
> > >   you are talking to through a local TCP pipe. That means with any -v
> > >   local-path:container-path the local-path is really local to the
> > >   boot2docker VM and not the local workstation.
> > > 
> > > This leaves us with 2 choices:
> > > 1. Build in the boot2docker VM and copy results back out, either
> > >    through stdout from the docker run or via boot2docker ssh
> > > 2. Build the final docker images inside the boot2docker VM inside of
> > >    a docker container. Yup, this means running docker in docker. This is
> > >    supported with dind but it gets complicated. There is a repo (
> > >    https://github.com/jpetazzo/dind) with support but...
> > >    - There is no license in that repo. Issue: jpetazzo/dind#21
> > >      https://github.com/jpetazzo/dind/issues/21
> > >    - The Docker build process itself uses dind but has a forked
> > >      script checked in to the Docker repo. These have diverged and I worry about
> > >      continued support for the separate dind repo. Issue to harmonize these:
> > >      jpetazzo/dind#22 https://github.com/jpetazzo/dind/issues/22
> > > 
> > > Right now I'm leaning toward copying stuff in and out (option 1).
> > > 
> > > —
> > > Reply to this email directly or view it on GitHub
> > > https://github.com/GoogleCloudPlatform/kubernetes/issues/19#issuecomment-46091452
> > > .
"
130	108	delroth	2014-06-14 19:10:59	NONE	"Maybe also support busybox-style command selection via argv[0]? That way you can symlink/hardlink the main binary under different names.

http://www.busybox.net/FAQ.html#source
"
131	108	jbeda	2014-06-14 21:20:12	CONTRIBUTOR	"@delroth I love the busybox model but I'm not sure that we want to rely on it completely.  We will probably want to have the client tools run on Windows at some point and I'm not sure how that'd work.

In any case, everyone seems on board so I'll mark this as a ""feature request"" instead of a question.
"
132	105	jbeda	2014-06-14 21:22:55	CONTRIBUTOR	"@lavalamp I've seen this too.  Filed #109 to track it.
"
133	19	jbeda	2014-06-14 21:52:00	CONTRIBUTOR	"Thanks for the comments @proppy.

I want the resultant container image to be minimal.  I like the idea of layering it on the busybox image.

That means that the image used to build should be different than the image used at runtime.  Doing `docker cp` to copy things around is one thing I'm looking to avoid.  `dind` is one solution there.  Rebasing will require either `dind` or `docker cp`.  If we don't do this carefully we end up packaging up 60+MB every time we build the image.  That takes too long :)
"
134	103	lavalamp	2014-06-15 00:02:49	MEMBER	"I can see why this is desirable, but it's pretty hard to sell the current behavior where your deletion _timing_ determines the survival of the pods as a feature :)

Let me suggest that we say that after modifying (stopping) a replication controller, the replication controller is guaranteed one pass to make the requested changes. If we then also allowed deleting replication controllers w/out stopping them first, we'd get the semantics of:
- deleting a running replication controller leaves all of its pods running.
- deleting a stopped replication controller leaves none of its pods running.

Which seems more sensible than the race we have now.

We should just implement http async code for all http operations that take longer than some fixed amount of time. Go should make that pretty easy. We should also do api admission, user authorization, etc. Not sure that this stuff should be at the top of our list right now, though.
"
135	19	proppy	2014-06-15 01:47:44	CONTRIBUTOR	"FYI, I have a pending patch to docker that could provide an hacky
alternative.
https://github.com/dotcloud/docker/pull/5715

This would allow something like

```
docker build -t builder ; (docker run builder | docker build -t runner -)
```

On Jun 14, 2014 2:52 PM, ""Joe Beda"" notifications@github.com wrote:

> Thanks for the comments @proppy https://github.com/proppy.
> 
> I want the resultant container image to be minimal. I like the idea of
> layering it on the busybox image.
> 
> That means that the image used to build should be different than the image
> used at runtime. Doing docker cp to copy things around is one thing I'm
> looking to avoid. dind is one solution there. Rebasing will require
> either dind or docker cp. If we don't do this carefully we end up
> packaging up 60+MB every time we build the image. That takes too long :)
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/19#issuecomment-46100695
> .
"
136	113	brendandburns	2014-06-15 03:24:19	CONTRIBUTOR	"Just don't call it config...  ;)
"
137	115	brendandburns	2014-06-16 03:28:01	CONTRIBUTOR	"Thanks!
"
138	2	brendandburns	2014-06-16 03:36:33	CONTRIBUTOR	"Fixed in #111 and #112 

Closing.
"
139	9	brendandburns	2014-06-16 03:37:18	CONTRIBUTOR	"Closed by #98 
"
140	33	brendandburns	2014-06-16 03:40:16	CONTRIBUTOR	"Closed by #102 
"
141	116	brendanburns	2014-06-16 03:52:35	CONTRIBUTOR	"Addresses #114 
"
142	116	lavalamp	2014-06-16 04:11:19	MEMBER	"You may want to take a look at your .git/config; your change appears as though it's authored by ""real-cost""...
"
143	116	brendanburns	2014-06-16 04:15:58	CONTRIBUTOR	"yep.  Already fixed...  (took me a while to figure it out :P)

That's what I get for using my personal account.

On Sun, Jun 15, 2014 at 9:11 PM, Daniel Smith notifications@github.com
wrote:

> You may want to take a look at your .git/config; your change appears as
> though it's authored by ""real-cost""...
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/116#issuecomment-46139736
> .
"
144	117	brendanburns	2014-06-16 04:43:38	CONTRIBUTOR	"closes #96 
"
145	118	brendanburns	2014-06-16 05:11:31	CONTRIBUTOR	"LGTM, but I don't like the name.  Perhaps ""local-up-cluster.sh""?

THanks!
--brendan
"
146	118	lavalamp	2014-06-16 05:16:08	MEMBER	"Renamed.
"
147	123	jbeda	2014-06-16 16:13:18	CONTRIBUTOR	"Thanks for the PR, Ryan.  Unfortunately, I can't merge this unless you can sign our CLA as described in CONTRIB.md.  Even for a small change like this the lawyers are pretty strict.

Thanks!
"
148	122	brendandburns	2014-06-16 16:33:02	CONTRIBUTOR	"Hey Daz,
Thanks for the question.

To find the IP address, figure out what machine the pod is running on, using

```
cloudcfg.sh list /pods
```

Then either use gcutil or the [google cloud console](http://cloud.google.com/console) to determine the external IP address of that machine.

```
gcutil getinstance <instance-name>
```

Let me know if you have more questions, and I'll update our docs to include this.

Thanks!
--brendan
"
149	122	jbeda	2014-06-16 16:34:54	CONTRIBUTOR	"@DazWilkin -- thanks for trying it out.  

Right now mapping a service on a port to an externally addressable IP isn't super smooth.  Right now you need to figure out which minion the container is running on and then find the IP address for that minion -- probably through `gcutil listinstances`.  This is obviously not good enough.

Some things to make this smoother:
- An easier way to decode the true external IP from the API/tools.  It should be easy to figure out what you point your web browser at to get to a specific container.  I've just filed #124 to capture this.
- Easily map a set of containers to the GCP load balancer feature.  This would give a stable IP no matter how the containers move around. This is already captured in #87 
- Build/use a reverse proxy image (nginx? HAProxy?) that is configured via etcd with an API for programming that reverse proxy. That way you could have a single IP:80 for your cluster and direct traffic to the appropriate container/app based on the HTTP host or the request path. 
"
150	123	ryanwalters	2014-06-16 16:39:37	CONTRIBUTOR	"Done! Submitted the form electronically, not sure how long it takes for you guys to get confirmation or what other steps are required. Hopefully that satiates the small army of lawyers lurking around.
"
151	123	jbeda	2014-06-16 16:44:33	CONTRIBUTOR	"Awesome!  Thanks.
"
152	8	brendandburns	2014-06-16 17:56:12	CONTRIBUTOR	"There is now a 'Pending' state, we can add a 'sync' flag to cloudcfg to wait for process startup.
"
153	122	DazWilkin	2014-06-16 18:00:35	NONE	"Thanks for the help. Yes, this worked. I had to create a new firewall rule for the 2 minions for tcp:8080
"
154	125	lavalamp	2014-06-16 18:02:34	MEMBER	"I think you need a rebase...
"
155	125	brendandburns	2014-06-16 18:16:43	CONTRIBUTOR	"Done.
"
156	34	brendandburns	2014-06-16 18:24:12	CONTRIBUTOR	"This is now fixed everywhere.
"
157	103	bgrant0607	2014-06-16 22:37:16	MEMBER	"We definitely want to allow the replicationController to be deleted without deleting the pods it created. It is designed to be loosely coupled to the pods.

replicationController is eventually consistent, so this sounds like a bug in the CLI more than anything. 

We should provide an easy way (ideally an API call) to wait until currentState matches desiredState, and then use that in the command-line tool to wait for completion of all async operations.

We could also provide a bulk delete operation of the kind jbeda suggested, but using general label queries rather than being specific to replicationController.

We should be wary of leaving resources in weird states such that they are in use but can no longer be queried or updated or deleted.
"
158	113	bgrant0607	2014-06-16 22:42:30	MEMBER	"For now, we should ensure that it's straightforward to write robust, idempotent ""up"" and ""down"" scripts with our tools and APIs. I'd lump a more general declarative approach with update support.
"
159	66	bgrant0607	2014-06-17 01:01:10	MEMBER	"I view per-container liveness probes as having 4 main parts:
1. Probe control parameters: At minimum, there needs to be a probe interval (in seconds is probably fine) and timeout period (in same units as probe interval), with reasonable defaults for both. An initial (post-(re)start) delay is also typically needed, to allow for non-trivial application startup times. We could also support a threshold for the number of failures to allow before action is taken (called unhealthy_threshold in the load-balancing context). This would cover retry in the case of spurious failure. If we do, we may also want to support a number of successes before we reset this failure count (healthy_threshold).
2. Probe mechanism. 
   - HTTP GET includes at least port, path, and perhaps URL parameters. 200==success is easy to implement and to understand, but would mean that it could not share the same handler as load-balancer health (i.e., readiness) checks.  Consequently, we may want to treat 404, 500, and 503 as success, also. Intentional failure would be indicated by not responding. Non-standard success/failure criteria and/or more complex logic could be implemented using commands (e.g., wget or curl).
   - Command. Exit 0 would imply success. Agree that ""run in"" would be lighter-weight than a separate container.
3. Action control parameters: The main one is the grace period -- how long to wait before using SIGKILL. We could support configuration of a default grace period for all stop operations on the container, but it is also useful to use different grace periods for different kinds of stop reasons.
4. Action mechanism.
   - SIGTERM. Convenient in many languages but hard to pass other information, such as termination reason and grace period.
   - HTTP POST / web hook.
   - Command, again using ""run in"".

We also want it to be easy to disable/reenable these checks, such as for attaching a debugger and stopping at a breakpoint.
"
160	128	jbeda	2014-06-17 01:47:47	CONTRIBUTOR	"@kseifriedredhat Totally agree -- can you point to stuff as you see it and we'll fix it up.  Or send a PR.

Thanks!
"
161	128	ghost	2014-06-17 01:51:07	NONE	"Just grep the source, so for example:

./kubernetes-master/cluster/saltbase/salt/kube-proxy/default:DAEMON_ARGS=""$DAEMON_ARGS
--etcd_servers=http://{{ ips[0][0] }}:4001""
./kubernetes-master/cluster/saltbase/salt/kubelet/default:DAEMON_ARGS=""$DAEMON_ARGS
-etcd_servers=http://{{ ips[0][0] }}:4001 -address=$HOSTNAME""
./kubernetes-master/cluster/saltbase/salt/apiserver/default:DAEMON_ARGS=""$DAEMON_ARGS
-etcd_servers=http://{{ ips[0][0] }}:4001""
./kubernetes-master/cluster/saltbase/salt/controller-manager/default:DAEMON_ARGS=""$DAEMON_ARGS
-etcd_servers=http://{{ ips[0][0] }}:4001""
"
162	128	jbeda	2014-06-17 01:56:52	CONTRIBUTOR	"Those are all intra-cluster communication.  In the typical deployment none of those would be going over a WAN link.  While ideally that stuff would be over TLS also, distributing key material in a secure way becomes difficult in an automated way.

I'd prioritize places where we grab resources over the internet over securing intra-cluster communication.
"
163	128	jbeda	2014-06-17 02:01:11	CONTRIBUTOR	"I filed #129 to track intra-cluster communication.
"
164	130	brendandburns	2014-06-17 03:42:03	CONTRIBUTOR	"Basically LGTM, some minor stuff.
"
165	130	lavalamp	2014-06-17 05:13:41	MEMBER	"All fixed, PTAL
"
166	132	brendandburns	2014-06-17 15:18:07	CONTRIBUTOR	"Hello,
Thanks for taking a look at Kubernetes. (and thanks for volunteering to write docs!)

There are a couple of things that I think are wrong with your set up:

1) You need to point the kubelet at etcd also (`-etcd_servers=http://10.0.1.115:4001`)

2) The apiserver comes up on port 8080 by default, so you need to use that as your host for cloudcfg

``` sh
./cmd/cloudcfg/cloudcfg -h http://localhost:8080 ...
```

Port 10250 is used for communication between the apiserver and the kubelet for container details (basically 'docker inspect ...'), but that should just work.

Let me know if that helps, or if you run into more problems.  We hang out on irc on #google-containers, so feel free to drop by there for some more interactive help. 

Best (and thanks again for volunteering to help with docs ;)

--brendan
"
167	132	lavalamp	2014-06-17 17:14:56	MEMBER	"One other thing to check: kublet uses the `hostname` command to figure out what to call itself. That needs to match the thing you pass to apiserver's `-machines=` flag. You can use kubelet's `-hostname_override=` flag to force it to use another identifier if necessary.
"
168	135	lavalamp	2014-06-17 18:12:34	MEMBER	"Hm, I see some binary files and .hg directories from dependencies. Is it a problem for the next person to re-get if we add them to .gitignore?
"
169	136	proppy	2014-06-17 18:17:33	CONTRIBUTOR	"LGTM
"
170	136	lavalamp	2014-06-17 18:34:41	MEMBER	"Merging anyway, looks like travis's network blipped.
"
171	135	brendandburns	2014-06-17 19:28:45	CONTRIBUTOR	"Ok, addressed comments, removed the .hg files, cleaned things up.  I think its all set now.

PTAL
Thanks!
--brendan
"
172	135	brendandburns	2014-06-17 19:29:04	CONTRIBUTOR	"(note I still need to add unit tests, so don't merge.  Once there's LGTM, I'll add tests.)
"
173	135	lavalamp	2014-06-17 19:30:33	MEMBER	"LGTM now. One nit, cloudprovider.CloudInterface could be named just cloudprovider.Interface.
"
174	66	bgrant0607	2014-06-17 19:48:36	MEMBER	"It's worth noting that docker stop sends SIGTERM, waits for a parameterized grace period, and then sends SIGKILL, which is basically the behavior we want:
POST /containers/(id)/stop?t=(seconds)
http://docs.docker.com/reference/api/docker_remote_api_v1.12/

FWIW, some do not like SIGKILL:
https://github.com/dotcloud/docker/issues/6446
It was pointed out that kill takes a signal parameter, which maybe they also want to support in stop, but I think a longer grace period is mostly what they need.
"
175	142	lavalamp	2014-06-17 22:13:10	MEMBER	"Thanks for the change! Can you please sign our CLA (instructions in CONTRIB.md) so that we can accept it?
"
176	142	jjhuff	2014-06-17 22:18:15	CONTRIBUTOR	"Oops, forgot about that. Signed.

On Tue, Jun 17, 2014 at 3:13 PM, Daniel Smith notifications@github.com
wrote:

> Thanks for the change! Can you please sign our CLA (instructions in
> CONTRIB.md) so that we can accept it?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/142#issuecomment-46372647
> .
"
177	143	jjhuff	2014-06-17 22:53:14	CONTRIBUTOR	"Worth mentioning that this allows https://github.com/GoogleCloudPlatform/kubernetes#running-a-container-simple-version to work now:)
"
178	135	brendandburns	2014-06-18 00:27:38	CONTRIBUTOR	"Comments addressed.  Tests added.  Ready to merge when you're ready.
"
179	143	lavalamp	2014-06-18 01:08:56	MEMBER	"Thanks for finding this. It seems the unit tests don't exercise it, I'll add a test.
"
180	143	jjhuff	2014-06-18 04:48:42	CONTRIBUTOR	"Sounds good!

On Tue, Jun 17, 2014 at 6:09 PM, Daniel Smith notifications@github.com
wrote:

> Thanks for finding this. It seems the unit tests don't exercise it, I'll
> add a test.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/143#issuecomment-46385551
> .
"
181	132	discordianfish	2014-06-18 10:32:51	CONTRIBUTOR	"Thanks, I've fixed 1) and 2) now still having the hostname issue: Is there a reason not to use the IP address as identifier? I suppose if someone needs to change the IP of a given system, you need to take it down anyways and kubernetes would reschedule the instances, right?
"
182	153	brendandburns	2014-06-18 15:03:54	CONTRIBUTOR	"Thanks for trying Kubernetes, sorry it didn't work for you.

Do you have an old version of go installed?  We currently are building with go 1.2, you can run `go -version` to find the version.

I tried to repro. this on a clean client, and I can't seem to reproduce the problem, also our Travis build is green: https://travis-ci.org/GoogleCloudPlatform/kubernetes, so I think that the problem must be somewhere in your set up.

Please let me know some more details and we can help you debug.

Best!
--brendan
"
183	132	brendandburns	2014-06-18 15:08:46	CONTRIBUTOR	"machine name tends to be more robust since it survives dhcp re-negotiation, VM restarts, etc.

that said, I would think that IP addresses should work, as Daniel mentioned, I think you need to set `--hostname_override=10.0.1.115` on the kubelet binary.  That should override the hostname look up.

Please let us know if you have further problems, or if that doesn't work.

Best!
--brendan
"
184	147	vmarmol	2014-06-18 17:32:13	CONTRIBUTOR	"Since we use Docker to start containers, we'll need support there as well. @rjnagal @vishh and myself have been working towards that. There should already be some support for this and more will drop in the near future.
"
185	134	jjhuff	2014-06-18 17:35:03	CONTRIBUTOR	"It seems like a new ManifestSet/ManifestList type might make the most sense. Changing the existing manifest type will have more tendrils.

I also like the idea of reading a dir in addition to a file. That starts to make bootstrapping easier...
"
186	153	brendandburns	2014-06-18 17:42:46	CONTRIBUTOR	"I validated that this is due to trying to compile with Go version 1.0 rather than 1.2.

I'll update the scripts to validate that the appropriate version of Go is installed.

So please upgrade your installed version of golang, and try again.

Thanks!
--brendan
"
187	122	jbeda	2014-06-18 17:48:01	CONTRIBUTOR	"Closing this -- let us know if you have any more snags.
"
188	156	brendandburns	2014-06-18 18:20:41	CONTRIBUTOR	"I think we'd rather not have the Kubelet calling back into the master, since it can result in massive fan-in storms of messages though.

However, I can definitely see the value in caching the information inside the apiserver.  So what do you think about having the apiserver periodically poll all Kubelets for information, and caching that information locally.  I think that would satisfy all of the needs you enumerated, while still enabling the master to control the flow of information.

What do you think?
"
189	156	jbeda	2014-06-18 18:30:44	CONTRIBUTOR	"In the past I've suggested a strategy where we have the master (or some fellow traveller server process) scrapes the nodes regularly and writes back to the master/etcd.  We'd then return how stale results are in our API.

Further, there'd be an API option to ask for ""up to the second"" results that would result in a sync call out to the node.
"
190	156	jjhuff	2014-06-18 18:33:19	CONTRIBUTOR	"I think that any apiserver->kublet polling will have problems in some
deployments. For example;
- Using a mix of internal machines and GCE. Without a fair amount of work,
  neither has full access to the other's network -- each has to talk via NAT.
- Running the master on, say, AppEngine. That'd be super handy for
  reducing common-mode failures.

I hear ya on the fan-in problem. That problem already exists to some extent
with the HTTP polling option. Since apiserver is stateless (yay!), it
should scale-out reasonably well as long as the backing store can handle
it...but we already have that problem.

Perhaps only push updates on state changes?

On Wed, Jun 18, 2014 at 11:20 AM, brendandburns notifications@github.com
wrote:

> I think we'd rather not have the Kubelet calling back into the master,
> since it can result in massive fan-in storms of messages though.
> 
> However, I can definitely see the value in caching the information inside
> the apiserver. So what do you think about having the apiserver periodically
> poll all Kubelets for information, and caching that information locally. I
> think that would satisfy all of the needs you enumerated, while still
> enabling the master to control the flow of information.
> 
> What do you think?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/156#issuecomment-46473525
> .
"
191	156	brendandburns	2014-06-18 18:45:17	CONTRIBUTOR	"I hear you about the problems with mixed topologies, pushing updates on state changes actually does have the same fan-in problems, suppose all of your tasks fail at the same time with a packet of death, you'll still see a storm of messages.

Hacking in the polling from the apiserver is going to be easier in the short term, so I think we'll at least do that first.
"
192	156	bgrant0607	2014-06-18 19:52:44	MEMBER	"The main reasons for the apiserver to contact kubelets rather than the other way are:
- connection storms, which admittedly can be mitigated by fuzzing
- other communication storms, which can be mitigated by the apiserver returning callback times to the kubelets
- facilitating use of kubelets without apiserver, which admittedly could be controlled via configuration
- support multiple independent state scrapers
- shard state scraping amongst multiple servers, which admittedly could be handled with redirects
- natural for heartbeating, starting new containers, and remote management of kubelet itself

Regardless which components initiate the connection, we maywant to implement a number of optimizations, especially once we start to collect resource stats:
- state caching
- change notification rather than polling
- change significance filtering
"
193	155	lavalamp	2014-06-18 19:55:51	MEMBER	"Hm, good luck parsing travis's dev go version output  :(
"
194	144	lavalamp	2014-06-18 20:12:59	MEMBER	"Comments addressed, controller is now over 80% test coverage.
"
195	150	lavalamp	2014-06-18 20:16:45	MEMBER	"LGTM other than missing boilerplate.
"
196	127	bgrant0607	2014-06-18 20:20:15	MEMBER	"Thoughts on multi-container pods:

First of all, I think restart behavior should be specified at the pod level rather than the container level. It wouldn't make sense for one container to terminate and another to restart forever, for example.

Run forever is obviously easy -- we're doing it now.

Run once is fairly easy, too, I think. As soon as one container terminates, probably all should be terminated (call this policy ""any"").

For run until success, we could restart individual containers until each succeeds (call this policy ""all"").

We should make all vs. any a separate policy from forever vs. success vs. once. Another variant people would probably want is a ""leader"" container, to which the other containers' lifetimes would be tied. Since we start containers in order, the leader would need to be the first one in the list.  To play devil's advocate, if we had event hooks (#140), the user could probably implement ""any"" and ""leader"" policies if we only provided ""all"" semantics.

Now, set-level behavior:

replicationController at least needs to know the conditions under which it should replace terminated/lost instances. It's hard to provide precise success semantics since containers can be lost with indeterminate exit status, but that's technically true even for a single pod. replicationController should be able to see the restart policies and termination reasons of the pods it controls. If a pod terminates and should not be restarted, I think replicationController should just automatically reduce its desired replica count by one.

I could also imagine users wanting any/all/leader behavior at the set level. However, I don't think we should do that, which leads me to believe we shouldn't do it at the pod level for now, either. If we were to provide the functionality at the set level, it shouldn't be tied to repllicationController. Instead, it would be a separate policy resource associated with the pods via its own label selector. This would allow it to work at either the service level or replicationController level or over any other grouping the user desired. We should ensure that it's not too hard to implement these types of policies using event hooks.
"
197	150	brendandburns	2014-06-18 20:25:33	CONTRIBUTOR	"Comment addressed.  ptal

Thanks!
--brendan
"
198	150	lavalamp	2014-06-18 20:33:47	MEMBER	"Unsure why the integration test failed. This shouldn't have broken it.
"
199	150	brendandburns	2014-06-18 20:35:25	CONTRIBUTOR	"Ugh, fixed.  I'll look at the tests...

--brendan

On Wed, Jun 18, 2014 at 1:33 PM, Daniel Smith notifications@github.com
wrote:

> Unsure why the integration test failed. This shouldn't have broken it.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/150#issuecomment-46490191
> .
"
200	155	brendandburns	2014-06-18 20:46:30	CONTRIBUTOR	"Used a Travis env var as an escape hatch.  This should be good now.

--brendan
"
201	156	jjhuff	2014-06-18 20:48:30	CONTRIBUTOR	"My concern has mainly been figuring out the blockers to actually
deploying kubernetes outside of a strictly GCE environment given security
and network topology constraints. That makes me want to take scissors to
these all-all communication patterns:)

Perhaps a hybrid approach is going to be the best:
- Cache state on the master -- either in memory or backed by etcd/whatever.
  In memory should be reasonable for a single apiserver instance, but we'd
  need persistence for anything more.
- Optional state push (interval or change based) from the kubelets. Much
  like all of it's existing options.  The state push would just populate the
  cache

This adds caching to the baseline config, gives the option to reverse the
apiserver-kubelet communication, and preserves the ability for other tools
to scrape the kubelets.

On Wed, Jun 18, 2014 at 12:52 PM, bgrant0607 notifications@github.com
wrote:

> The main reasons for the apiserver to contact kubelets rather than the
> other way are:
> - connection storms, which admittedly can be mitigated by fuzzing
> - other communication storms, which can be mitigated by the apiserver
>   returning callback times to the kubelets
> - facilitating use of kubelets without apiserver, which admittedly
>   could be controlled via configuration
> - support multiple independent state scrapers
> - shard state scraping amongst multiple servers, which admittedly
>   could be handled with redirects
> - natural for heartbeating, starting new containers, and remote
>   management of kubelet itself
> 
> Regardless which components initiate the connection, we maywant to
> implement a number of optimizations, especially once we start to collect
> resource stats:
> - state caching
> - change notification rather than polling
> - change significance filtering
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/156#issuecomment-46485272
> .
"
202	150	brendandburns	2014-06-18 20:55:00	CONTRIBUTOR	"Looks like etcd flaked in the integration test.  Let's submit this, we're tracking that flake elsewhere.
"
203	156	brendandburns	2014-06-18 21:03:05	CONTRIBUTOR	"SGTM.

I will send a PR for the poll and cache support today, and then work on the
optional Kubelet -> apiserver code path.

Thanks for bearing with us as we sort through this stuff ;)

--brendan

On Wed, Jun 18, 2014 at 1:48 PM, Justin Huff notifications@github.com
wrote:

> My concern has mainly been figuring out the blockers to actually
> deploying kubernetes outside of a strictly GCE environment given security
> and network topology constraints. That makes me want to take scissors to
> these all-all communication patterns:)
> 
> Perhaps a hybrid approach is going to be the best:
> - Cache state on the master -- either in memory or backed by
>   etcd/whatever.
>   In memory should be reasonable for a single apiserver instance, but we'd
>   need persistence for anything more.
> - Optional state push (interval or change based) from the kubelets. Much
>   like all of it's existing options. The state push would just populate the
>   cache
> 
> This adds caching to the baseline config, gives the option to reverse the
> apiserver-kubelet communication, and preserves the ability for other tools
> to scrape the kubelets.
> 
> On Wed, Jun 18, 2014 at 12:52 PM, bgrant0607 notifications@github.com
> wrote:
> 
> > The main reasons for the apiserver to contact kubelets rather than the
> > other way are:
> > - connection storms, which admittedly can be mitigated by fuzzing
> > - other communication storms, which can be mitigated by the apiserver
> >   returning callback times to the kubelets
> > - facilitating use of kubelets without apiserver, which admittedly
> >   could be controlled via configuration
> > - support multiple independent state scrapers
> > - shard state scraping amongst multiple servers, which admittedly
> >   could be handled with redirects
> > - natural for heartbeating, starting new containers, and remote
> >   management of kubelet itself
> > 
> > Regardless which components initiate the connection, we maywant to
> > implement a number of optimizations, especially once we start to collect
> > resource stats:
> > - state caching
> > - change notification rather than polling
> > - change significance filtering
> > 
> > —
> > Reply to this email directly or view it on GitHub
> > <
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/156#issuecomment-46485272>
> > 
> > .
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/156#issuecomment-46491857
> .
"
204	156	jjhuff	2014-06-18 21:13:30	CONTRIBUTOR	"No problem! Thanks for even doing the work -- I was happy to do that.  I was thinking of tackling #134 as well, but I can wait if you want to avoid conflicts.
"
205	156	brendandburns	2014-06-18 21:22:04	CONTRIBUTOR	"We're happy to take the work!  Feel free to take on #134, and I might
delegate the push stuff to you too.  We'll see how the pull cache goes....

Best
--brendan

On Wed, Jun 18, 2014 at 2:13 PM, Justin Huff notifications@github.com
wrote:

> No problem! Thanks for even doing the work -- I was happy to do that. I
> was thinking of tackling #134
> https://github.com/GoogleCloudPlatform/kubernetes/issues/134 as well,
> but I can wait if you want to avoid conflicts.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/156#issuecomment-46495019
> .
"
206	160	brendandburns	2014-06-18 21:29:27	CONTRIBUTOR	"I think I prefer option 1.  There is already a communication channel from master to kublet (see kublet_server.go) and I think we can merge the info in there.

Let me know if you want/need more details.

Let's not store this in etcd for now, I don't see too much utility in storing this kind of transient info in etcd.
"
207	161	lavalamp	2014-06-18 21:30:36	MEMBER	"LGTM, but integration doesn't build.
"
208	161	brendandburns	2014-06-18 21:31:38	CONTRIBUTOR	"Yeah, it was dependent on #150.  I'm updating it now.

--brendan

On Wed, Jun 18, 2014 at 2:30 PM, Daniel Smith notifications@github.com
wrote:

> LGTM, but integration doesn't build.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/161#issuecomment-46496974
> .
"
209	160	monnand	2014-06-18 21:36:58	CONTRIBUTOR	"@brendanburns Oh, I didn't notice kubelet has already exposed a REST API. OK. I could work on it this week. Thank you @brendandburns .
"
210	161	brendandburns	2014-06-18 21:39:49	CONTRIBUTOR	"Comments addressed.  PTAL

Thanks!
--brendan
"
211	161	brendandburns	2014-06-18 21:58:34	CONTRIBUTOR	"comments addressed.  ptal.

Thanks!
--brendan
"
212	162	brendandburns	2014-06-19 00:00:38	CONTRIBUTOR	"Comments addressed, please re-check.
"
213	140	bgrant0607	2014-06-19 00:25:10	MEMBER	"Docker issues requesting postcreate hooks:
https://github.com/dotcloud/docker/issues/3317
https://github.com/dotcloud/docker/issues/252

One of these mentioned that Docker planned to support hooks for every event, but that was quite a while ago, so I'm not sure whether that's still in plan.
"
214	164	lavalamp	2014-06-19 00:26:04	MEMBER	"This isn't ready to merge yet because I need to do the raml doc generation.
"
215	164	lavalamp	2014-06-19 01:24:29	MEMBER	"Hm, the doc generation isn't working on my system. Will have to debug later.
"
216	164	brendandburns	2014-06-19 03:11:49	CONTRIBUTOR	"This basically LGTM, once you do the RAML gen, and look at the gofmt part, I'm ready to merge.

Thanks!
"
217	87	brendandburns	2014-06-19 03:13:23	CONTRIBUTOR	"Closed by #149 and #135 
"
218	124	brendandburns	2014-06-19 03:14:01	CONTRIBUTOR	"Closed by #163 and #161 
"
219	164	brendandburns	2014-06-19 04:13:19	CONTRIBUTOR	"Ok, LGTM.  It looks like the HTML was generated?  Is this ready to merge?
"
220	165	lavalamp	2014-06-19 04:32:28	MEMBER	"LGTM
"
221	165	brendandburns	2014-06-19 04:33:39	CONTRIBUTOR	"Updated to add the comment.
"
222	166	lavalamp	2014-06-19 05:23:42	MEMBER	"I had some other ideas on how to do this... let's chat tomorrow, maybe what I'm thinking won't work.
"
223	166	proppy	2014-06-19 05:35:07	CONTRIBUTOR	"Something you can consider is keeping the synchronous operation and just wrapping the call in a goroutine.

```
outc := make(chan interface{})
errc := make(chan error)
go func() {
   obj, err := storage.Create(object)
   if err != nil {
      errc <- obj
      return
   }
   objc <- obj
}()
select {
  case obj := <-objc:
        // ...
  case err := <-errc:
        // ...
  case <- time.After(1 * time.Second):
        // ...
}
```
"
224	97	thockin	2014-06-19 06:30:28	MEMBER	"I endorse this message.
"
225	13	proppy	2014-06-19 08:37:43	CONTRIBUTOR	"PTAL, refactored all client tests and add more coverage with service + makeRequests
"
226	44	drewcsillag	2014-06-19 13:46:19	CONTRIBUTOR	"Looks like this PR is no longer applicable.  Closing
"
227	43	drewcsillag	2014-06-19 13:52:14	CONTRIBUTOR	"CLA should now be signed by my employer.
"
228	167	brendanburns	2014-06-19 15:48:14	CONTRIBUTOR	"Awesome! Thanks for doing this. I'll take a detailed look today. In the
meantime, can you sign our CLA?

Instructions are in CONTRIB.md. It's basically identical to the Apache CLA

Thanks again.
Brendan

On Thu, Jun 19, 2014, 5:53 AM, discordianfish notifications@github.com
wrote:

> Hi,
> 
> I've put all this in one PR, hope that's okay. It's basically what I had
> to change to get it working in my everything-is-dockerized environment.
> I've tried to come up with ""good commits"". Let me know if you prefer
> single PRs for each of them.
> 
> So the idea here is to run kubernetes itself in Docker containers,
> therefor I created three Dockerfiles. The documentation assumes that this
> will be added as automated builds in the Google account. Guess that's up
> for discussion so let me know if I should change that.
> The Dockerfile in / will create a kubernetes 'base image' which just
> includes the code/binaries and is used by kubernetes-node and
> kubernetes-server. This might not fit all all deployments but it's a good
> start and easy to understand.
> To make this possible I added a -docker flag to point to the docker
> address (since with the bind-mount we can't use /var/run/docker.sock in the
> container). Beside that I made kubelet use the client lib for pulling the
> 
> ## image instead of the binary (which isn't installed in the container).
> 
> You can merge this Pull Request by running
> 
>   git pull https://github.com/discordianfish/kubernetes various-fixes
> 
> Or view, comment on, or merge it at:
> 
>   https://github.com/GoogleCloudPlatform/kubernetes/pull/167
> Commit Summary
> - Add docker addr flag and check for etcd_machines
> - Print errors as string
> - Use docker client lib instead of binary for pulls
> - Add Dockerfiles and deployment instructions
> 
> File Changes
> - _A_ Dockerfile
>   https://github.com/GoogleCloudPlatform/kubernetes/pull/167/files#diff-0
>   (14)
> - _M_ README.md
>   https://github.com/GoogleCloudPlatform/kubernetes/pull/167/files#diff-1
>   (3)
> - _M_ cmd/kubelet/kubelet.go
>   https://github.com/GoogleCloudPlatform/kubernetes/pull/167/files#diff-2
>   (9)
> - _A_ kubernetes-node/Dockerfile
>   https://github.com/GoogleCloudPlatform/kubernetes/pull/167/files#diff-3
>   (12)
> - _A_ kubernetes-node/README.md
>   https://github.com/GoogleCloudPlatform/kubernetes/pull/167/files#diff-4
>   (13)
> - _A_ kubernetes-node/run.sh
>   https://github.com/GoogleCloudPlatform/kubernetes/pull/167/files#diff-5
>   (14)
> - _A_ kubernetes-server/Dockerfile
>   https://github.com/GoogleCloudPlatform/kubernetes/pull/167/files#diff-6
>   (6)
> - _A_ kubernetes-server/README.md
>   https://github.com/GoogleCloudPlatform/kubernetes/pull/167/files#diff-7
>   (9)
> - _A_ kubernetes-server/run.sh
>   https://github.com/GoogleCloudPlatform/kubernetes/pull/167/files#diff-8
>   (12)
> - _M_ pkg/kubelet/kubelet.go
>   https://github.com/GoogleCloudPlatform/kubernetes/pull/167/files#diff-9
>   (40)
> 
> Patch Links:
> - https://github.com/GoogleCloudPlatform/kubernetes/pull/167.patch
> - https://github.com/GoogleCloudPlatform/kubernetes/pull/167.diff
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/167.
"
229	167	proppy	2014-06-19 15:54:29	CONTRIBUTOR	"Related: #19 and #141 
"
230	113	bgrant0607	2014-06-19 16:00:21	MEMBER	"I filed another issue for idempotent creation (#148). We can use this issue for multi-resource declarative configuration, starting with up/down.
"
231	134	jjhuff	2014-06-19 16:09:38	CONTRIBUTOR	"Are you guys concerned about breaking compatibility with the kubelet's http GET/PUT interface?
"
232	167	jbeda	2014-06-19 16:49:03	CONTRIBUTOR	"Wow!  Thanks for the PR.

Unfortunately, this overlaps with some work that I have in progress now too.  So we'll have to figure out how to merge it.

First off -- if you could break out the addr flag commit, the error commit and the client lib commit into separate PRs (or a single one) -- that'd be great -- we can land those right away once we clear the CLA.

My plan is a little bit more ambitious than what you have here -- but that is maybe why it is taking longer.  I'm looking to pre-compile the binaries before packaging the runtime Dockerfiles.  This'll help to minimize download/upload size and will help cluster start time.  I also want to rework the GCE cluster scripts to use these.  That is obviously separable though.

Things that overlap and are different:
- I lean toward running 1 process per container instead of bundling multiple servers into a single container.  This is the model we use internally at Google and it has worked out well.  In fact, the pod concept was introduced to represent a bundle of containers that work together like this. Advantages here:
  - As we add extend container services like log collection and monitoring, being able to pinpoint these to a binary instead of a container is super useful.  If memory usage on a container starts to run away, you want to know the process/binary that is doing it.
  - The kubelet will monitor for crashes and restart containers in that case.  Over time, these resources are another production metric that you'll want to track and alert on.  If we bundle multiple processes in a container, we hide that.
  - In long running production systems, being able to upgrade/push different components on different schedules is useful.  We may want to push a new etcd server without taking down the rest of the binaries on a machine.
- I'd love to start using the kubelet in a more ""static"" mode to handle the master containers.  That would consist of getting the kubelet running on the master (with out looking an etcd) and then have it run a pod for the master components by reading out of a manifest file on disk.
- While there are some that run etcd on every node, it just doesn't scale in a clustered way that well.  Once the coreos guys introduce a 'read replica' of etcd, this may make more sense.  Details on etcd cluster size [here](https://coreos.com/docs/cluster-management/scaling/etcd-optimal-cluster-size/).

That being said, having a single Docker image for the master and node does simplify start up!  Hopefully we can get the ""manual set up"" steps simple enough that it is easy to port Kubernetes to new environments.

Hopefully I'll have a PR out today or tomorrow (I need to as I'm going on vacation after that) that should get some of this stuff more concrete.  You can see the start of what I'm doing in the `build/` directory.
"
233	134	jbeda	2014-06-19 16:50:44	CONTRIBUTOR	"Hopefully we can work this out using the version string.  If we had to break compat at this early stage it wouldn't be the end of the world but we should get in the habit of not doing that.
"
234	167	discordianfish	2014-06-19 17:12:20	CONTRIBUTOR	"First of all, I've split out #169 

Regarding using pre-built binaries: I would also prefer separating build- from runtime so we would just have the built binaries in the images. But since Docker unfortunately doesn't support that (yet) and using a Dockerfile is the canonical way to build Docker images I went this way. Since Docker caches the images layer by layer the actual size it needs to fetch on update, even if we keep all build stuff around, shouldn't be a big problem.

Totally agree with ""running one app per container"" in general. But I'm being pragmatic here: Instead of writing tons of bash scripts to build and run the various components on a vanilla docker cluster I decided to wrap up the required components in two easily deployable images. It's more meant as a quick start than to be used as it is for production deployments. It will help people trying kubernetes on their docker hosts. It took me quite some time without that until I got it running. 

But I love the idea of using kubernetes to host kubernetes, so to me it depends on how fast we can get there and whether you want to provide something in the meanwhile so people can play with it.
"
235	164	lavalamp	2014-06-19 17:15:56	MEMBER	"Still having problems getting boot2docker working... The HTML is only half changed, it's missing the change that involved more than just find-and-replace.
"
236	167	discordianfish	2014-06-19 17:24:53	CONTRIBUTOR	"(I've just moved cdd6b3c back in here so this branch can be built/used)
"
237	140	bgrant0607	2014-06-19 18:35:49	MEMBER	"Just got support from tianon for runin/exec/enter support: https://github.com/dotcloud/docker/issues/1228
So we can use that to execute hooks inside containers
"
238	164	lavalamp	2014-06-19 20:29:43	MEMBER	"OK, I'm pretty sure it's not a problem with my docker setup. Let's merge this and I'll work with @bgrant0607 to get the help gen working.
"
239	156	brendandburns	2014-06-19 20:50:49	CONTRIBUTOR	"Polling from the master was added in #171 

I'll work on optional push next.
"
240	174	vmarmol	2014-06-19 20:51:55	CONTRIBUTOR	"I think maybe a more manageable approach would be for Kubernetes to depend on the released version of cAdvisor (e.g.: the Docker image), rather than to build it and use it from source. WDYT? @brendanburns 
"
241	166	brendandburns	2014-06-19 20:59:40	CONTRIBUTOR	"Ok, I discussed this offline w/ @lavalamp and we came up with the following approach, I added a MakeAsync method that is used by the registries now, this means that individual storage's don't need deal in containers.

Take another look and let me know what you think.
"
242	174	vmarmol	2014-06-19 21:08:49	CONTRIBUTOR	"Spoke offline, we need the client library and structs for the data. What do you all think about just importing those two paths from cadvisor? If not, the whole source it is.
"
243	166	proppy	2014-06-19 21:14:53	CONTRIBUTOR	"I would have thought the MakeAsync logic would be around the `ControllerRegistryStorage` methods calls ot in their impl.
"
244	173	jjhuff	2014-06-19 22:05:03	CONTRIBUTOR	"Looks like the dir test is flaky. Fixing.
"
245	174	monnand	2014-06-19 22:13:33	CONTRIBUTOR	"@lavalamp  Addressed the comment. PTAL.
"
246	166	brendandburns	2014-06-19 22:18:04	CONTRIBUTOR	"The problem with the go routine wrapping the whole thing is that you don't get immediate feedback for validation errors.  In this approach, you get an immediate error if the request isn't accepted, and then you get a channel that you can optionally wait on for the result.
"
247	13	brendandburns	2014-06-19 22:21:46	CONTRIBUTOR	"Rebase, and then LGTM.
"
248	173	jjhuff	2014-06-19 22:22:45	CONTRIBUTOR	"That's better!
"
249	174	monnand	2014-06-19 22:29:33	CONTRIBUTOR	"@lavalamp PTAL.
"
250	174	monnand	2014-06-19 22:49:27	CONTRIBUTOR	"I forgot to mention: the current code is not wired with kubelet now. I will add it in another PR.
"
251	166	brendandburns	2014-06-19 22:50:08	CONTRIBUTOR	"Comments addressed, ptal.

Thanks
--brendan
"
252	173	lavalamp	2014-06-19 22:58:01	MEMBER	"LGTM. Thanks for the change!
"
253	174	monnand	2014-06-20 00:29:50	CONTRIBUTOR	"I think the license part is cleared now.
"
254	178	thockin	2014-06-20 00:30:59	MEMBER	"Caveat: Still very much learning go.  I'm worried about the direction this goes.  I think we will see other ""kinds"" of volumes, and I don't think we want to represent them all as flags on the volume struct.

I was thinking something more like (pardon the nonsense syntax):

volume {
  name: ""host_foo_bar""
  source {
    kind: ""HOST_FS""
    host_path: ""/foo/bar""
  }
}

and we could formalize on-disk volumes as

volume {
  name: ""pod_local_disk""
  source {
    kind: ""LOCAL""
  }
}

With the explicit default for source to be kind: ""LOCAL""

Something like that.
"
255	178	vmarmol	2014-06-20 00:32:21	CONTRIBUTOR	"I agree that we want to go somewhere along those lines, my understanding was that there was already work toward doing this ""the right way"" but that it would take longer. This is the minimal change needed to get this feature in.
"
256	178	vmarmol	2014-06-20 00:33:55	CONTRIBUTOR	"I didn't want to make anything beyond that since it'd make assumptions of what that final API would be. If people want to do that route now we can, I just don't want us to get sidetracked by it :)
"
257	178	lavalamp	2014-06-20 00:52:04	MEMBER	"I would vote for using an enumeration instead of a boolean, even in the interim.
"
258	181	jkaplowitz	2014-06-20 03:25:02	CONTRIBUTOR	"LGTM. Nice, straightforward enough for me to understand even as a Go novice.
"
259	178	brendandburns	2014-06-20 03:59:40	CONTRIBUTOR	"+1, can we convert this into an enumeration?  That should still keep it pretty simple.

Thanks!
--brendan
"
260	178	vmarmol	2014-06-20 04:03:39	CONTRIBUTOR	"SGTM, any prefered way to do an enum in JSON? I'm guessing a string with pre-defined values is the best way to go? Its easier to write by hand than an int value.
"
261	174	monnand	2014-06-20 04:06:58	CONTRIBUTOR	"All comments addressed. PTAL.
"
262	174	brendandburns	2014-06-20 04:09:15	CONTRIBUTOR	"Generally LGTM, some minor comments.
"
263	178	monnand	2014-06-20 04:11:12	CONTRIBUTOR	"@vmarmol FYI, here is how go's protobuf library deals with enum:

```
type SomeEnumType int

func (self SomeEnumType) MarshalJson() ([]byte, error){
// ...
}

func (self *SomeEnumeType) UnmarshalJson([]byte) error {

// ...
}
```

I think in old cAdvisor code, I wrote some helpers would could automate such step. Let me try to find it and feel free to use it or not.
"
264	178	brendandburns	2014-06-20 04:11:40	CONTRIBUTOR	"Yeah, that's generally the way Apiary has done it.

On Thu, Jun 19, 2014 at 9:03 PM, Victor Marmol notifications@github.com
wrote:

> SGTM, any prefered way to do an enum in JSON? I'm guessing a string with
> pre-defined values is the best way to go? Its easier to write by hand than
> an int value.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/178#issuecomment-46643057
> .
"
265	178	vmarmol	2014-06-20 04:12:01	CONTRIBUTOR	"Done. PTAL @brendanburns @thockin @lavalamp 
"
266	178	vmarmol	2014-06-20 04:12:53	CONTRIBUTOR	"@monnand ah yes I've seen those, they are nifty and we could use them as a string over the wire. May be okay as is for now though
"
267	178	brendandburns	2014-06-20 04:12:56	CONTRIBUTOR	"Sorry to be nit-picky, can we go all-caps for the values, to demonstrate that their enum, not string?

thx!
"
268	178	vmarmol	2014-06-20 04:15:00	CONTRIBUTOR	"@brendanburns haha, np. done. PTAL
"
269	174	monnand	2014-06-20 04:22:59	CONTRIBUTOR	"Changed the code accordingly. Use `w.Write(data)` instead of `fmt.FprintX()`. PTAL.
"
270	178	brendandburns	2014-06-20 04:28:43	CONTRIBUTOR	"thanks!
"
271	178	thockin	2014-06-20 04:43:35	MEMBER	"I'm OK with this, but I will try to make the case to deprecate this field
ASAP :)

On Thu, Jun 19, 2014 at 9:28 PM, brendandburns notifications@github.com
wrote:

> thanks!
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/178#issuecomment-46643902
> .
"
272	178	vmarmol	2014-06-20 04:46:32	CONTRIBUTOR	"I happily welcome that effort :)
"
273	178	brendandburns	2014-06-20 04:48:27	CONTRIBUTOR	"Hrm, @thockin Sorry didn't mean to over rule you.  I thought this was implementing your suggestion.

I'm imagining extending this by adding a ""Source"" field

MountType: PERSISTENT_DISK
MountPath: /foo/bar
Source: http://some/pd/path

or

MountType: GIT
MountPath: /my/git/path
Source: git://some/git/repo@commit

I suppose elevating it into its own type is ok, but I'm not sure it buys much....
"
274	178	brendandburns	2014-06-20 04:49:06	CONTRIBUTOR	"fwiw, if @thockin if you feel strongly, I'll do the deprecate PR tomorrow.

--brendan
"
275	178	thockin	2014-06-20 05:09:57	MEMBER	"No, this is fine for now, until we have a full solution and demonstration
of why it is better.  You didn't overrule me in any sense :)

My main point here is that we have two structures - Volume and VolumeMount.
 Volume is supposed to represent that ""what"" and VolumeMount is supposed to
represent the ""Where"" and ""How"".

Up until this change, ""what"" was always ""a pod-private local directory"".
 With this change we have added a new ""what"" but we've stuck it in the
""where"" structure.  You'll notice that this can ONLY identity-mount things
- there is no opportunity to remap host /foo onto container /bar (very
  similar to Docker's --volumes-from, which is broken in the same way).

A ""source"" field is the right direction, but it belongs in Volume, not
VolumeMount.  Consider that a volume definition is pod-scope, but mounts
are container-scope.  You want to spell out the source once, at pod-scope,
and let each container mount it where they want.

This also lays groundwork to get fancier with permissions, storage medium,
quota, etc.

I'm happy to go into my ideas here more, if you like.  We spent a lot of
time on this area internally, and I think (hope!) we can apply some of
those lessons here.  But I also want to hear from others if you all feel
we're screwing it up.

I would strike out on this now, but for this fabled intern who wants to
tackle some of this ;)

Tim

On Thu, Jun 19, 2014 at 9:49 PM, brendandburns notifications@github.com
wrote:

> fwiw, if @thockin https://github.com/thockin if you feel strongly, I'll
> do the deprecate PR tomorrow.
> 
> --brendan
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/178#issuecomment-46644628
> .
"
276	178	brendandburns	2014-06-20 05:18:22	CONTRIBUTOR	"Ah, you are 100% right.  I should have caught that.

Sorry!

(and yes, Jimmy claims that his intern will be starting on this on Monday,
I'll double check tomorrow)

That said, there's no reason that we can't lay in the right data structures
before the intern does the implementation.

--brendan

On Thu, Jun 19, 2014 at 10:10 PM, Tim Hockin notifications@github.com
wrote:

> No, this is fine for now, until we have a full solution and demonstration
> of why it is better. You didn't overrule me in any sense :)
> 
> My main point here is that we have two structures - Volume and
> VolumeMount.
> Volume is supposed to represent that ""what"" and VolumeMount is supposed to
> represent the ""Where"" and ""How"".
> 
> Up until this change, ""what"" was always ""a pod-private local directory"".
> With this change we have added a new ""what"" but we've stuck it in the
> ""where"" structure. You'll notice that this can ONLY identity-mount things
> - there is no opportunity to remap host /foo onto container /bar (very
>   similar to Docker's --volumes-from, which is broken in the same way).
> 
> A ""source"" field is the right direction, but it belongs in Volume, not
> VolumeMount. Consider that a volume definition is pod-scope, but mounts
> are container-scope. You want to spell out the source once, at pod-scope,
> and let each container mount it where they want.
> 
> This also lays groundwork to get fancier with permissions, storage medium,
> quota, etc.
> 
> I'm happy to go into my ideas here more, if you like. We spent a lot of
> time on this area internally, and I think (hope!) we can apply some of
> those lessons here. But I also want to hear from others if you all feel
> we're screwing it up.
> 
> I would strike out on this now, but for this fabled intern who wants to
> tackle some of this ;)
> 
> Tim
> 
> On Thu, Jun 19, 2014 at 9:49 PM, brendandburns notifications@github.com
> wrote:
> 
> > fwiw, if @thockin https://github.com/thockin if you feel strongly,
> > I'll
> > do the deprecate PR tomorrow.
> > 
> > --brendan
> > 
> > —
> > Reply to this email directly or view it on GitHub
> > <
> > https://github.com/GoogleCloudPlatform/kubernetes/pull/178#issuecomment-46644628>
> > 
> > .
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/178#issuecomment-46645281
> .
"
277	178	thockin	2014-06-20 05:22:13	MEMBER	"I think there's room for interpretation on what ""the right structures"" are
and I don't want to prescribe exactly what it should be without
reconsidering it anew.  I'm looking forward to some design discussion with
intern Danny.

Tim

On Thu, Jun 19, 2014 at 10:18 PM, brendandburns notifications@github.com
wrote:

> Ah, you are 100% right. I should have caught that.
> 
> Sorry!
> 
> (and yes, Jimmy claims that his intern will be starting on this on Monday,
> I'll double check tomorrow)
> 
> That said, there's no reason that we can't lay in the right data
> structures
> before the intern does the implementation.
> 
> --brendan
> 
> On Thu, Jun 19, 2014 at 10:10 PM, Tim Hockin notifications@github.com
> wrote:
> 
> > No, this is fine for now, until we have a full solution and
> > demonstration
> > of why it is better. You didn't overrule me in any sense :)
> > 
> > My main point here is that we have two structures - Volume and
> > VolumeMount.
> > Volume is supposed to represent that ""what"" and VolumeMount is supposed
> > to
> > represent the ""Where"" and ""How"".
> > 
> > Up until this change, ""what"" was always ""a pod-private local directory"".
> > With this change we have added a new ""what"" but we've stuck it in the
> > ""where"" structure. You'll notice that this can ONLY identity-mount
> > things
> > - there is no opportunity to remap host /foo onto container /bar (very
> >   similar to Docker's --volumes-from, which is broken in the same way).
> > 
> > A ""source"" field is the right direction, but it belongs in Volume, not
> > VolumeMount. Consider that a volume definition is pod-scope, but mounts
> > are container-scope. You want to spell out the source once, at
> > pod-scope,
> > and let each container mount it where they want.
> > 
> > This also lays groundwork to get fancier with permissions, storage
> > medium,
> > quota, etc.
> > 
> > I'm happy to go into my ideas here more, if you like. We spent a lot of
> > time on this area internally, and I think (hope!) we can apply some of
> > those lessons here. But I also want to hear from others if you all feel
> > we're screwing it up.
> > 
> > I would strike out on this now, but for this fabled intern who wants to
> > tackle some of this ;)
> > 
> > Tim
> > 
> > On Thu, Jun 19, 2014 at 9:49 PM, brendandburns notifications@github.com
> > 
> > wrote:
> > 
> > > fwiw, if @thockin https://github.com/thockin if you feel strongly,
> > > I'll
> > > do the deprecate PR tomorrow.
> > > 
> > > --brendan
> > > 
> > > —
> > > Reply to this email directly or view it on GitHub
> > > <
> > 
> > https://github.com/GoogleCloudPlatform/kubernetes/pull/178#issuecomment-46644628>
> > 
> > > .
> > 
> > —
> > Reply to this email directly or view it on GitHub
> > <
> > https://github.com/GoogleCloudPlatform/kubernetes/pull/178#issuecomment-46645281>
> > 
> > .
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/178#issuecomment-46645551
> .
"
278	178	jkaplowitz	2014-06-20 05:41:54	CONTRIBUTOR	"Just emailed him urging him to switch primary focus and establish contact
with you ASAP, with a bit of procedural guidance (nothing about the
substance) and a link to this discussion. His other work is at the point
where this switch makes good sense.
On Jun 19, 2014 10:22 PM, ""Tim Hockin"" notifications@github.com wrote:

> I think there's room for interpretation on what ""the right structures"" are
> and I don't want to prescribe exactly what it should be without
> reconsidering it anew. I'm looking forward to some design discussion with
> intern Danny.
> 
> Tim
> 
> On Thu, Jun 19, 2014 at 10:18 PM, brendandburns notifications@github.com
> 
> wrote:
> 
> > Ah, you are 100% right. I should have caught that.
> > 
> > Sorry!
> > 
> > (and yes, Jimmy claims that his intern will be starting on this on
> > Monday,
> > I'll double check tomorrow)
> > 
> > That said, there's no reason that we can't lay in the right data
> > structures
> > before the intern does the implementation.
> > 
> > --brendan
> > 
> > On Thu, Jun 19, 2014 at 10:10 PM, Tim Hockin notifications@github.com
> > wrote:
> > 
> > > No, this is fine for now, until we have a full solution and
> > > demonstration
> > > of why it is better. You didn't overrule me in any sense :)
> > > 
> > > My main point here is that we have two structures - Volume and
> > > VolumeMount.
> > > Volume is supposed to represent that ""what"" and VolumeMount is
> > > supposed
> > > to
> > > represent the ""Where"" and ""How"".
> > > 
> > > Up until this change, ""what"" was always ""a pod-private local
> > > directory"".
> > > With this change we have added a new ""what"" but we've stuck it in the
> > > ""where"" structure. You'll notice that this can ONLY identity-mount
> > > things
> > > - there is no opportunity to remap host /foo onto container /bar (very
> > >   similar to Docker's --volumes-from, which is broken in the same way).
> > > 
> > > A ""source"" field is the right direction, but it belongs in Volume, not
> > > VolumeMount. Consider that a volume definition is pod-scope, but
> > > mounts
> > > are container-scope. You want to spell out the source once, at
> > > pod-scope,
> > > and let each container mount it where they want.
> > > 
> > > This also lays groundwork to get fancier with permissions, storage
> > > medium,
> > > quota, etc.
> > > 
> > > I'm happy to go into my ideas here more, if you like. We spent a lot
> > > of
> > > time on this area internally, and I think (hope!) we can apply some of
> > > those lessons here. But I also want to hear from others if you all
> > > feel
> > > we're screwing it up.
> > > 
> > > I would strike out on this now, but for this fabled intern who wants
> > > to
> > > tackle some of this ;)
> > > 
> > > Tim
> > > 
> > > On Thu, Jun 19, 2014 at 9:49 PM, brendandburns <
> > > notifications@github.com>
> > > 
> > > wrote:
> > > 
> > > > fwiw, if @thockin https://github.com/thockin if you feel
> > > > strongly,
> > > > I'll
> > > > do the deprecate PR tomorrow.
> > > > 
> > > > --brendan
> > > > 
> > > > —
> > > > Reply to this email directly or view it on GitHub
> > > > <
> > 
> > https://github.com/GoogleCloudPlatform/kubernetes/pull/178#issuecomment-46644628>
> > 
> > > > .
> > > 
> > > —
> > > Reply to this email directly or view it on GitHub
> > > <
> > 
> > https://github.com/GoogleCloudPlatform/kubernetes/pull/178#issuecomment-46645281>
> > 
> > > .
> > 
> > —
> > Reply to this email directly or view it on GitHub
> > <
> > https://github.com/GoogleCloudPlatform/kubernetes/pull/178#issuecomment-46645551>
> > 
> > .
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/178#issuecomment-46645702
> .
"
279	13	proppy	2014-06-20 06:20:42	CONTRIBUTOR	"abandonning this PR in favor of #183 since I can't push to my private fork anymore.
"
280	169	discordianfish	2014-06-20 11:01:20	CONTRIBUTOR	"Ok, I've removed the check. Working on CLA, will sent it later hopefully.
"
281	147	smarterclayton	2014-06-20 14:50:32	CONTRIBUTOR	"Would QoS tiers specified via labeling or orthogonal to labeling (assuming orthogonal given presentations, just want to make sure)?   Do you see the need for feedback between a minion about current resource load and the scheduler, and if so, is this a backchannel (a formal event bus) concept that is part of Kubernetes or encapsulated behind the scheduler interface?
"
282	184	brendandburns	2014-06-20 15:33:09	CONTRIBUTOR	"Thanks for the fix.  I hate to ask, but its required: can you sign our CLA, details are in:

https://github.com/GoogleCloudPlatform/kubernetes/blob/master/CONTRIB.md

Thanks again!
--brendan
"
283	174	brendandburns	2014-06-20 15:36:19	CONTRIBUTOR	"LGTM, thanks!
"
284	147	thockin	2014-06-20 15:55:02	MEMBER	"Caveat, still coming up to speed on everything, but that won't stop me from
having an opinion.

QoS and administrative or config information should never be labels.
 Labels are the domain of the user, not the system.

On Fri, Jun 20, 2014 at 7:50 AM, Clayton Coleman notifications@github.com
wrote:

> Would QoS tiers specified via labeling or orthogonal to labeling (assuming
> orthogonal given presentations, just want to make sure)? Do you see the
> need for feedback between a minion about current resource load and the
> scheduler, and if so, is this a backchannel (a formal event bus) concept
> that is part of Kubernetes or encapsulated behind the scheduler interface?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/147#issuecomment-46686825
> .
"
285	182	thockin	2014-06-20 16:07:34	MEMBER	"LGTM modulo not knowing go that well :)
"
286	147	bgrant0607	2014-06-20 16:12:22	MEMBER	"@smarterclayton : I agree with @thockin . QoS specifications need to be first-class fields in our API, together with resource specifications (#168).

For one possible example of what this could look like, see lmctfy:
https://github.com/google/lmctfy/blob/master/include/lmctfy.proto
"
287	184	lavalamp	2014-06-20 16:16:28	MEMBER	"Argh, I missed the comma here, too. Thanks for finding! Will merge as soon as you sign the CLA. :)
"
288	169	lavalamp	2014-06-20 16:20:04	MEMBER	"LGTM, thanks for the change.
"
289	173	lavalamp	2014-06-20 16:22:14	MEMBER	"Can you rebase this so we can merge? Thanks!
"
290	173	jjhuff	2014-06-20 16:35:21	CONTRIBUTOR	"Done. Only a few conflicts, but I think they got sorted out. Just waiting on travis.
"
291	188	smarterclayton	2014-06-20 16:45:09	CONTRIBUTOR	"The VM per pod model would negate the container efficiency gains - I'm not sure how many people would be interested in deploying like that vs. simply using a VM.  

Practically speaking, if routable IP per pod is out of the technical capabilities of existing clouds at reasonable densities (is the 60 IP per amazon m1.xlarge too low for practical use cases?) or out of the administrative / organizational capabilities of non-cloud shops (which deserves further investigation), and if IPv6 is still 2-3 years out from reasonable deployment, then the kubernetes model is only deployable on GCE in practice.  Would be good to list out the practical limits in other clouds (openstack neutron, aws, soft layer) as well as a recommended IP per pod configuration that would work without a ton of admin headaches on metal.

It's possible that dynamic port allocation could be an alternate mode, supported with a subset of features and known limitations.  What would that abstraction have to look like for Kubernetes to continue to work on the ideal path?  A few things I can think of - the scheduler has to be aware of port exhaustion and record allocated ports OR the exposed ports have to be reported back via some backchannel to the master.  If there is a global record of allocated ports, the mechanism to efficiently distribute that port information to the appropriate proxies is required.  You _must_ implement at least one level of abstraction between container communication (either in a local or shared proxy or an iptables NAT translation a la geard).  You also must implement a more complex migration path for things like CRIU - with more steps before and afterwards to ensure that network abstraction is ready to accept the moved container.
"
292	174	monnand	2014-06-20 16:59:13	CONTRIBUTOR	"Thank you @brendanburns !
"
293	180	brendandburns	2014-06-20 17:48:36	CONTRIBUTOR	"Small stuff, basically LGTM
"
294	180	lavalamp	2014-06-20 18:00:03	MEMBER	"I'm still debugging this a bit. Will address comments later and repush later today.

What do you think about having this talk to the cloud provider, such that creating/deleting a minion creates/removes a new instance?
"
295	180	jbeda	2014-06-20 18:31:58	CONTRIBUTOR	"@lavalamp Right now the salt code will automatically reconfigure and restart the apiserver when the set of minions change.  I think you might have to do a highstate on the master but we could fix that.
"
296	184	gottwald	2014-06-20 18:41:05	CONTRIBUTOR	"signed
"
297	184	lavalamp	2014-06-20 18:43:39	MEMBER	"Thanks again!
"
298	108	gottwald	2014-06-20 19:08:32	CONTRIBUTOR	"I like the subcommand idea as seen in git, go command, gcloud and so on....
"
299	140	bgrant0607	2014-06-21 00:25:33	MEMBER	"I investigated Docker's event stream. It provides container id and event type (e.g., start, stop). It doesn't provide further details about the events. Also, it's obviously asynchronous with respect to the events. At least pre-start and post-termination event hook commands would be most useful if executed synchronously, inline with container execution.

Docker restart allows restarting the same command in the same container with the same container id and filesystem, even after the death of the previous process. There's no way to change the command executed AFAICT, however. If the forthcoming ""runin""/""exec"" allowed execution in dead containers, we could maybe use it for post-termination hooks. Pre-start hook commands look ugly without Docker support. No response yet to my docker-dev question about whether Docker is actually planning to add hook support.

We could override the container entrypoint with the pre-start hook command and then use ""runin"" to execute the real entrypoint, but the container's status, wait, restart, etc. would be broken. Creating a new container image that included the pre-start command, actual entrypoint, and post-termination hook might work, but we'd need to carefully propagate arguments, signals, exit status, etc.

The main reason for a post-start hook would be consistency, but it also might be convenient for start actions that don't block the start of the container's entrypoint, such as registration in a third-party discovery service, pushing events to pubsub, etc. ""runin"" should just work, though there may be a race if the application immediately terminated. Probably it would be useful to serialize execution with respect to later hooks on the same entity.

If we wrapped the application we could intercept SIGTERM in order to execute the pre-termination hook, and just pass the signal on to the application if no hook were specified. Again, we probably want to serialize with respect to the post-termination hook. It might not be super-useful to execute the pre-termination hook in the case that it weren't a planned container stop, but I'm not sure whether it's more natural to execute it prior to the post-termination hook regardless or whether it would be annoying for the pre-termination hook to execute when the application was already dead. OTOH, that case would probably need to be handled since the application could die at any time, including concurrently with the start of the pre-termination hook.

Asynchronous webhooks would be comparatively easy to support. The most difficult issue is what to do about auth. If generated by a command in the container, presumably they could authenticate as the container, so it looks appealing to only support command hooks. However, there are situations where we wouldn't have an obvious container to execute commands in, such as for pod lifecycle hooks or, even more obviously, replicationController and service lifecycle event hooks.
"
300	180	lavalamp	2014-06-21 01:04:44	MEMBER	"I have this mostly ready to go, but I'd like to get #196 merged first.
"
301	197	lavalamp	2014-06-21 15:33:08	MEMBER	"Thank you very much for the test!
"
302	198	lavalamp	2014-06-21 19:40:59	MEMBER	"Thanks. I think this is a general issue, I'll file a bug.
"
303	166	lavalamp	2014-06-21 20:50:11	MEMBER	"btw, @brendandburns @brendanburns-- I'm working on some changes to cloudcfg, so you don't need to update that.
"
304	200	lavalamp	2014-06-21 20:56:05	MEMBER	"I agree that the -- logic is a terrible hack. To solve it for real, though, I was thinking we should just make unique names and keep track of what names we've used, rather than parsing the names for our pattern. I'm OK with this patch in the interim, though.
"
305	200	jjhuff	2014-06-21 20:59:57	CONTRIBUTOR	"Yeah.
Does docker allow us to attach arbitrary data to a container's info? Or maybe go with nested containers? Although, that'd probably mean that any managed containers would die if the kubelet died, I'm not sure how I feel about that...
"
306	200	lavalamp	2014-06-21 21:05:04	MEMBER	"Yeah, the kubelet would have to store the mapping persistently, or possibly apiserver can pre-generate globally unique ids for this purpose (a la #199). It's a requirement that containers keep running even if kubelet restarts, so any subcontainer design would have to take that into account.
"
307	200	jjhuff	2014-06-21 21:10:33	CONTRIBUTOR	"Cool. #199 looks like the ticket.

On Sat, Jun 21, 2014 at 2:05 PM, Daniel Smith notifications@github.com
wrote:

> Yeah, the kubelet would have to store the mapping persistently, or
> possibly apiserver can pre-generate globally unique ids for this purpose (a
> la #199 https://github.com/GoogleCloudPlatform/kubernetes/issues/199).
> It's a requirement that containers keep running even if kubelet restarts,
> so any subcontainer design would have to take that into account.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/200#issuecomment-46764749
> .
"
308	201	jjhuff	2014-06-22 16:34:50	CONTRIBUTOR	"Yup, that's lots cleaner!
"
309	201	lavalamp	2014-06-22 18:03:00	MEMBER	"Thanks for the change!
"
310	180	lavalamp	2014-06-22 19:18:19	MEMBER	"Will fix this build once #196 is merged.
"
311	196	lavalamp	2014-06-23 16:55:24	MEMBER	"Comments addressed.
"
312	180	jjhuff	2014-06-23 16:59:25	CONTRIBUTOR	"""What do you think about having this talk to the cloud provider, such that creating/deleting a minion creates/removes a new instance?""
If I'm understanding what you mean, this feels 'backwards' to me. Creating a new instance often includes a number of infrastructure/environment specific things (network, images, packages, etc). I doubt we want to tackle wrapping all of that complexity.
It might be cleaner to have a minions register with the master, which would also remove the need for salt to restart it.
"
313	180	lavalamp	2014-06-23 17:08:18	MEMBER	"Sounds good. As this is written now, minions could use this API to add themselves. But we need to figure out some security stuff (How do minions trust the master? How does master trust the minions?) first before enabling that.

I do at least intend to use the cloud provider package to look up a minion's IP address upon a GET request. That seems pretty useful.
"
314	196	lavalamp	2014-06-23 17:37:51	MEMBER	"(That build failure looks like the etcd flake we get once in a while)
"
315	207	lavalamp	2014-06-23 17:51:50	MEMBER	"It shouldn't:

https://github.com/GoogleCloudPlatform/kubernetes/blob/master/hack/e2e-test.sh#L30

But my shell-fu is weak, maybe there's something wrong with that?
"
316	207	lavalamp	2014-06-23 17:55:46	MEMBER	"I think it's missing some quotes. I'll make a PR.
"
317	208	lavalamp	2014-06-23 19:04:22	MEMBER	"Will merge when build finishes.

Also possible (and more go-ish) with:

$ source hack/config-go.sh
$ cd pkg/kublet
$ go test
"
318	208	lavalamp	2014-06-23 19:18:10	MEMBER	"Looks like travis is down or something. Merging.
"
319	209	thockin	2014-06-23 20:16:12	MEMBER	"Don't you want to update README.md to add this to the hooks setup?
"
320	209	lavalamp	2014-06-23 20:17:22	MEMBER	"If you linked (`ln -s ...`) as written in the readme, it's already taken care of for you.
"
321	209	lavalamp	2014-06-23 20:18:47	MEMBER	"I'll see about adding a check for this to the travis build.
"
322	180	lavalamp	2014-06-23 20:38:20	MEMBER	"I think this is safe to merge now.
"
323	209	thockin	2014-06-23 20:49:48	MEMBER	"Ahh, I missed it being called from an existing hook.  Sorry.
"
324	209	jkaplowitz	2014-06-23 20:55:35	CONTRIBUTOR	"We should be careful that our automation doesn't remove external copyright notices from contributed code, nor add Google copyright notices to files we haven't modified. At the same time, we should make sure that Google-authored files do have our copyright notice, and that the Apache license notice is present wherever it should be. Our internal open source legal folks might have good advice on how to strike the right balance here.
"
325	209	lavalamp	2014-06-23 20:59:27	MEMBER	"The script doesn't modify any files, it only flags files, and it can be overridden. We're excluding third party dependencies already.
"
326	209	jkaplowitz	2014-06-23 21:03:33	CONTRIBUTOR	"SGTM
"
327	212	jkaplowitz	2014-06-23 22:10:58	CONTRIBUTOR	"The concept expressed in the pull request title LGTM as a useful fix. Leaving a proper code review to someone who knows Go (not me).
"
328	139	vishh	2014-06-23 23:48:33	MEMBER	"Is /run a tmpfs? There is an [outstanding PR](https://github.com/docker/libcontainer/pull/16) for this in libcontainer.
"
329	139	bgrant0607	2014-06-24 01:31:20	MEMBER	"Good call. Looks like not.

Here's df from a google/nodejs container:
Filesystem                                             1K-blocks    Used Available Use% Mounted on
rootfs                                                  10188088 1639712   8007808  17% /
none                                                    10188088 1639712   8007808  17% /
tmpfs                                                     304556       0    304556   0% /dev
shm                                                        65536       0     65536   0% /dev/shm
/dev/disk/by-uuid/485b0b37-5e5f-4878-85a4-2d8653315786  10188088 1639712   8007808  17% /.dockerinit
/dev/disk/by-uuid/485b0b37-5e5f-4878-85a4-2d8653315786  10188088 1639712   8007808  17% /etc/resolv.conf
/dev/disk/by-uuid/485b0b37-5e5f-4878-85a4-2d8653315786  10188088 1639712   8007808  17% /etc/hostname
/dev/disk/by-uuid/485b0b37-5e5f-4878-85a4-2d8653315786  10188088 1639712   8007808  17% /etc/hosts
/dev/disk/by-uuid/485b0b37-5e5f-4878-85a4-2d8653315786  10188088 1639712   8007808  17% /data
tmpfs                                                     304556       0    304556   0% /proc/kcore
"
330	213	brendandburns	2014-06-24 02:47:37	CONTRIBUTOR	"Generally LGTM, but integration tests are failing...

I think pulling it is ok, but if we see lots of flakes, we may need to change out policy.
"
331	205	brendandburns	2014-06-24 02:51:30	CONTRIBUTOR	"basically LGTM, two small nits.
"
332	214	lavalamp	2014-06-24 04:02:50	MEMBER	"Thanks for the change!
"
333	127	bgrant0607	2014-06-24 04:59:41	MEMBER	"FWIW, two people have recommended the Erlang supervisor model/spec:
http://www.erlang.org/doc/design_principles/sup_princ.html
http://www.erlang.org/doc/man/supervisor.html

IIUC, Erlang calls my ""any"" policy ""one for all"" and my ""all"" policy ""one for one"".
"
334	199	thockin	2014-06-24 05:07:17	MEMBER	"Internally we use RFC4122 UUIDs for identifying pods.  Any objections to making this part of the pod setup?  I guess it would really be a string (like ""id"") but with the strong suggestion that it be an encoded UUID.

Or we could use docker-style 256 bit randoms, but that might get confusing.

If we further lock down container names to RFC1035 labels, we can use <uuid>.<name> as the docker container name, which seems much nicer than the current dashes and underscores :)

What think?
"
335	213	lavalamp	2014-06-24 05:07:44	MEMBER	"Yeah, I think this is going to be too flaky. Will fix tomorrow.
"
336	216	thockin	2014-06-24 05:13:15	MEMBER	"updated
"
337	195	thockin	2014-06-24 05:15:01	MEMBER	"+1 - I'd advocate dropping the .sh so we have freedom to change it up when we need to grow beyond shell.
"
338	188	thockin	2014-06-24 05:16:02	MEMBER	"I've started a doc on this topic, but will be out of office about half of this week.
"
339	175	thockin	2014-06-24 05:17:34	MEMBER	"Interesting.  Let's consider how to expose this carefully.  I had an idea around exposing the existing --net=container through the API such that we could move to private IP per container and still have a clean APi for saying which mode people wanted per pod.  This ... does not fit my model :)
"
340	195	proppy	2014-06-24 05:19:10	CONTRIBUTOR	"did you consider having a `k8s` or `kube` singleton command with actions?
"
341	139	thockin	2014-06-24 05:20:36	MEMBER	"Is /run LSB compliant?
"
342	134	thockin	2014-06-24 05:23:11	MEMBER	"Is it possible to detect the top-most JSON object and swictch modes based on whether it is an array (of pods) or an object (single pod)?  I know this is getting sketchy...
"
343	134	lavalamp	2014-06-24 05:25:48	MEMBER	"See my pending pr which does this.
On Jun 23, 2014 10:23 PM, ""Tim Hockin"" notifications@github.com wrote:

> Is it possible to detect the top-most JSON object and swictch modes based
> on whether it is an array (of pods) or an object (single pod)? I know this
> is getting sketchy...
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/134#issuecomment-46933276
> .
"
344	134	proppy	2014-06-24 05:26:50	CONTRIBUTOR	"#213
"
345	97	thockin	2014-06-24 05:31:52	MEMBER	"From other discussion:

We have two structures - Volume and VolumeMount.  Volume is supposed to represent that ""what"" and VolumeMount is supposed to represent the ""Where"" and ""How"".

The ""what"" is, so far, ""a pod-private local directory"".  We want to add more kinds of ""what"" but we've stuck it (the host FS flag) in the ""where"" structure.  You'll notice that this can ONLY identity-mount things - there is no opportunity to remap host /foo onto container /bar (very similar to Docker's --volumes-from, which is broken in the same way).

A ""source"" field is the right direction, but it belongs in Volume, not VolumeMount.  Consider that a volume definition is pod-scope, but mounts are container-scope.  You want to spell out the source once, at pod-scope, and let each container mount it where they want.

This also lays groundwork to get fancier with permissions, storage medium, quota, etc.

I'm happy to go into my ideas here more, if you like.  We spent a lot of time on this area internally, and I think (hope!) we can apply some of those lessons here.  But I also want to hear from others if you all feel we're screwing it up.
"
346	15	thockin	2014-06-24 05:32:32	MEMBER	"Why did we disable IP tables?
"
347	139	bgrant0607	2014-06-24 05:36:05	MEMBER	"It will be once LSB is updated to FHS 3.0:
http://www.linuxbase.org/betaspecs/fhs/fhs.html#runRuntimeVariableData

http://askubuntu.com/questions/57297/why-has-var-run-been-migrated-to-run
"
348	217	brendandburns	2014-06-24 05:36:33	CONTRIBUTOR	"fwiw, I fixed the tests...
"
349	97	discordianfish	2014-06-24 10:43:54	CONTRIBUTOR	"@brendandburns What about extending volumes in Docker itself? There is current an issue for collecting general issues with volumes to discuss changes: https://github.com/dotcloud/docker/issues/6496
"
350	15	jbeda	2014-06-24 13:21:33	CONTRIBUTOR	"We needed to disable the blanket NAT for all traffic out of the bridge and only nat for `! 10.0.0.0/8`.  But the Docker flag for disable that also disabled the NAT for setting up iptables for incoming port maps.  Docker then falls back on user mode proxying of that traffic.

We either need to make Docker itself more flexible or start setting up the port forwarding in the kubelet.  If we do it in the kubelet we could do something like:
1. Set up the netns container for the pod
2. Set up the port forwards
3. Launch other containers for the pod

If we want to adapt Docker, it might be enough to split the iptables flag into a flag for configuring the iptables for the bridge in general vs. configuring iptables for the incoming port mapping.
"
351	195	jbeda	2014-06-24 13:24:20	CONTRIBUTOR	"@thockin To get rid of the .sh, see #194 

@proppy Single binary w/ subcommands, see #108.  I'd be okay with one server command and one client command.  But at least one single server binary would help speed things along and simplify deployments.
"
352	175	jbeda	2014-06-24 13:26:21	CONTRIBUTOR	"The proxy is special.  We could run it outside of docker or outside the kubelet, but ideally we'd manage it from the kubelet.

Or we could find a way to port map a range to it, but ideally we'd avoid NAT for all traffic through the proxy.
"
353	97	thockin	2014-06-24 14:33:07	MEMBER	"I am in favor of extending docker, in theory, but I am wary of how flexible
they want to make it.  I can imagine a lot of cool stuff modeled as
volumes, and I don't know how to handle it generically.  It seems way
simpler to say that docker can put a volume around anything you can mount
in the host.
On Jun 24, 2014 3:44 AM, ""discordianfish"" notifications@github.com wrote:

> @brendandburns https://github.com/brendandburns What about extending
> volumes in Docker itself? There is current an issue for collecting general
> issues with volumes to discuss changes: dotcloud/docker#6496
> https://github.com/dotcloud/docker/issues/6496
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/97#issuecomment-46956476
> .
"
354	15	thockin	2014-06-24 15:26:35	MEMBER	"I feel like a moron for not understanding the problem after that
explanation.  Why do we need to disable NAT?  I'm still digging into the
impl of Kubelet, so maybe it will emerge.

Aren't you on vacation?

On Tue, Jun 24, 2014 at 6:21 AM, Joe Beda notifications@github.com wrote:

> We needed to disable the blanket NAT for all traffic out of the bridge and
> only nat for ! 10.0.0.0/8. But the Docker flag for disable that also
> disabled the NAT for setting up iptables for incoming port maps. Docker
> then falls back on user mode proxying of that traffic.
> 
> We either need to make Docker itself more flexible or start setting up the
> port forwarding in the kubelet. If we do it in the kubelet we could do
> something like:
> 1. Set up the netns container for the pod
> 2. Set up the port forwards
> 3. Launch other containers for the pod
> 
> If we want to adapt Docker, it might be enough to split the iptables flag
> into a flag for configuring the iptables for the bridge in general vs.
> configuring iptables for the incoming port mapping.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/15#issuecomment-46969487
> .
"
355	15	brendandburns	2014-06-24 15:39:33	CONTRIBUTOR	"Joe is the expert on our config, but I think we needed to disable NAT to be
able to assign our own IP addresses.

Brendan
On Jun 24, 2014 8:26 AM, ""Tim Hockin"" notifications@github.com wrote:

> I feel like a moron for not understanding the problem after that
> explanation. Why do we need to disable NAT? I'm still digging into the
> impl of Kubelet, so maybe it will emerge.
> 
> Aren't you on vacation?
> 
> On Tue, Jun 24, 2014 at 6:21 AM, Joe Beda notifications@github.com
> wrote:
> 
> > We needed to disable the blanket NAT for all traffic out of the bridge
> > and
> > only nat for ! 10.0.0.0/8. But the Docker flag for disable that also
> > disabled the NAT for setting up iptables for incoming port maps. Docker
> > then falls back on user mode proxying of that traffic.
> > 
> > We either need to make Docker itself more flexible or start setting up
> > the
> > port forwarding in the kubelet. If we do it in the kubelet we could do
> > something like:
> > 1. Set up the netns container for the pod
> > 2. Set up the port forwards
> > 3. Launch other containers for the pod
> > 
> > If we want to adapt Docker, it might be enough to split the iptables flag
> > into a flag for configuring the iptables for the bridge in general vs.
> > configuring iptables for the incoming port mapping.
> > 
> > ## 
> > 
> > Reply to this email directly or view it on GitHub
> > <
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/15#issuecomment-46969487
> > 
> > .
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/15#issuecomment-46986391
> .
"
356	15	jbeda	2014-06-24 15:43:25	CONTRIBUTOR	"Quick clarification and then back to vacation :)

We want traffic between containers to use the container API across nodes.  Say we have `Node A` with a container IP space of `10.244.1.0/24` and `Node B` with a container IP space of `10.244.2.0/24`.  And we have `Container A1` at `10.244.1.1` and `Container B1` at `10.244.2.1`.  We want `Container A1` to talk to `Container B1` directly with no NAT.  `B1` should see the ""source"" in the IP packets of `10.244.1.1` -- not the ""primary"" IP for `Node A`.  That means that we want to turn off NAT for traffic between containers (and also between VMs and containers).

But we don't support (yet?) the extra container IPs that we've provisioned talking to the internet directly.  We don't map external IPs to the container IPs.  So we solve that problem by having traffic that isn't to the internal network (`! 10.0.0.0/8`) get NATed through the primary host IP address so that it can get 1:1 NATed by the GCE networking when talking to the internet.  Similarly, incoming traffic from the internet has to get NATed/proxied through the host IP.

So we end up with 3 cases:
1. `Container -> Container` or `Container <-> VM`.  These should use `10.` addresses directly and there should be no NAT.
2. `Container -> Internet`.  These have to get mapped to the primary host IP so that GCE knows how to egress that traffic.  There is actually 2 layers of NAT here: `Container IP -> Internal Host IP -> External Host IP`.  The first level happens in the guest with IP tables and the second happens as part of GCE networking.  The first one (`Container IP -> internal host IP`) does dynamic port allocation while the second maps ports 1:1.
3. `Internet -> Container`.  This also has to go through the primary host IP and also has 2 levels of NAT, ideally.  However, the path currently is a proxy with `(External Host IP -> Internal Host IP -> Docker) -> (Docker -> Container IP)`.  Once this bug is closed, it should be `External Host IP -> Internal Host IP -> Container IP`.  But to get that second arrow we have to set up the port forwarding iptables rules per mapped port.

If this is still confusing let me know and I can find some time for a hangout to dive into the details.
"
357	97	discordianfish	2014-06-24 16:23:51	CONTRIBUTOR	"@thockin That's up for discussion but I would be a vocal proponent of extending it and right now is the best time to discuss that. I see both at least using physical disks as well as network storage for volumes as very useful.
"
358	134	jjhuff	2014-06-24 16:54:37	CONTRIBUTOR	"Besides the backward compatibility side of things, is support for a single pod likely to be useful?  That'd essentially limit the kubelet to running only a single pod via http.
"
359	219	lavalamp	2014-06-24 17:00:45	MEMBER	"Nice catch.
"
360	205	lavalamp	2014-06-24 17:09:48	MEMBER	"Should be ready to merge now.
"
361	169	lavalamp	2014-06-24 17:38:56	MEMBER	"Let us know when the CLA happens. I'd love to merge this change :)
"
362	213	lavalamp	2014-06-24 17:44:02	MEMBER	"Hm, I just ran it half a dozen times locally with no flakes. Let's see if travis passes it this time.
"
363	220	jkaplowitz	2014-06-24 17:54:21	CONTRIBUTOR	"This is something I was planning to mention after the pace of this week
subsides. It's definitely a wise idea, both for our users and for whoever
takes on ongoing maintenance of the container-optimized image builds.

On Tue, Jun 24, 2014 at 10:50 AM, Daniel Smith notifications@github.com
wrote:

> How often do we make breaking changes, and what does it look like to
> upgrade a running kubernetes cluster? Should we start cutting releases,
> with tags and things?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/220.
"
364	213	lavalamp	2014-06-24 17:56:53	MEMBER	"OK, I'm clearly going to have to make a local copy to get this to work in Travis.
"
365	223	thockin	2014-06-24 18:03:48	MEMBER	"Dup of #175 
"
366	221	lavalamp	2014-06-24 18:04:42	MEMBER	"Our startup scripts redirect stderr/out to (e.g.) /var/log/kublet.log. I haven't actually investigated glog; will that still work as expected?
"
367	221	thockin	2014-06-24 18:12:15	MEMBER	"Nope - great point.  What do we want the behavior to be?  We can log
everything to stderr and redirect to /var/log/kubelet.log, or we can do it
glog-style and write logfiles to /var/log/kubelet/\* (kubelet.INFO, and so
on).

stderr+redirect will capture etcd and anyone else who uses log.\* so I think
I vote for that.

On Tue, Jun 24, 2014 at 11:04 AM, Daniel Smith notifications@github.com
wrote:

> Our startup scripts redirect stderr/out to (e.g.) /var/log/kublet.log. I
> haven't actually investigated glog; will that still work as expected?
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/221#issuecomment-47007596
> .
"
368	218	thockin	2014-06-24 18:17:04	MEMBER	"I stripped the Debugf change for now.  PTAL
"
369	221	lavalamp	2014-06-24 18:17:10	MEMBER	"Can we make a new log.Logger for etcd, and have it write to a /var/log/kublet/kubelet.ETCD?

I think that's nicer, but we may need to provide a flag so users can control the output directory. If that's difficult, then continuing the redirection is fine with me, too.
"
370	169	discordianfish	2014-06-24 18:25:20	CONTRIBUTOR	"@lavalamp We've signed and mailed the CLA last Friday
"
371	217	brendandburns	2014-06-24 18:26:27	CONTRIBUTOR	"Updated to use the new Client api.  PTAL.

Thanks!
--brendan
"
372	169	lavalamp	2014-06-24 18:28:58	MEMBER	"Ah, OK, great. I'll merge as soon as it shows up on our list.
"
373	199	jjhuff	2014-06-24 18:41:19	CONTRIBUTOR	"Both points sound great to me.  These encoded names are just plain ugly:)

Id is also a required field (network containers break otherwise), so it seems weird to have them marked as 'omitempty'. This is probably more an issue for config files than anything else.
"
374	217	lavalamp	2014-06-24 19:01:00	MEMBER	"I'm merging this to unblock Brendan. Nits can be fixed with another PR.
"
375	221	lavalamp	2014-06-24 19:03:01	MEMBER	"Brendan and I both merged a commit, so it looks like you'll have to rebase and do the find/replace again. Sorry... 
"
376	221	thockin	2014-06-24 19:30:11	MEMBER	"Do we really want to split the logs across files?  It's etcd for now, but
who know what else it will be in the future.  For better or worse, glog is
NOT the go logging standard.

On Tue, Jun 24, 2014 at 12:03 PM, Daniel Smith notifications@github.com
wrote:

> Brendan and I both merged a commit, so it looks like you'll have to rebase
> and do the find/replace again. Sorry...
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/221#issuecomment-47016063
> .
"
377	226	lavalamp	2014-06-24 20:16:48	MEMBER	"Thanks for the test!
"
378	227	lavalamp	2014-06-24 20:27:11	MEMBER	"Definitely an improvement. Thanks.
"
379	232	jjhuff	2014-06-24 22:18:59	CONTRIBUTOR	"Handy, I was just about to do this... Thanks!
"
380	213	lavalamp	2014-06-25 00:02:57	MEMBER	"YES! I finally got this to pass travis.

Before this change, integration wasn't testing kubelet. Now it at least verifies that it tries to start containers!
"
381	234	lavalamp	2014-06-25 00:21:57	MEMBER	"The GET command tacks on the hostname currently. I'm not sure if we want to promise that scheduling will be so instant that you always get a hostname immediately when you create a pod...
"
382	236	lavalamp	2014-06-25 00:30:59	MEMBER	"This is configurable in cluster/config-default.sh, but perhaps it makes sense to have them confirm the first time they run.
"
383	221	thockin	2014-06-25 00:32:43	MEMBER	"OK.  I have made it so that all logs go to glog now, even etcd.  I meant to rebase and then add that as an extra diff, but I messed it up, sorry.

This still needs to define log dirs per command.

PTAL @lavalamp 
"
384	239	brendandburns	2014-06-25 02:32:13	CONTRIBUTOR	"I don't think there is anything in flight that I know of.  Someone can always look at the history and merge them back in.
"
385	221	thockin	2014-06-25 03:29:44	MEMBER	"Fixed Forever() comment and rebased.
"
386	221	lavalamp	2014-06-25 03:44:26	MEMBER	"Just one more nit, also looks like a merge problem in integration.go.
"
387	239	lavalamp	2014-06-25 03:46:22	MEMBER	"Thanks for noticing this, I meant to have this in my previous PR but it totally slipped my mind.
"
388	221	thockin	2014-06-25 03:52:55	MEMBER	"OK, actually builds this time.
"
389	221	lavalamp	2014-06-25 04:01:15	MEMBER	"Thanks for the change!
"
390	221	thockin	2014-06-25 04:09:22	MEMBER	"Whoah, uhh, shouldn't we decide where logs should actually be going before
we commit this?  Right now they get written to /tmp, which seems like a
less than ideal answer.

I suggest we back this out post haste :)

On Tue, Jun 24, 2014 at 9:01 PM, Daniel Smith notifications@github.com
wrote:

> Thanks for the change!
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/221#issuecomment-47058366
> .
"
391	193	rafael	2014-06-25 04:35:11	CONTRIBUTOR	"+1
"
392	243	lavalamp	2014-06-25 05:13:57	MEMBER	"Thanks for the fix!
"
393	244	brendandburns	2014-06-25 05:31:49	CONTRIBUTOR	"Oops, this isn't ready for merge, I need to add a filter.  I'll do that tomorrow, and ping the PR when ready.
"
394	222	smarterclayton	2014-06-25 15:12:13	CONTRIBUTOR	"Should it be the interface, or an abstract network interface type?  Are interfaces better represented by ""internal, external, fast, private"" or by ""eth3"" which might be different on every minion?  Is it reasonable to expect all minions to have the same named interface?
"
395	222	thockin	2014-06-25 16:28:05	MEMBER	"There's a natural tension between people who want to schedule with some
understanding of the underlying hardware and people who want to schedule
abstractly.

IMO we should encourage the latter and discourage the former, but provide
escape hatches to do more ""advanced"" stuff.  I don't think it will be the
normal case for people to want to bind to specific interfaces of the
machine.  It should be possible, but I don't think it is the path with the
smoothest surface.

I acknowledge that this note was not helpful in deciding how to do it. :)

On Wed, Jun 25, 2014 at 8:12 AM, Clayton Coleman notifications@github.com
wrote:

> Should it be the interface, or an abstract network interface type? Are
> interfaces better represented by ""internal, external, fast, private"" or by
> ""eth3"" which might be different on every minion? Is it reasonable to expect
> all minions to have the same named interface?
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/222#issuecomment-47114507
> .
"
396	222	smarterclayton	2014-06-25 16:56:36	CONTRIBUTOR	":)  I strongly agree on the ""encourage abstraction, allow escape hatch"" duality here.

Some concrete use cases we've discussed in relation to selecting interfaces
- Deploy a replicated service that wants to use a separate, ""internal/private"" network interface to route cluster traffic.  I.e. mongodb in replica set mode, where the containers in the replica set want to communicate over a private network configured on a specific interface
- As an app deployer, allow choosing to expose ports on an interface that is internal to the network/security group that minions are on, or alternatively expose it on an interface that is reachable by a larger network (other Kube clusters, intranet, internet)
- As a Kube cluster admin, configure a loadbalancer/proxy (Kube proxy, HAProxy functioning in the same role) that is on a specific, reserved device on a set of machines.  If the interface isn't available, that's likely to be an error condition (an input to scheduling)

It's also worth distinguishing between the cluster-owner and tenant-user roles - the former has access to make cluster wide decisions and configure minions in very specific ways, the latter only has access to options exposed by the Kubernetes API
"
397	246	jjhuff	2014-06-25 17:46:49	CONTRIBUTOR	"The kubelet already supports reading config from a file, does that work?
"
398	246	lavalamp	2014-06-25 17:50:19	MEMBER	"Yeah, I think the work to do here is:

1) Finish up jbeda's work to dockerize all our components.
2) Adjust our startup scripts to use kubelet to launch apiserver/replication-controller via the config file.
"
399	127	smarterclayton	2014-06-25 19:12:20	CONTRIBUTOR	"How would an API client know that the individual container ""succeeded""? (for a definition of success)
"
400	247	lavalamp	2014-06-25 19:27:05	MEMBER	"Thanks for the change!

It may be time to remove localkube. We can always find it in the history if we decide to move to a single binary system-- localkube would be a good starting point for that.
"
401	247	jjhuff	2014-06-25 19:42:50	CONTRIBUTOR	"Cool. I can do the removal PR later today.
On Jun 25, 2014 12:27 PM, ""Daniel Smith"" notifications@github.com wrote:

> Thanks for the change!
> 
> It may be time to remove localkube. We can always find it in the history
> if we decide to move to a single binary system-- localkube would be a good
> starting point for that.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/247#issuecomment-47146901
> .
"
402	127	bgrant0607	2014-06-25 19:46:39	MEMBER	"@smarterclayton If you're asking about how Kubernetes will detect successful termination, we need machine-friendly, actionable status codes from Docker and from libcontainer (https://github.com/GoogleCloudPlatform/kubernetes/issues/137). Every process management and workflow orchestration system in the universe is going to need that. Normal termination with exit code 0 should indicate success.

If you're asking how Kubernetes's clients would detect termination, they could poll the currentState of the pod. We don't really have per-container status information there yet -- we'd need to add that. A Watch API would be better than polling -- that's worth an issue of its own. We could also provide a special status communication mechanism to containers and/or to event hooks (e.g., tmpfs file or environment variable). On top of these primitives, we could build a library and command-line operation to wait for termination and return the status.
"
403	127	aspyker	2014-06-25 19:47:51	NONE	"ran into this for Acme Air for case # 3 (run-once) for initial database loader processes
"
404	127	smarterclayton	2014-06-25 20:13:34	CONTRIBUTOR	"@bgrant0607 Probably fair to restate my question as whether you have a model in mind (based on previous experience at Google) that defines what you feel is a scalable and reliable mechanism for communicating fault and exit information to an API consumer - for instance, to implement run-once or run-at-least-once containers that are expected to exit and not restart as aspyker mentioned.

For instance, Mesos defines a protocol between master and slave that attempts to provide some level of guarantees for communicating the exit status, subject to the same limitations you noted above about not being truly deterministic.  That model assumes bidirectional communication between master and slave, which Kubernetes does not.

Agree that watch from client->slave or client->master->slave is better than polling, although it seems more difficult to scale the master when the number of watchers+tasks grows.  Do you see the master recording exit status for run-once containers in a central store, or that being a separate subsystem that could scale orthogonally to the api server / replication server and aggregate events generated by the minions?  I could imagine that transient failures of containers with a ""restart-always"" policy would be useful to know to an api consumer - to be able to see that container X restarted at time T1, T2, and T3.
"
405	127	bgrant0607	2014-06-25 23:43:09	MEMBER	"@smarterclayton First, I think the master should delegate the basic restart policy to the slave: always restart, restart on failure, never restart. The master should only handle cross-node restarts directly. And, yes, the master should pull status from the slaves and store it (#156), as well as periodically check their health (#193). As scaling demands grow, that responsibility could indeed be split out to a separate component or set of components.

Reason for last termination (#137), termination message from the application (#139), time of last termination, and number of terminations should be among the information collected. State of terminated containers/pods should be kept around on the slaves long enough for the master to observe it the vast majority of the time (e.g., 10x the normal polling interval, or 2x the period after which an unresponsive node would be considered failed, anyway; explicit decoupling of stop vs. delete would also enable the master to control deletion of observed terminated containers/pods), but the master would record unobserved terminations as having failed, ideally with as much specificity as possible about what happened (node was unresponsive, node failed, etc.). A monotonically increasing count of restarts could be converted to approximate recency, sliding-window counts, rates, and other useful information by continuous observers. A means of setting or resetting the count is sometimes useful, but non-essential. Termination events could also be buffered (in a bounded-sized ring buffer with an event sequence number) and streamed off the slave and logged for debugging, but shouldn't be necessary for correctness, since events could always be lost.

Reasons for system-initiated container stoppages (e.g., due to liveness probe failures - #66) should be explicitly recorded (#137), but can be treated as failures with respect to restart policy. User-initiated cancellation should override the restart policy, as should user-initiated restarts (#159).

With more comprehensive failure information from libcontainer and Docker we could distinguish container setup errors from later-stage execution failures, but if in doubt, the slave (and master) should be conservative about not restarting ""run once"" containers that may have started execution.

Containers should have unique identifiers so the system doesn't confuse different instances or incarnations (#199).

Overall system architecture for availability, scalability, fault tolerance, etc. should be discussed elsewhere.
"
406	251	lavalamp	2014-06-26 00:20:49	MEMBER	"I'm not on a mac at the moment so can't test, but does http://brew.sh/ allow you to install `htpasswd`?
"
407	251	dgageot	2014-06-26 00:22:47	NONE	"No. I'm investigating...
"
408	251	dgageot	2014-06-26 00:34:34	NONE	"Definitely not in homebrew
"
409	251	lavalamp	2014-06-26 00:38:43	MEMBER	"Was mentioned in IRC a while ago, but no resolution yet. Thanks for filing the issue.

https://botbot.me/freenode/google-containers/msg/16261657/
"
410	251	dgageot	2014-06-26 01:08:52	NONE	"I'm trying to use a [docker container](https://registry.hub.docker.com/u/dgageot/htpasswd/) as a workaround. Will tell you if it works. 
"
411	199	smarterclayton	2014-06-26 01:35:30	CONTRIBUTOR	"Was going to get familiar and try to fix this - sounds like the suggestion is to add ""ID string"" to Container, and fail PodRegistryStorage.Create() if Container.ID is empty?  Or should PodRegistryStorage.Create() populate unset DesiredState.Manifest.Containers[].ID that are empty?  Latter seems more flexible (server controls default UUID generation for clients)
"
412	251	dgageot	2014-06-26 01:43:24	NONE	"OK. It works. Here's the commit that uses docker to run htpasswd inside a docker https://github.com/dgageot/kubernetes/commit/231048477876623a02f4a5ffddda908612e8d281 

I'm not sure this is something that could be used by other than me. The user needs to have docker installed and my docker image has to be trusted.

Anyway it helped me!
"
413	199	thockin	2014-06-26 02:08:52	MEMBER	"I started in on some validation logic this morning.

It's not clear whether unique ID is something users should have to spec or
not.  I lean towards not.  That means the master (api server?) Has to
generate a uuid upon acceptance.  That uuid would have to flow down to
Kubelet.

Is that in the same vein as you were thinking?
On Jun 25, 2014 6:35 PM, ""Clayton Coleman"" notifications@github.com wrote:

> Was going to get familiar and try to fix this - sounds like the suggestion
> is to add ""ID string"" to Container, and fail PodRegistryStorage.Create() if
> Container.ID is empty? Or should PodRegistryStorage.Create() populate unset
> DesiredState.Manifest.Containers[].ID that are empty?
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/199#issuecomment-47179178
> .
"
414	199	smarterclayton	2014-06-26 02:31:17	CONTRIBUTOR	"As an API consumer I like not having to specify things that the server can do for me - having to generate a UUID on the command line to curl a new pod into existence feels wrong.  I started [here](https://github.com/smarterclayton/kubernetes/commit/984334f42dfcb470a6add005613330477108a146#diff-89731e89e31105da32e30576a5939fb6R149) but didn't pass down to kubelet yet.
"
415	199	vmarmol	2014-06-26 02:38:14	CONTRIBUTOR	"One thing to note with global unique identifier is static containers. Today
we setup cAdvisor as a static container and have no way to assign it a
unique ID globally.

On Wed, Jun 25, 2014 at 7:31 PM, Clayton Coleman notifications@github.com
wrote:

> As an API consumer I like not having to specify things that the server can
> do for me - having to generate a UUID on the command line to curl a new pod
> into existence feels wrong. I started here
> https://github.com/smarterclayton/kubernetes/commit/984334f42dfcb470a6add005613330477108a146#diff-89731e89e31105da32e30576a5939fb6R149
> but didn't pass down to kubelet yet.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/199#issuecomment-47181803
> .
"
416	199	smarterclayton	2014-06-26 02:43:09	CONTRIBUTOR	"Static as in ""defined on each host via a config file""?  Would it make sense for the Kubelet to auto assign a UUID for containers pulled from files based on the host MAC and the position in the file (or a SHA1 of the contents of the manifest plus the host MAC)?
"
417	199	thockin	2014-06-26 02:46:35	MEMBER	"Are you setting it up by config file?

Can we generate a new uuid when we write the config file, or do we want it
to be identical across machines (google prod style)?

We cod do something like: if Kubelet finds a config without a uuid, it will
assign it a uuid and log it.

Internally we go one step further and define ""master space"" where each
configuration originator can choose an ID and then manage ids within that
space, meaning the master doesn't have to generate a UUID at all just a
unique masterspace + pod name.

Do we need to go that far?  Uuids are notoriously human-hostile.
On Jun 25, 2014 7:38 PM, ""Victor Marmol"" notifications@github.com wrote:

> One thing to note with global unique identifier is static containers.
> Today
> we setup cAdvisor as a static container and have no way to assign it a
> unique ID globally.
> 
> On Wed, Jun 25, 2014 at 7:31 PM, Clayton Coleman notifications@github.com
> 
> wrote:
> 
> > As an API consumer I like not having to specify things that the server
> > can
> > do for me - having to generate a UUID on the command line to curl a new
> > pod
> > into existence feels wrong. I started here
> > <
> > https://github.com/smarterclayton/kubernetes/commit/984334f42dfcb470a6add005613330477108a146#diff-89731e89e31105da32e30576a5939fb6R149>
> > 
> > but didn't pass down to kubelet yet.
> > 
> > ## 
> > 
> > Reply to this email directly or view it on GitHub
> > <
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/199#issuecomment-47181803>
> > 
> > .
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/199#issuecomment-47182140
> .
"
418	199	vmarmol	2014-06-26 02:48:20	CONTRIBUTOR	"Yes, it is a static file we leave on the machine. I don't think we have the
ability to assign it a unique ID (outside of running some custom init
script). Having the Kubelet assign it a UUID seems reasonable to me.

On Wed, Jun 25, 2014 at 7:43 PM, Clayton Coleman notifications@github.com
wrote:

> Static as in ""defined on each host via a config file""? Would it make sense
> for the Kubelet to auto assign a UUID for containers pulled from files
> based on the host MAC and the position in the file (or a SHA1 of the
> contents of the manifest plus the host MAC)?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/199#issuecomment-47182360
> .
"
419	199	thockin	2014-06-26 02:50:45	MEMBER	"To be clearer on masterspace.  Each new pod gets fields such as:

Masterspace: kubernetes.google.com
Name: ""a name unique within this masterspace"".  I am sultaneously trying to
argue that new should be DNS compatible, which lits them to 64 lowercase
alphanums, and gives names semantic meaning.  Making then bad as unique
IDs.  Need to think more about this...
On Jun 25, 2014 7:46 PM, ""Tim Hockin"" thockin@google.com wrote:

> Are you setting it up by config file?
> 
> Can we generate a new uuid when we write the config file, or do we want it
> to be identical across machines (google prod style)?
> 
> We cod do something like: if Kubelet finds a config without a uuid, it
> will assign it a uuid and log it.
> 
> Internally we go one step further and define ""master space"" where each
> configuration originator can choose an ID and then manage ids within that
> space, meaning the master doesn't have to generate a UUID at all just a
> unique masterspace + pod name.
> 
> Do we need to go that far?  Uuids are notoriously human-hostile.
> On Jun 25, 2014 7:38 PM, ""Victor Marmol"" notifications@github.com wrote:
> 
> > One thing to note with global unique identifier is static containers.
> > Today
> > we setup cAdvisor as a static container and have no way to assign it a
> > unique ID globally.
> > 
> > On Wed, Jun 25, 2014 at 7:31 PM, Clayton Coleman <
> > notifications@github.com>
> > wrote:
> > 
> > > As an API consumer I like not having to specify things that the server
> > > can
> > > do for me - having to generate a UUID on the command line to curl a new
> > > pod
> > > into existence feels wrong. I started here
> > > <
> > > https://github.com/smarterclayton/kubernetes/commit/984334f42dfcb470a6add005613330477108a146#diff-89731e89e31105da32e30576a5939fb6R149>
> > > 
> > > but didn't pass down to kubelet yet.
> > > 
> > > ## 
> > > 
> > > Reply to this email directly or view it on GitHub
> > > <
> > > https://github.com/GoogleCloudPlatform/kubernetes/issues/199#issuecomment-47181803>
> > > 
> > > .
> > 
> > ## 
> > 
> > Reply to this email directly or view it on GitHub
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/199#issuecomment-47182140
> > .
"
420	199	lavalamp	2014-06-26 03:09:05	MEMBER	"I want the container names from the different sources to have different namespaces, so there won't be any collisions. E.g., kubelet prepends/appends "".etcd"" (or something) to etcd-sourced containers, "".cfg"" to containers from the config file, "".http"" to containers from the manifest url, etc. Then it is up to each source to stay unique.

Api server can stay unique via uuid or counting up. Config files & manifest url stay unique by humans not screwing up; kubelet rejects them otherwise.

This has the nice effect that the container name produced from a config file is predictable without having to do a lookup. This would be good for our own container vm image and anything like it.
"
421	253	lavalamp	2014-06-26 03:16:06	MEMBER	"@jjhuff is working on fixing up the kubelet's container naming scheme, which is the other half of this. See: https://github.com/jjhuff/kubernetes/commit/ff131a93170e96733fc63185780bf7de18045bc2
"
422	199	thockin	2014-06-26 03:50:38	MEMBER	"This started structured and turned into a stream0-of-consiousness, sorry.

I agree that unique names are desirable, but I'm not sure ""etcd"" and
""httpd"" are sufficient.

The reason we have masterspace internally is because we also need to attach
metadata about the master that created a pod (e.g. which cluster it thought
it was in, what magic number it had (generation number) and so on).

Here's what I think I have convinced myself of, so far.

1) All pods have a required name string, which is human-friendly and DNS
friendly (probably rfc1035 subdomain rather than label).

2) All pods which are running have a uuid (suggest RFC4122 but could be
something else).  This of this as a cluster-wide PID.  It has no semantic
meaning and changes whenever a pod is started on a new host.

3) The kubelet API includes this uuid.  If the uuid field is not specified,
the kubelet will assign a new UUID.

This _does not_ have the property that Daniel wants - predictable container
names.  The problem is that you putting semantic meaning into the unique
ID.  There's a reason that databse best practices involve surrogate keys
and that UNIX syscalls operate on PIDs rather than command names.  Consider
what happens if we get a phantom instance on a split network - both pods
end up with the same name - not unique any more.

That said, I could maybe be convinced.  If we put the rule that the pod
name had to be unique within a masterspace, we could sort of punt the
problem a bit, for now.

E.g.

Pod {
  masterspace = ""k8s.mydomain.com""
  id = ""id8675309.tims-pod""
  containers [
    {
      name = ""apache""

...would be created with container name
apache.id8675309.tims-pod.k8s.mydomain.com

If the apiserver did not care about phantoms, it could leave off the
id8675309 noise.  for Google people, this should look very familiar.

I still have a vague foreboding about making the uniqueness be the master's
problem, but it would get rid of the need for opaque UUIDs.  Or would it?
 We need to handle static pods (e.g. cAdvisor).  If they are allowed to
collide, then some hyopthetical cluster-data aggregator can not use ID as
primary key, and we will need to disambiguate queries by hostname.  Blech.

What if the rule is that masterspace is optional.  If not specified,
kubelet will make something up derived from the source.  So cAdvsor would
say:

Pod {
  #no masterspace
  id = ""cadvisor""
  containers [
    {
      name = ""cadvisor""

...would be created with container name cadvisor.cadvisor.file.<host_fqdn>

This still sort of sucks in that you can't aggregate by masterspace, e.g.
""give me a list of all containers in the cadvisor masterspace"".  But maybe
that's better served by labels anyway.  Yes, I think so.

Thoughts?  I could go wither way (UUIDs or masterspace + unique name).  Or
does someone have other ideas?  I feel like this got complicated pretty
fast.

Tim

On Wed, Jun 25, 2014 at 8:09 PM, Daniel Smith notifications@github.com
wrote:

> I want the container names from the different sources to have different
> namespaces, so there won't be any collisions. E.g., kubelet
> prepends/appends "".etcd"" (or something) to etcd-sourced containers, "".cfg""
> to containers from the config file, "".http"" to containers from the manifest
> url, etc. Then it is up to each source to stay unique.
> 
> Api server can stay unique via uuid or counting up. Config files &
> manifest url stay unique by humans not screwing up; kubelet rejects them
> otherwise.
> 
> This has the nice effect that the container name produced from a config
> file is predictable without having to do a lookup. This would be good for
> our own container vm image and anything like it.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/199#issuecomment-47183465
> .
"
423	245	thockin	2014-06-26 04:12:07	MEMBER	"Can we detail places where we're not currently compatible?
"
424	199	lavalamp	2014-06-26 05:12:47	MEMBER	"Just to be clear, I only think that the property of predictable names is desirable for containers that come from the manifest url and maybe the container, because without that property, `docker ps` is unhelpful. But you've got me somewhat convinced that maybe I shouldn't care so much about that.

I don't know if we want to wait to solve naming in general before we accept a better solution than what we do currently.
"
425	199	thockin	2014-06-26 05:20:47	MEMBER	"I could put my weight behind either approach.  The problem with UUIDs (be
they RFC4122 or SHA or whatever) is that they suck for humans.  The problem
with !UUIDs is that we have to count on the ""master"" to get uniqueness
right.

On Wed, Jun 25, 2014 at 10:12 PM, Daniel Smith notifications@github.com
wrote:

> Just to be clear, I only think that the property of predictable names is
> desirable for containers that come from the manifest url and maybe the
> container, because without that property, docker ps is unhelpful. But
> you've got me somewhat convinced that maybe I shouldn't care so much about
> that.
> 
> I don't know if we want to wait to solve naming in general before we
> accept a better solution than what we do currently.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/199#issuecomment-47188351
> .
"
426	199	jjhuff	2014-06-26 05:32:27	CONTRIBUTOR	"""I feel like this got complicated pretty fast."" Yup:)

[I think I've read & parsed all of the comments, but I could be mistaken]

I like the idea of DNS-styled hierarchical namespaces. I'm not super worried about multiple masters scheduling over the same cluster of minions, but it does make multi-tenant masters easier (i.e. a provider is running a master, and customers run their minions). It's also something that people are used to.

I think it's also reasonable to require that users produce unique names for managing in the system. 

Machine-generated unique IDs are more useful for running containers (i.e. a global PID). Having this be globally unique is _really_ handy for things like log aggregation, etc. Why not always have the kubelet generate that? The master can learn it when it lists the running containers. 

It's worth noting that the only restriction of docker names is that they are unique to the host. We can still encode useful human data (manifest+container names) in addition to the unique ID into the name...even if we only parse out the unique ID!

FWIW, I'm just polishing my ID cleanups to remove dead (or dieing) code. My goal was to normalize on docker IDs for many/most things inside the kubelet. I should have it out for review tomorrow.
"
427	199	jjhuff	2014-06-26 06:07:19	CONTRIBUTOR	"Also, clarifying some terminology might be handy.  Here's how I think of things (could easily be wrong!):
- `ContainerManifest` is a collection of `Containers`. 
- `Pods` are an instance of a `ContainerManifest`. They can be running (or not). They are assigned to a host.

Things that confuse me:
- `ContainerManifests` have an Id. `Containers` have a name. `Pods` have an Id. Seems like the manifest should only have a name (but this gets to the discussion above about names vs ids).
- `ReplicationController` has a `PodTemplate` which really just wraps `ContainerManifest`. Seems like overkill.
"
428	199	thockin	2014-06-26 06:22:03	MEMBER	"On Wed, Jun 25, 2014 at 10:32 PM, Justin Huff notifications@github.com wrote:

> ""I feel like this got complicated pretty fast."" Yup:)
> 
> [I think I've read & parsed all of the comments, but I could be mistaken]
> 
> I like the idea of DNS-styled hierarchical namespaces. I'm not super worried about multiple masters scheduling over the same cluster of minions, but it does make multi-tenant masters easier (i.e. a provider is running a master, and customers run their minions). It's also something that people are used to.

I'm more worried about ""masters"" that are config files crafted by
humans without coordination.  Those should not accidentally collide :)

> I think it's also reasonable to require that users produce unique names for managing in the system.

Here ""users"" == apiserver?

> Machine-generated unique IDs are more useful for running containers (i.e. a global PID). Having this be globally unique is really handy for things like log aggregation, etc. Why not always have the kubelet generate that? The master can learn it when it lists the running containers.

Yeah, this is doable (and is in fact closer to what we do internally)

> It's worth noting that the only restriction of docker names is that they are unique to the host. We can still encode useful human data (manifest+container names) in addition to the unique ID into the name...even if we only parse out the unique ID!

I think it is useful, but not critical that docker-reported names be
human-friendly.

> FWIW, I'm just polishing my ID cleanups to remove dead (or dieing) code. My goal was to normalize on docker IDs for many/most things inside the kubelet. I should have it out for review tomorrow.

I'm keenly interested in this, and I'd like to stabilize it all ASAP.
Please do not merge this change until I have had time to feedback on
it.  Caveat: I am part-time the rest of this week.

> ## 
> 
> Reply to this email directly or view it on GitHub.
"
429	199	thockin	2014-06-26 06:27:09	MEMBER	"On Wed, Jun 25, 2014 at 11:07 PM, Justin Huff notifications@github.com wrote:

> Also, clarifying some terminology might be handy. Here's how I think of things (could easily be wrong!):
> 
> ContainerManifest is a collection of Containers.
> Pods are an instance of a ContainerManifest. They can be running (or not). They are assigned to a host.

The names as defined in pkg/api are somewhat confusing.  It all
depends on perspective.  We really have two APIs here (user -> master;
master -> kubelet), and we're cramming the spec into one file.  We
should consider whether we really want to do that.

From Kubelet's POV ContainerManifest == Pod.

> Things that confuse me:
> 
> ContainerManifests have an Id. Containers have a name. Pods have an Id. Seems like the manifest should only have a name (but this gets to the discussion above about names vs ids).

Yes, let's settle the discussion about UUIDs vs Names before we rename
things.  and then lets rename things.

> ReplicationController has a PodTemplate which really just wraps ContainerManifest. Seems like overkill.

I think this was to leave room for growth.  Kubelet should do the same
and define a Pod type that has an api.ContainerManifest and maybe the
UUID (if we do that).

BTW: I have a change pending (not sent yet) to do validation of a
ContainerManifest - so it will reject manifests that, for example, do
not have an ID.  The core of it is done, but it needs a test and has
some open questions.  Hope to have it out tomorrow.

> ## 
> 
> Reply to this email directly or view it on GitHub.
"
430	199	bgrant0607	2014-06-26 06:54:09	MEMBER	"Whew! This is long. This issue started with no context about what we're trying to do.

Starting with the end: Yes, we should clean up the identifiers. Currently, every object/resource includes JSONBase, which has an ID (which probably should be Id). ContainerManifest also has an Id. Container has Name. Port has Name. I'll point out that Container and Port are not standalone resources right now. ContainerManifest is the way it is due to compatibility with the container-vm release.

What are we trying to do? I saw several things mentioned in this issue:
- Human-friendly, memorable, semantically meaningful, short-ish references in operations
- Predictable references in operations and/or configuration files
- Unambiguous references in space, such as pods/containers created by multiple masters or by static configuration
- Unambiguous references in time, such as for logging
- Idempotent creation (#148) 
- Auto-generation of DNS names (#146)
- Identification of sets for aggregation

Both unique identifiers and human-friendly names have value. Human-friendly names can only be unique in space, not in time. We should use ""Id"" for the former and ""Name"" for the latter. Names could be used to ensure idempotent (at most once) creation, though a non-indexed resource value could be used for that, also. 

The replicationController would treat ""Name"" in the template as a prefix and would append relatively short random numbers for uniqueness. Static pods could be treated similarly.

Label selectors should be used for set formation / aggregation.

If users are providing Names, we could also use them for DNS (#146). I'd use ""domain"" rather than ""masterspace"".

Argh, my laptop needs to be rebooted...
"
431	199	bgrant0607	2014-06-26 08:01:27	MEMBER	"Continuing...

I agree we want a mechanism that permits unique id allocation by Kubelets and that doesn't require centralized and/or persistent state. I like UUIDs. There are the issues of ensuring unique MAC addresses in VMs and/or namespaces, and determinism for testing, but I think we've found ways around these issues. 

I've considered not having human-friendly names for pods before and just using labels instead. Labels are predictable and human-friendly, but don't require uniqueness, and don't require concatenating lots of identifying info together in order to ensure uniqueness, which users WILL do for names (and they'll want to parameterize them). They aren't short, though. Also, I guess part of the problem is that Docker doesn't support labels. We should push on that.

Idempotence could be ensured by a client-generated cookie, such as a fingerprint/hash or PR number. It wouldn't be required for static configs.

DNS names for instances aren't super-useful if they aren't predictable and DNS-like human-friendly names aren't that friendly if they are long. Short nicknames don't have to be predictable (so they could have a short uniquifying suffix) and don't even need to be semantically relevant -- just memorable (hence Docker's silly auto-generated names, I guess).

Services need predictable DNS names, OTOH, and ports need predictable names (for DNS SRV lookup or ENV vars or whatever), and pod-relative hostnames of containers should be predictable, so they can communicate with each other, though I don't know that they actually need to be FQDNs. Other types of services (e.g., master-elected services) and groups will need predictable DNS names, also.

Is the main motivation for names for pods to be consistent across all resource types? If so, I could buy into DNS-like hierarchical names for them. I'd use something like ""domain"" or ""namespace"" instead of ""masterspace"". 
"
432	199	smarterclayton	2014-06-26 14:26:39	CONTRIBUTOR	"Pod-relative hostnames and stable internal names are definitely valuable - and limiting ""name"" to rfc1035 subdomain has been extremely valuable in practice to us on OpenShift.  

> The replicationController would treat ""Name"" in the template as a prefix and would append relatively short random numbers for uniqueness. Static pods could be treated similarly.

Quasi-uniqueness I assume?

> Other types of services (e.g., master-elected services) and groups will need predictable DNS names, also.

How predictable?  As a concrete example, with something like Zookeeper you need the container to have an identifier/name that is stable across restarts / reschedules in a shared config (so not a pod ID).  I'd assumed you'd model this with a set of replication controllers (vs a shared controller) so you had 3 replication controllers with 1 item each, and you'd be able to either set an ENV per pod, or use the name in order to apply that.  If name changes over pod instances that rules out the reuse there.
"
433	199	lavalamp	2014-06-26 16:26:28	MEMBER	"One last thought from me; it occurs to me that docker already generates a long container id. Perhaps we can consider using that directly as our spatial/temporal unique identifier, and use this hypothetical dns-style solution as the friendly human name. We'd need to investigate just how unique docker's id's are, and we may not want to depend on docker for that, but it would reduce the number of IDs needed.

Also, as a footnote, if we end up with both pod.ID and Manifest.ID, IMO they should be the same identifier.
"
434	199	jjhuff	2014-06-26 16:37:47	CONTRIBUTOR	"> One last thought from me; it occurs to me that docker already generates a
> long container id. Perhaps we can consider using that directly as our
> spatial/temporal unique identifier, and use this hypothetical dns-style
> solution as the friendly human name. We'd need to investigate just how
> unique docker's id's are, and we may not want to depend on docker for that,
> but it would reduce the number of IDs needed.
> 
> Yes, I was starting to think the same thing.
> 
> Also, as a footnote, if we end up with both pod.ID and Manifest.ID, IMO
> they should be the same identifier.
> 
> My only concern here is that starts to feel weird when a Manifest is used
> as part of a ReplicationController. I think. You end up with N pods and a
> Manifest each with ID fields.
"
435	199	lavalamp	2014-06-26 16:46:14	MEMBER	"For clarity, let me suggest the convention that ""name"" means a friendly mostly human readable string, possibly in the style of dns names, and that ""id"" means an opaque, machine generated identifier, guaranteed unique at some level of resolution. Maybe everyone except for me is already using this convention. :) But I want to talk about ids and names in general without referring to a particular implementation.

> My only concern here is that starts to feel weird when a Manifest is used as part of a ReplicationController. I think. You end up with N pods and a Manifest each with ID fields.

I think, in that case, the rep. controller itself has a name, which it can use to make names for the pods it creates (prepend/append indices or something). IDs for the pods could still be generated by the apisever or wherever we decide they need to be generated. I was trying to say that since there's a 1:1 relationship between pods and manifests, we shouldn't make different IDs for each.
"
436	233	brendandburns	2014-06-26 17:26:06	CONTRIBUTOR	"ok, migrated to http.FileServer
"
437	253	jjhuff	2014-06-26 18:55:08	CONTRIBUTOR	"My internal Id cleanup is in #254 
"
438	199	thockin	2014-06-26 19:00:51	MEMBER	"Trying to collect thoughts and ideas into a proposal.  It's tricky
considering it from all angles (users, apiserver, replication controllers,
kubelet).  My background is node-centric, so I am counting on people to
smack me if I say something dumb for higher-levels :)

I have to run out right now, but I'm hoping we can distill the discussion
into a design O(soon).

NB: We spec names as RFC 1035 compatible, though we might extend that to
allow purely numeric tokens (e.g. 123.something.com) but I forget which RFC
that is.  Docker names allow underscores, which kubelet reserves for use in
infrastructure containers.

From kubelet's point of view:

1) All pods have a namespace string, which is human-friendly and DNS
friendly (rfc1035 subdomain fragment).  For example: ""k8s.mydomain.com"".
 This is used to indicate the provenance of the pod.  If the namespace is
not specified when creating a pod, kubelet will assign a namespace derived
from the source.  For example, a file ""/etc/k8s/cadvsor.pod"" on host ""
xyz123.mydomain.com"" might get masterspace ""
file-f5sxiyzpnm4hgl3dmfshm2ltn5zc44dpmqfa.xyz123.mydomain.com"" (the big
mess is a base32 encoding of the path).

2) All pods have a name string, which is human-friendly and DNS friendly
(rfc1035 subdomain fragment).  For example: ""id8675309.myjob"".  These names
are typically NOT user-provided, but are generated from infrastructure.
 For example, if the user asked for a job named ""myjob"" the apiserver must
uniquify that into a pod name.

3) The namespace + name pair is assumed to be unique.  This provides simple
idempotency, for example when re-reading a config file.

Open: Do we need UUIDs at all?  For what purpose?  The only argument I can
come up with is to protect against accidentally non-unique names at cluster
scope (e.g. in a giant database of namespace, name, uuid, stats) you could
disambiguate colliding namespace+names pairs with uuid.  Other
justifications?

From the apiserver's point of view:

1) The apiserver has a configured namespace.

2) All incoming pods have a name.  I don't know what the formatting and
uniqueness rules are here.

3) Upon acceptance, a pod is assigned a unique ID of some sort.

4) Upon assignment to a minion, the name and unique ID become the pod name.

Open: Should the unique ID persist if the pod is moved to a new minion?
 Maybe we need two IDs - one that sticks across moves and one that does not?

On Thu, Jun 26, 2014 at 9:46 AM, Daniel Smith notifications@github.com
wrote:

> For clarity, let me suggest the convention that ""name"" means a friendly
> mostly human readable string, possibly in the style of dns names, and that
> ""id"" means an opaque, machine generated identifier, guaranteed unique at
> some level of resolution. Maybe everyone except for me is already using
> this convention. :) But I want to talk about ids and names in general
> without referring to a particular implementation.
> 
> My only concern here is that starts to feel weird when a Manifest is used
> as part of a ReplicationController. I think. You end up with N pods and a
> Manifest each with ID fields.
> 
> I think, in that case, the rep. controller itself has a name, which it can
> use to make names for the pods it creates (prepend/append indices or
> something). IDs for the pods could still be generated by the apisever or
> wherever we decide they need to be generated. I was trying to say that
> since there's a 1:1 relationship between pods and manifests, we shouldn't
> make different IDs for each.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/199#issuecomment-47249802
> .
"
439	244	brendandburns	2014-06-26 19:01:33	CONTRIBUTOR	"Ok, this is ready for merge now.

PTAL
Thanks!
--brendan
"
440	244	lavalamp	2014-06-26 19:39:38	MEMBER	"Generally LGTM; one thing to consider, I'd like the GET minions/<minion> api call to be able to return the minion's external IP address. That may affect the types which a minion registry returns to the MinionStorage. Doesn't have to be addressed in this PR.

(Footnote: Our XXStorage/XXRegistry notation is unfortunate because the XXRegistry does the storage and the XXStorage is just the access layer for the apiserver. :( )
"
441	255	lavalamp	2014-06-26 19:44:27	MEMBER	"Travis is sleeping, I guess? Merging anyway, thanks for the change.
"
442	256	lavalamp	2014-06-26 19:54:09	MEMBER	"Thanks for the change! Can you sign the CLA so we can accept it? Instructions in CONTRIB.md. Thanks!
"
443	238	lavalamp	2014-06-26 19:59:06	MEMBER	"Needs a rebase due to your other PR that I merged. Also, cloudcfg is now kubecfg. Other than that, is this ready to merge?
"
444	254	lavalamp	2014-06-26 20:50:10	MEMBER	"Generally LGTM. Will wait for LGTM from Tim or Brendan before merging.
"
445	254	jjhuff	2014-06-26 21:00:46	CONTRIBUTOR	"Ok, fixes pushed. I haven't done the dockerId string->type conversion. I'm happy to tackle that in another PR, or this one if you guys want.
"
446	254	thockin	2014-06-26 21:07:16	MEMBER	"I have to run off - will take a detailed look this evening, if that is OK
"
447	256	lavalamp	2014-06-26 21:16:23	MEMBER	"I found your CLA. Thanks again!
"
448	199	bgrant0607	2014-06-26 21:17:30	MEMBER	"Numeric host names: http://tools.ietf.org/html/rfc1123#page-13
"
449	199	smarterclayton	2014-06-26 21:26:19	CONTRIBUTOR	"Regarding the open question: is a pod on a new minion the same as the old pod?  Is a move an action that is logically part of kubernetes, or is only ""remove"" and ""create new"" available?  If it's the former, it seems like the ID should be the same, if it's the later it seems like it should be different.

We have a use case for being able to move a pod from minion to minion (and any volumes that come with it) - however, since this requires the volume data on disk to be in a resting state, it's not an operation that seems to lend itself well to the replication controller (since the move is inherently stateful).  So I would expect this to be managed as an operation above the replication controller, vs part of it.

One namespace per apiserver can be limiting if the namespace is automatically bound to DNS and you're dealing with very large numbers of containers, but it doesn't sound unreasonable if you are using wildcard DNS.
"
450	148	smarterclayton	2014-06-26 21:44:57	CONTRIBUTOR	"Re #199, if this is a PUT would the caller have to specify the unique pod ID in order to preserve idempotency?
"
451	257	lavalamp	2014-06-26 21:45:59	MEMBER	"Looks like you need to increase the timeout in the integration test.
"
452	254	jjhuff	2014-06-26 21:47:04	CONTRIBUTOR	"Yup, that's fine.
"
453	238	brendandburns	2014-06-26 21:56:50	CONTRIBUTOR	"Please don't merge this yet.  Few more changes coming.

--brendan
"
454	258	smarterclayton	2014-06-26 22:06:48	CONTRIBUTOR	"Also appears https://github.com/GoogleCloudPlatform/kubernetes/blob/546600371ed26c2cd59265561517f790c03f8125/pkg/registry/endpoints.go#L55
"
455	258	lavalamp	2014-06-26 22:08:45	MEMBER	"Good catch, let me fix that one, too, before merging.
"
456	199	jjhuff	2014-06-26 22:11:48	CONTRIBUTOR	"@lavalamp BTW, it looks like docker IDs are more or less just 32 random bytes: https://github.com/dotcloud/docker/blob/master/utils/utils.go#L412

They enforce them to be unique per-machine: https://github.com/dotcloud/docker/blob/master/daemon/daemon.go#L470
"
457	199	bgrant0607	2014-06-26 22:35:06	MEMBER	"Unique ids disambiguate among multiple instances reusing the same name over
time, such as when polling termination status.

Beware of moving unique ids. Once we do that, they are no longer unique,
such as in the cases of live migration or even just preloading. If we do
move ids, we should have 2 ids, the movable one at the cluster level and a
non-movable one at the node level, and the movable ids would correspond to
zero or more of the non-movable ids.
"
458	148	bgrant0607	2014-06-26 22:50:14	MEMBER	"At the master level, I think the direction we're headed is similar to what we do internally: user-provided space-unique human-friendly names and system-generated spacetime-unique numerical ids. The caller would specify the name. Duplicate creations would be rejected with an ALREADY_EXISTS error.

Since nodes are both used directly by users and as slaves, we probably want similar user-oriented behavior, but the master would auto-generate names by combining the user-assigned name and master-generated id, as proposed by @thockin in #199. 
"
459	258	lavalamp	2014-06-26 23:22:20	MEMBER	"OK, should be good to go. IPv6 is hard enough without doing broken concatenations :)
"
460	234	lavalamp	2014-06-26 23:50:21	MEMBER	"I believe my change #249 will have fixed this as a side effect.
"
461	258	lavalamp	2014-06-26 23:58:09	MEMBER	"Hm, that was an interesting flake. Added a small change to make the test fail cleaner if that happens again.
"
462	261	lavalamp	2014-06-27 00:00:51	MEMBER	"Thanks for the change! Can you sign our CLA, as in the CONTRIB.md file, and tell me what name to look for? Thanks!
"
463	261	meirf	2014-06-27 00:06:00	CONTRIBUTOR	"Mark Fischer

Thanks!!
On Jun 26, 2014 8:01 PM, ""Daniel Smith"" notifications@github.com wrote:

> Thanks for the change! Can you sign our CLA, as in the CONTRIB.md file,
> and tell me what name to look for? Thanks!
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/261#issuecomment-47294645
> .
"
464	234	dchen1107	2014-06-27 00:20:49	MEMBER	"Yes, I noticed it and verified. The issue can be closed.

On Thu, Jun 26, 2014 at 4:50 PM, Daniel Smith notifications@github.com
wrote:

> I believe my change #249
> https://github.com/GoogleCloudPlatform/kubernetes/pull/249 will have
> fixed this as a side effect.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/234#issuecomment-47294023
> .
"
465	262	lavalamp	2014-06-27 01:40:12	MEMBER	"Just so I understand things correctly, you're fixing this by watching a smaller portion of the tree? That's good and we should definitely do it, but if we kill/restart on an unrelated change, we should fix that, too...
"
466	262	jjhuff	2014-06-27 01:55:53	CONTRIBUTOR	"Yes to watching a smaller part of the tree. Watching too much was the source of the kill/restart behavior. The master does two things when creating a new pod on a kubelet:
1. Set /register/hosts/<host>/pod/<id> to be the pod's json blob
2. Write /register/hosts/<host>/kubelet
3. Add the new manifest (extracted from the pod) to the list from 2
4. Write back to  /register/hosts/<host>/kubelet
FWIW, I don't think that 2-4 is safe.

The killing was because the kubelet would pickup the change under /pod/, parse it, and notify SyncManifests, which assumes that it's always being notified with the full set of manifests rather than just deltas.
"
467	262	brendandburns	2014-06-27 02:00:14	CONTRIBUTOR	"fwiw, I think that Justin meant for step 2 to be ""read /register/hosts/<machine>/kublet"" rather than write.

But yes, you're right, its not safe.  We need to add a lock in there to prevent read/update/write races.

I'll merge this, since it fixes the probem, but we should do more in the master as well.
"
468	238	brendandburns	2014-06-27 02:02:07	CONTRIBUTOR	"Ok, fixed, this is ready to merge.

Thanks!
--brendan
"
469	262	jjhuff	2014-06-27 02:03:46	CONTRIBUTOR	"Yup, that's what I meant.
I'll open an issue for the master side of things.
"
470	262	lavalamp	2014-06-27 02:04:41	MEMBER	"LGTM.
"
471	263	jjhuff	2014-06-27 02:10:04	CONTRIBUTOR	"Beat me to it.

I'm not a fan of B w/ lock when we have etcd available. 
"
472	257	lavalamp	2014-06-27 02:15:19	MEMBER	"LOL-- I think my test is broken if the random number at the end of the docker name is < 0x1fffffff
"
473	264	lavalamp	2014-06-27 02:37:14	MEMBER	"Thanks!
"
474	257	brendandburns	2014-06-27 02:55:58	CONTRIBUTOR	"Ok, with padding in place, Travis now appears to be a happy camper...
"
475	263	brendandburns	2014-06-27 03:00:05	CONTRIBUTOR	"Ok, I'm going to take a stab at implementing this w/ etcd semantics
"
476	265	lavalamp	2014-06-27 03:41:45	MEMBER	"How difficult is it to write a test that would expose any racy behavior?
"
477	265	brendandburns	2014-06-27 03:43:28	CONTRIBUTOR	"Yeah, I was thinking about that.  You could probably do it by adding some synchronization logic to the fake etcd client, which caused the write to sleep until you performed the other write.

I kind of feel like I should do it, because this is tricky code to get right.  At the same time, its going to be a total pain in the a*\* to write.  Maybe we can merge this with a TODO to write a test :P
"
478	265	lavalamp	2014-06-27 03:47:32	MEMBER	"I'm OK with a TODO. I wouldn't mind writing such a test tomorrow.
"
479	199	thockin	2014-06-27 04:04:47	MEMBER	"OK, collecting thoughts again.  If this is acceptable, I (or Clayton) can
write up a short .md file covering identifiers.  There are still a few
FIXME comments in here.  Please tell me if I am capturing any of this
incorrectly.

Global: We spec most names as RFC 1035/1123 compatible.  Docker allows
container names to include underscores, which we reserves for use by
infrastructure.

From kubelet's point of view:

1) All pods have a namespace string, which is human-friendly and DNS
friendly (an rfc1035/1132 subdomain).  For example: ""k8s.mydomain.com"".
 This is used to indicate the provenance of the pod.  If the namespace is
not specified when creating a pod, kubelet will assign a namespace derived
from the source of the pod.  For example, a file ""/etc/k8s/cadvisor.pod"" on
host ""xyz123.mydomain.com"" might get masterspace ""
file-f5sxiyzpnm4hgl3dmfshm2ltn5zc44dpmqfa.xyz123.mydomain.com"" (the big
mess is a base32 encoding of the path).  (FIXME: do we need this if we have
a UUID as spec'ed below?)

2) All pods have a name string, which is human-friendly and DNS friendly
(an rfc1035/1132 subdomain fragment).  For example: ""8675309.myjob"".  These
names are typically NOT user-provided, but are generated by infrastructure.
 For example, if the user asked for a job named ""myjob"" the apiserver must
uniquify that into a pod name.

3) The namespace + name pair is assumed to be unique.  This provides simple
idempotency, for example when re-reading a config file.

4) When starting an instance of a pod for the first time (i.e. not
restarting an existing pod), kubelet will assign an rfc4122 compatible UUID
to the pod.  This provides an ID that is guaranteed unique across time and
space.  If the pod is stopped and an identical pod (same namespace + name)
is started, a new UUID will be assigned.

5) Kubelet will use the aforementioned identifiers to produce unique
container names, for example
""8675309.myjob.k8s.mydomain.com_7c9fc7d1-5aac-46f5-b76f-b0d8d0effe2a"".

NB: The UUID is per-pod, not to be confused with Docker's own container IDs
which are per-container.

From the apiserver's point of view:

1) The apiserver has a configured namespace, for example ""k8s.mydomain.com"".

2) All incoming pods have a name assigned by the originator of the pod (be
they human users or other infrastructure).  Pod names must be unique in
space, but not time.

3) Upon acceptance, a pod is assigned a unique ID (FIXME: just spec it as
rfc4122 again?  could maybe be simpler here?).  This provides an easy way
to disambiguate successive pods with the same name.  This ID will persist
for the lifetime of the pod, across restarts and moves.  (FIXME: is this
really needed or is name good enough?)

4) Upon assignment to a minion, the name and unique ID become the pod name.

5) The namespace + name together must be less no more than 255 characters
long.

If DNS service is to be configured automatically (not a feature yet, but
has been discussed), the pod namespace + name will already be DNS compliant.

> On Thu, Jun 26, 2014 at 9:46 AM, Daniel Smith notifications@github.com
> wrote:
> 
> > For clarity, let me suggest the convention that ""name"" means a friendly
> > mostly human readable string, possibly in the style of dns names, and that
> > ""id"" means an opaque, machine generated identifier, guaranteed unique at
> > some level of resolution. Maybe everyone except for me is already using
> > this convention. :) But I want to talk about ids and names in general
> > without referring to a particular implementation.
> > 
> > My only concern here is that starts to feel weird when a Manifest is
> > used as part of a ReplicationController. I think. You end up with N pods
> > and a Manifest each with ID fields.
> > 
> > I think, in that case, the rep. controller itself has a name, which it
> > can use to make names for the pods it creates (prepend/append indices or
> > something). IDs for the pods could still be generated by the apisever or
> > wherever we decide they need to be generated. I was trying to say that
> > since there's a 1:1 relationship between pods and manifests, we shouldn't
> > make different IDs for each.
> > 
> > ## 
> > 
> > Reply to this email directly or view it on GitHub.

On Thu, Jun 26, 2014 at 3:35 PM, bgrant0607 notifications@github.com
wrote:

> Unique ids disambiguate among multiple instances reusing the same name over
> time, such as when polling termination status.
> 
> Beware of moving unique ids. Once we do that, they are no longer unique,
> such as in the cases of live migration or even just preloading. If we do
> move ids, we should have 2 ids, the movable one at the cluster level and a
> non-movable one at the node level, and the movable ids would correspond to
> zero or more of the non-movable ids.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/199#issuecomment-47288913
> .
"
480	199	jjhuff	2014-06-27 05:51:10	CONTRIBUTOR	"I think this seems like a good outline!
Two comments:
1. For config files, maybe use a hash of the file vs a base64 encoding of the path as the component of the namespace. Just seems more natural.
2. ""These names are typically NOT user-provided, but are generated by infrastructure."" I'd consider being more explicit about that behavior, even if it's as simple as a ""unique prefix will be added to the name"". This sorta touches on the whole id/name discussion....
"
481	199	bgrant0607	2014-06-27 13:29:46	MEMBER	"Another positive attribute of unique ids: they are typically more concise
than names, and are therefore more efficient to use in cross references in
logs.

Based on experience in Omega, I think there's no question that we need
unique ids.

Also, if you play around with Docker for a short while, you'll find a pile
of old objects that only have unique ids and can no longer be referenced by
name. How do you query them, resurrect them, or delete them once their
names have been recycled?

---

Is there a compelling reason to use a different kind of unique identifier
in the apiserver than in the kubelet? I don't think so.

We could probably shorten it (e.g., remove the locator) when we combine it
with the name to form the name for kubelet.
"
482	199	smarterclayton	2014-06-27 15:18:10	CONTRIBUTOR	"Definitely prefer hash of something in the config file (maybe hash of the JSON of the pod's manifest or something) for use in the namsepace component.
"
483	199	smarterclayton	2014-06-27 15:19:14	CONTRIBUTOR	"I'm happy to write this up as part of #253 (along with some more tests and the apiserver implications) once folks reach closure.
"
484	246	smarterclayton	2014-06-27 15:31:41	CONTRIBUTOR	"As a further item (possibly as a separate issue), being able to change code on your devenv and either one-line a command (hack/update-local) or have the image/source automatically reloaded would be valuable for reducing code-test loop time.  Probably the former though
"
485	199	smarterclayton	2014-06-27 15:39:33	CONTRIBUTOR	"One minor note regarding Docker ecosystem - ""name"" is currently being used in Docker for a lot of lightweight integrations (linking, sky dock dns, hostname in container).  A side effect of any generated name for a Docker container is that those lightweight integrations may become more difficult for end admins.  Is there a practical way to make the name appropriately unique on the minion without breaking those potential integrations (making the name a subdomain fragment by omitting '.' for instance)
"
486	199	brendanburns	2014-06-27 15:45:40	CONTRIBUTOR	"I would also like to see us drive labels all the way down into Docker. That
would enable us to abandon much of what we are encoding into the name today.

Brendan

On Fri, Jun 27, 2014, 8:39 AM, Clayton Coleman notifications@github.com
wrote:

> One minor note regarding Docker ecosystem - ""name"" is currently being used
> in Docker for a lot of lightweight integrations (linking, sky dock dns,
> hostname in container). A side effect of any generated name for a Docker
> container is that those lightweight integrations may become more difficult
> for end admins. Is there a practical way to make the name appropriately
> unique on the minion without breaking those potential integrations?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/199#issuecomment-47362231
> .
"
487	199	bgrant0607	2014-06-27 16:04:33	MEMBER	"+1 to what brendanburns@ wrote. Labels provide much cleaner solutions for
many use cases.
"
488	265	brendandburns	2014-06-27 16:13:06	CONTRIBUTOR	"Ok, updated.  I added a pretty extensive TODO with the recipe for the test in etcd_registry_test.go
"
489	266	brendandburns	2014-06-27 16:20:11	CONTRIBUTOR	"Thanks for the fix!
"
490	267	lavalamp	2014-06-27 16:52:57	MEMBER	"Check out the change @brendanburns made in #244, to figure out the list of minions via query to cloud provider. It's not wired in yet, but when it is it'll be pretty easy to completely customize this just by adding a custom cloud provider plugin.
"
491	158	brendandburns	2014-06-27 17:38:52	CONTRIBUTOR	"Closing this, I believe it is currently addressed in the readme.
"
492	1	brendandburns	2014-06-27 17:40:36	CONTRIBUTOR	"We're now at 58%, still not great, but I think we can close this issue, and track improving coverage as a general project goal.
"
493	267	smarterclayton	2014-06-27 17:53:57	CONTRIBUTOR	"Good to subdivide this into two use cases - I'm on a cloud, or I'm on a custom environment/non-cloud.   I don't think small non-cloud deployments would be able to write their own plugin effectively - they might prefer the simple API/CLI for add/remove minion.  However, that does limit the system's ability to release resources / ask for more resources (which probably is an abstraction separate from the minion API).
"
494	267	lavalamp	2014-06-27 17:59:29	MEMBER	"Yeah. Maybe we should write a generic non-cloud CloudProvider. We could make one that shelled out for various commands, for example, or just provide default useful actions.
"
495	267	smarterclayton	2014-06-27 18:43:48	CONTRIBUTOR	"Definitely - @mrunalp / @ironcladlou this may fit with the stuff you mentioned you were looking at
"
496	254	jjhuff	2014-06-27 19:11:55	CONTRIBUTOR	"Pushed.
"
497	254	thockin	2014-06-27 19:13:45	MEMBER	"I have to go pick up my kids, but I'll do a final pass during naptime. :)

Is there any way to see the delta between this and the last version I
looked at?

On Fri, Jun 27, 2014 at 12:12 PM, Justin Huff notifications@github.com
wrote:

> Pushed.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/254#issuecomment-47389092
> .
"
498	254	jjhuff	2014-06-27 19:24:41	CONTRIBUTOR	"Sounds good.
As for the last version. I don't think so. I like to keep the merged commits to a minimum, so I typically squash minor commits prior to pushing. Perhaps a better workflow would be to postpone the rebase -i until later?
"
499	188	smarterclayton	2014-06-27 20:09:45	CONTRIBUTOR	"Still gathering more feedback from customers and ops folks, but there's a lot of concern about being able to deploy the IP-per-container model outside of the big cloud providers.  Recording comments I'm hearing:
- In most non-cloud deployments this involves setting up the necessary network configuration to make IP allocation scalable
- This is expensive for operations teams
- In some organizations this may be a non-starter (maybe not many, but it's tough to get configured)
- If it's possible to hack this together slowly, it's less of a concern.  I.e. if you start with 1-2 minions and there's some simple scripts you can run to hack together a VPC for the containers, you might be ok up to 4-5 minions.  Then you need to switch to something more maintainable
- A lot of ops shops expect to be in control of things like DHCP, and are leery of deploying multiple DHCP servers just to configure a special VPC.  So they'd have to do the config to integrate the container use case into their existing DHCP which can be frustrating.
"
500	254	thockin	2014-06-27 20:45:09	MEMBER	"In hindsight, maybe so.  At least for non-trivial changes.  reviewing now.

On Fri, Jun 27, 2014 at 12:24 PM, Justin Huff notifications@github.com
wrote:

> Sounds good.
> As for the last version. I don't think so. I like to keep the merged
> commits to a minimum, so I typically squash minor commits prior to pushing.
> Perhaps a better workflow would be to postpone the rebase -i until later?
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/254#issuecomment-47390374
> .
"
501	254	thockin	2014-06-27 21:01:56	MEMBER	"I'm good with this change, let me know if you want to make the last tweak
or not.

On Fri, Jun 27, 2014 at 1:44 PM, Tim Hockin thockin@google.com wrote:

> In hindsight, maybe so.  At least for non-trivial changes.  reviewing now.
> 
> On Fri, Jun 27, 2014 at 12:24 PM, Justin Huff notifications@github.com
> wrote:
> 
> > Sounds good.
> > As for the last version. I don't think so. I like to keep the merged
> > commits to a minimum, so I typically squash minor commits prior to pushing.
> > Perhaps a better workflow would be to postpone the rebase -i until later?
> > 
> > ## 
> > 
> > Reply to this email directly or view it on GitHub
> > https://github.com/GoogleCloudPlatform/kubernetes/pull/254#issuecomment-47390374
> > .
"
502	254	jjhuff	2014-06-27 21:05:37	CONTRIBUTOR	"I just did the rename and rebased to master, so it should be good to go.

On Fri, Jun 27, 2014 at 2:02 PM, Tim Hockin notifications@github.com
wrote:

> I'm good with this change, let me know if you want to make the last tweak
> or not.
> 
> On Fri, Jun 27, 2014 at 1:44 PM, Tim Hockin thockin@google.com wrote:
> 
> > In hindsight, maybe so. At least for non-trivial changes. reviewing now.
> > 
> > On Fri, Jun 27, 2014 at 12:24 PM, Justin Huff notifications@github.com
> > wrote:
> > 
> > > Sounds good.
> > > As for the last version. I don't think so. I like to keep the merged
> > > commits to a minimum, so I typically squash minor commits prior to
> > > pushing.
> > > Perhaps a better workflow would be to postpone the rebase -i until
> > > later?
> > > 
> > > ## 
> > > 
> > > Reply to this email directly or view it on GitHub
> > > <
> > > https://github.com/GoogleCloudPlatform/kubernetes/pull/254#issuecomment-47390374
> > > 
> > > .
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/254#issuecomment-47399867
> .
"
503	254	thockin	2014-06-27 21:20:55	MEMBER	"Thanks.  Great cleanup.

On Fri, Jun 27, 2014 at 2:14 PM, Daniel Smith notifications@github.com
wrote:

> Merged #254 https://github.com/GoogleCloudPlatform/kubernetes/pull/254.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/254#event-136083841
> .
"
504	251	brendandburns	2014-06-27 21:57:54	CONTRIBUTOR	"Please see #271 for a more generic fix.
"
505	251	brendandburns	2014-06-27 22:07:53	CONTRIBUTOR	"Closed by #271 
"
506	269	lavalamp	2014-06-27 22:18:04	MEMBER	"Looks like an unstable map ordering issue from #254 caused this flake-- I'll fix.
"
507	127	bgrant0607	2014-06-27 22:28:55	MEMBER	"Two relevant Docker issues being discussed:
https://github.com/dotcloud/docker/issues/26    auto-restart processes
https://github.com/dotcloud/docker/issues/1311   production-ready process monitoring

The former is converging towards supporting restarts in Docker, with the 3 modes proposed here: always, on failure, never.

The latter has been debating the merits of ""docker exec"", which would not run the process under supervision of the Docker daemon. The motivation is to facilitate management by process managers such as systemd, supervisord, upstart, monit, runit, etc. This approach is attractive for a number of reasons.
"
508	127	smarterclayton	2014-06-27 22:38:10	CONTRIBUTOR	"While not called out in the latter issue, the blocking dependency is the ability for the daemon to continue to offer a consistent API for managing containers.  This was one of the inspirations for libswarm - allowing the daemon to connect to a process running in the container namespace in order to issue commands that affect the container as a unit (stop, stream logs, execute a new process).  The refactored Docker engine to allow that currently exists in a branch of Michael Crosby's, but libchan and swarm are not mature enough yet to provide that behavior.
"
509	269	lavalamp	2014-06-27 22:41:27	MEMBER	"Added TODO about racy set. Should be good to go now.
"
510	273	lavalamp	2014-06-27 23:11:53	MEMBER	"See #169... Just waiting on a CLA to show up
"
511	160	proppy	2014-06-27 23:12:19	CONTRIBUTOR	"I'd be great if the scheduler was also leverage this data for pod placement, should I file a separate bug?
"
512	273	brendandburns	2014-06-27 23:14:14	CONTRIBUTOR	"hrm, I like mine, it uses DOCKER_HOST too ;)
"
513	160	vmarmol	2014-06-27 23:14:34	CONTRIBUTOR	"I believe that is planned, but we can file an issue to track it
On Jun 27, 2014 4:12 PM, ""Johan Euphrosine"" notifications@github.com
wrote:

> I'd be great if the scheduler was also leverage this data for pod
> placement, should I file a separate bug?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/160#issuecomment-47409498
> .
"
514	260	bgrant0607	2014-06-27 23:24:25	MEMBER	"Note that we should probably also rename service to lbservice or somesuch to distinguish them from other types of services.
"
515	273	lavalamp	2014-06-27 23:47:22	MEMBER	"One thing-- can you modify the call to docker pull, so that it attempts to use the same endpont as the client?
"
516	273	brendandburns	2014-06-27 23:56:46	CONTRIBUTOR	"Done.
"
517	273	brendandburns	2014-06-28 00:01:52	CONTRIBUTOR	"Good catch!  Hard to catch this...

Fixed.
"
518	273	lavalamp	2014-06-28 00:10:23	MEMBER	"I only know that because I was trying to make my local kubelet talk to a docker running in a GCE instance via a forwarded port the other day. :)
"
519	169	brendandburns	2014-06-28 00:17:22	CONTRIBUTOR	"This is superceded by #273 (sorry!)  We're still waiting on the CLA, and a user ran into a similar problem.

We'd love to integrate the rest of your PR when it gets broken up some more, and the CLA comes through.

Thanks!
--brendan
"
520	279	brendandburns	2014-06-28 04:48:23	CONTRIBUTOR	"Closes #272 
"
521	263	brendandburns	2014-06-28 04:49:52	CONTRIBUTOR	"Closed by #265
"
522	199	thockin	2014-06-28 06:21:42	MEMBER	"On Thu, Jun 26, 2014 at 10:51 PM, Justin Huff notifications@github.com wrote:

> I think this seems like a good outline!
> Two comments:
> 1. For config files, maybe use a hash of the file vs a base64 encoding of the path as the component of the namespace. Just seems more natural.

This is an implementation detail - human-friendliness of docker's
names is a nice-to-have not critical, I think.

> 1. ""These names are typically NOT user-provided, but are generated by infrastructure."" I'd consider being more explicit about that behavior, even if it's as simple as a ""unique prefix will be added to the name"". This sorta touches on the whole id/name discussion....

Yeah, where is the line between ""ID"" and ""name""?  The apiserver COULD
just send a UUID for the name, but I think we want the structure and
content of a pod spec to stay largely the same throughout the entire
system, at least for now.  I don't think we've got any consensus on
diverging it, anyway.  For that reason, I think ""name"" makes sense.
It is more human readable than not.
"
523	199	thockin	2014-06-28 06:25:32	MEMBER	"+1 to labels

On Fri, Jun 27, 2014 at 8:45 AM, brendanburns notifications@github.com
wrote:

> I would also like to see us drive labels all the way down into Docker.
> That
> would enable us to abandon much of what we are encoding into the name
> today.
> 
> Brendan
> 
> On Fri, Jun 27, 2014, 8:39 AM, Clayton Coleman notifications@github.com
> wrote:
> 
> > One minor note regarding Docker ecosystem - ""name"" is currently being
> > used
> > in Docker for a lot of lightweight integrations (linking, sky dock dns,
> > hostname in container). A side effect of any generated name for a Docker
> > container is that those lightweight integrations may become more
> > difficult
> > for end admins. Is there a practical way to make the name appropriately
> > unique on the minion without breaking those potential integrations?
> > 
> > ## 
> > 
> > Reply to this email directly or view it on GitHub
> > <
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/199#issuecomment-47362231>
> > 
> > .
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/199#issuecomment-47363939
> .
"
524	199	thockin	2014-06-28 06:26:27	MEMBER	"What would break on those?  We're not proposing to do anything with name
that is not totally valid as per standalone Docker?

On Fri, Jun 27, 2014 at 8:39 AM, Clayton Coleman notifications@github.com
wrote:

> One minor note regarding Docker ecosystem - ""name"" is currently being used
> in Docker for a lot of lightweight integrations (linking, sky dock dns,
> hostname in container). A side effect of any generated name for a Docker
> container is that those lightweight integrations may become more difficult
> for end admins. Is there a practical way to make the name appropriately
> unique on the minion without breaking those potential integrations?
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/199#issuecomment-47362231
> .
"
525	282	lavalamp	2014-06-28 18:29:11	MEMBER	"Nice catch, thanks!
"
526	283	thockin	2014-06-28 20:15:40	MEMBER	"LGTM.  Sadly, I'll need an IntSet, too.
"
527	283	lavalamp	2014-06-28 21:01:45	MEMBER	"Ah. In that case, maybe we should make a set package, with set.Int and set.String types. At least that will keep the name lengths down.
"
528	283	thockin	2014-06-28 21:32:13	MEMBER	"SGTM as a followup.  Apprently you CAN make a set that will hold any type -
the downside being that it will accept any type - more like void\* than a
template.  better or worse?

On Sat, Jun 28, 2014 at 2:01 PM, Daniel Smith notifications@github.com
wrote:

> Ah. In that case, maybe we should make a set package, with set.Int and
> set.String types. At least that will keep the name lengths down.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/283#issuecomment-47438412
> .
"
529	280	lavalamp	2014-06-28 21:32:52	MEMBER	"Travis is failing you for needing a gofmt, btw. Might want to double check that your hooks are linked from .git/hooks.
"
530	283	lavalamp	2014-06-28 21:36:01	MEMBER	"IMO, worse, because if you do a ""for key := range myGenericSet"", you'll have to do type assertions on the key to use it. I can see doing this only if you have a need to put multiple different types in the set.
"
531	283	lavalamp	2014-06-28 21:38:30	MEMBER	"We should see about getting a complete such set library added to the go standard library as ""container/set"". IMO it's ridiculous that everyone has to write their own.
"
532	199	smarterclayton	2014-06-28 21:55:24	CONTRIBUTOR	"It's not that the names aren't valid for Docker, its that something consuming those names (as a dns prefix a la skydock, or as a hostname) would break due to length or format on the Kube generated name.  I don't think that is a blocker to the proposed naming patterns above, but it's a consideration when thinking about how other software that plays well with Docker might react.

Concrete ideas might be to reduce the generated names' length and avoid using '.' as a separator.
"
533	283	thockin	2014-06-28 22:52:39	MEMBER	"+1 except let's keep it minimal - no point duplicating code for each type
when we won't use a lot of it.

Without custom comparators set is a lot less useful.
On Jun 28, 2014 2:38 PM, ""Daniel Smith"" notifications@github.com wrote:

> We should see about getting a complete such set library added to the go
> standard library as ""container/set"". IMO it's ridiculous that everyone has
> to write their own.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/283#issuecomment-47439232
> .
"
534	286	lavalamp	2014-06-28 23:01:20	MEMBER	"Weird-- integration passes for me. Will fix later.
"
535	287	lavalamp	2014-06-29 01:37:50	MEMBER	"Thanks for catching this! Can we get you to sign a CLA as described in CONTRIB.md so that we can merge it? Thanks!
"
536	288	lavalamp	2014-06-29 01:59:04	MEMBER	"I believe @vmarmol is or was working on this? Adding it so we don't forget about it.
"
537	288	thockin	2014-06-29 02:43:45	MEMBER	"Caution: This is a slippery slope.  One day people will start setting
resource limits.  Launching all pulls in parallel can easily exhaust those
resource limits or put so much pressure on the disk that they all slow to
crawl while the disk head thrashes about (for those of us with physical
disks).

Moreover, this is a general scalability problem - we need to build Kubelet
in a way that ensures that one pod can not disrupt the servicing of
another.  This is just one (very noticeable) aspect of that problem.

This is something we have put a lot of thought into internally, and I'd
like to do properly...

On Sat, Jun 28, 2014 at 6:59 PM, Daniel Smith notifications@github.com
wrote:

> I believe @vmarmol https://github.com/vmarmol is or was working on
> this? Adding it so we don't forget about it.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/288#issuecomment-47443687
> .
"
538	288	brendandburns	2014-06-29 02:45:45	CONTRIBUTOR	"Also, please note that docker barfs if you try to pull the same image in parallel, that's why the lock is there in the first place.

If we really want to do this right, we should add a sharded lock, that will enable us to both have parallelism, and limit the extend of the parallelism, with a single configuration.
"
539	289	brendandburns	2014-06-29 02:51:30	CONTRIBUTOR	"LGTM
"
540	286	brendandburns	2014-06-29 02:58:57	CONTRIBUTOR	"Small stuff, generally LGTM
"
541	289	lavalamp	2014-06-29 04:10:06	MEMBER	"Cool!
"
542	285	smarterclayton	2014-06-29 04:12:29	CONTRIBUTOR	"Related to #127, #137, and #156. Are there other pieces of information besides state that are related to a container's execution that should be rolled up?  Certainly the time that a state transition occurred is relevant.  I bring up because we tend to think of this as a three part problem - 

1) specific container events which are core to the platform being reported to the core infrastructure (state, exit status)
2) generic container events triggered by other host level monitors (resource threshold exceeded), and possibly host level events (no memory available, swap warnings)
3) making decisions based on 1) or 2) that might feed into the scheduler or replication controllers

At the start, we saw this as a separate logical subsystem (events) with a separate return channel (since this information is essentially untrustworthy in a multi-tenant system), feeding back into an event bus that detached components could react to.  However, for 1) in particular I could buy the argument that this is a core mechanism of the master->kubelet system and is separate from a generic event mechanism.
"
543	278	smarterclayton	2014-06-29 04:17:40	CONTRIBUTOR	"Will fix in #253
"
544	280	thockin	2014-06-29 04:29:38	MEMBER	"I have resolved all FIXMEs, run gofmt, etc etc.

There are a number of comments on code that I don't know if I have
satisfied or not.  Could anyone who cares about this change please take
another look?

On Sat, Jun 28, 2014 at 2:32 PM, Daniel Smith notifications@github.com
wrote:

> Travis is failing you for needing a gofmt, btw. Might want to double check
> that your hooks are linked from .git/hooks.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/280#issuecomment-47439119
> .
"
545	280	thockin	2014-06-29 05:00:13	MEMBER	"OK, I finally got a green CI build - integration tests FTW.  Running this on my desktop at work means I can't run integration tests directly.
"
546	285	thockin	2014-06-29 05:06:12	MEMBER	"I think you're right that some things are so core that they have to be part
of the primary API.  As Kubelet gets more complicated, we really do need to
expose some details about whats going on.  We don't really have a state
machine yet, but we will.  We don't really have local state yet, but I
think we will.

I think we want the user experience to be centralized - some common tools
or UI or database that shows you everything that has been going on.

On Sat, Jun 28, 2014 at 9:12 PM, Clayton Coleman notifications@github.com
wrote:

> Related to #127
> https://github.com/GoogleCloudPlatform/kubernetes/issues/127 and #137
> https://github.com/GoogleCloudPlatform/kubernetes/issues/137. Are there
> other pieces of information besides state that are related to a container's
> execution that should be rolled up? Certainly the time that a state
> transition occurred is relevant. I bring up because we tend to think of
> this as a three part problem -
> 
> 1) specific container events which are core to the platform being reported
> to the core infrastructure (state, exit status)
> 2) generic container events triggered by other host level monitors
> (resource threshold exceeded), and possibly host level events (no memory
> available, swap warnings)
> 3) making decisions based on 1) or 2) that might feed into the scheduler
> or replication controllers
> 
> At the start, we saw this as a separate logical subsystem (events) with a
> separate return channel (since this information is essentially
> untrustworthy in a multi-tenant system), feeding back into an event bus
> that detached components could react to. However, for 1) in particular I
> could buy the argument that this is a core mechanism of the master->kubelet
> system and is separate from a generic event mechanism.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/285#issuecomment-47445384
> .
"
547	199	thockin	2014-06-29 05:19:51	MEMBER	"I would argue (for the sake of arguing) that anyone who was making
assumptions about container names deserves to be broken.  But yeah, let's
keep an eye on it.

On Sat, Jun 28, 2014 at 2:55 PM, Clayton Coleman notifications@github.com
wrote:

> It's not that the names aren't valid for Docker, its that something
> consuming those names (as a dns prefix a la skydock, or as a hostname)
> would break due to length or format on the Kube generated name. I don't
> think that is a blocker to the proposed naming patterns above, but it's a
> consideration when thinking about how other software that plays well with
> Docker might react.
> 
> Concrete ideas might be to reduce the generated names' length and avoid
> using '.' as a separator.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/199#issuecomment-47439586
> .
"
548	290	lavalamp	2014-06-29 05:30:34	MEMBER	"LGTM. Strongly agree that %#v is almost never better than an error's default Error() message, have been meaning to do this for a while. Leaving open in case @brendanburns requires more convincing.
"
549	280	lavalamp	2014-06-29 05:45:35	MEMBER	"Will give another look tomorrow or Monday. You _should_ be able to run the integration test on your desktop; I am doing that. For this change, you probably also want to run the e2e tests (hack/e2e-test.sh), which test starting a cluster, the guestbook example and another more basic test, and stopping the cluster.
"
550	199	smarterclayton	2014-06-29 16:53:08	CONTRIBUTOR	"Agree, naming is hard
"
551	288	vmarmol	2014-06-29 17:48:34	CONTRIBUTOR	"I started work on this, but while something simple that works is not too
bad to implement. I imagine you won't all be happy with it :) Trying to see
what I can come up with that doesn't require large re-design.

On Sat, Jun 28, 2014 at 7:45 PM, brendandburns notifications@github.com
wrote:

> Also, please note that docker barfs if you try to pull the same image in
> parallel, that's why the lock is there in the first place.
> 
> If we really want to do this right, we should add a sharded lock, that
> will enable us to both have parallelism, and limit the extend of the
> parallelism, with a single configuration.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/288#issuecomment-47444321
> .
"
552	286	lavalamp	2014-06-29 19:36:32	MEMBER	"Comments addressed.
"
553	245	thockin	2014-06-29 19:51:49	MEMBER	"One of them: VolumeMount.Path vs MountPath.
"
554	291	brendandburns	2014-06-30 02:51:19	CONTRIBUTOR	"No this is a total hack to make it easier for me to wire the Docker inspect info straight into our API.  We should really define our own node level info struct.
"
555	290	brendandburns	2014-06-30 03:34:11	CONTRIBUTOR	"I guess my concern is that we're depending on the implementor of Error to do sane things.  I've seen at least one case where the string version of an RESTful API error didn't contain the status code, and so it was annoying to get it out.  That said, I'm not 100% wedded to %#v if we really find it more spammy, but I like additional info when I'm debugging.
"
556	290	thockin	2014-06-30 04:18:40	MEMBER	"what about %v by default and %+v where needed?

On Sun, Jun 29, 2014 at 8:34 PM, brendandburns notifications@github.com
wrote:

> I guess my concern is that we're depending on the implementor of Error to
> do sane things. I've seen at least one case where the string version of an
> RESTful API error didn't contain the status code, and so it was annoying to
> get it out. That said, I'm not 100% wedded to %#v if we really find it more
> spammy, but I like additional info when I'm debugging.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/290#issuecomment-47492231
> .
"
557	288	thockin	2014-06-30 05:47:13	MEMBER	"Empirically concurrent pulls all bloxk until it is done pulling
"
558	288	brendandburns	2014-06-30 06:04:22	CONTRIBUTOR	"For the same image?  Maybe this is a recent change/fix.  I definitely saw
concurrent pulls off the same image fail in previous versions of docker
(Jan-ish, I think)

Brendan
On Jun 29, 2014 10:47 PM, ""Tim Hockin"" notifications@github.com wrote:

> Empirically concurrent pulls all bloxk until it is done pulling
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/288#issuecomment-47496676
> .
"
559	288	thockin	2014-06-30 06:10:35	MEMBER	"root@thdev:/home/thockin# docker pull ubuntu
Repository ubuntu already being pulled by another client. Waiting.
root@thdev:/home/thockin# docker -v
Docker version 0.11.1, build fb99f99

On Sun, Jun 29, 2014 at 11:04 PM, brendandburns notifications@github.com
wrote:

> For the same image? Maybe this is a recent change/fix. I definitely saw
> concurrent pulls off the same image fail in previous versions of docker
> (Jan-ish, I think)
> 
> Brendan
> On Jun 29, 2014 10:47 PM, ""Tim Hockin"" notifications@github.com wrote:
> 
> > Empirically concurrent pulls all bloxk until it is done pulling
> > 
> > ## 
> > 
> > Reply to this email directly or view it on GitHub
> > <
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/288#issuecomment-47496676>
> > 
> > .
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/288#issuecomment-47497425
> .
"
560	253	smarterclayton	2014-06-30 13:47:05	CONTRIBUTOR	"Would like to include on top of #280 
"
561	302	brendanburns	2014-06-30 15:32:15	CONTRIBUTOR	"While this is possible, we resync every 30 secs, so at worst, this adds
latency...

Brendan
On Jun 30, 2014 7:59 AM, ""Tim Hockin"" notifications@github.com wrote:

> This function gets initial state and then sets up a watch. It looks like
> (have not proven it yet) there is a race wherein the stat might change
> between those two events and not get noticed.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/302.
"
562	290	brendandburns	2014-06-30 16:11:16	CONTRIBUTOR	"I'm fine to give this a try until I have trouble debugging something ;)
"
563	292	lavalamp	2014-06-30 16:21:52	MEMBER	"You beat me to this. Perhaps should be COLLAB.md, since all our other docs are all uppercase for reasons unknown to me?
"
564	290	lavalamp	2014-06-30 16:26:54	MEMBER	"Brendan, my score with %#v vs %v for debugging error messages via their printed form is something like a 0-5 shutout at the moment. With %#v I always get something like &etcd.BlahError{Message: &(0xhexnumber)(*string)} where all the useful stuff is obscured in an internal field that dosen't get expanded. On the other hand, %#v is great for printing objects.
"
565	294	lavalamp	2014-06-30 16:29:19	MEMBER	"Also need to consider how replication controllers produce IDs for the pods they make, right now I think they just copy the possibly-blank ID field in their pod template.
"
566	296	lavalamp	2014-06-30 16:31:13	MEMBER	"It does track this, just not over reboots. (But we never reboot, because we religiously use defer util.HandleCrash() in all goroutines. ;) ;) )
"
567	277	brendandburns	2014-06-30 16:31:13	CONTRIBUTOR	"Ready to merge, I think.  PTAL.

Thanks!
--brendan
"
568	299	lavalamp	2014-06-30 16:34:00	MEMBER	"For even more delicious horribleness, use encoding/gob and encoding/base64 to produce a name literally constructed from the metadata :)
"
569	301	lavalamp	2014-06-30 16:35:20	MEMBER	"This is the generalized version of #288.
"
570	286	brendandburns	2014-06-30 16:36:15	CONTRIBUTOR	"LGTM.  New protocol says 2 LGTMs for merge, @thockin ?
"
571	299	brendandburns	2014-06-30 16:40:33	CONTRIBUTOR	"I did this in an old version of Kubernetes.

I would actually like to see us actively work to push labels down into the
Docker daemon.  Then we could use them to encode all this stuff.

--brendan

On Mon, Jun 30, 2014 at 9:34 AM, Daniel Smith notifications@github.com
wrote:

> For even more delicious horribleness, use encoding/gob and encoding/base64
> to produce a name literally constructed from the metadata :)
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/299#issuecomment-47554040
> .
"
572	293	lavalamp	2014-06-30 16:46:13	MEMBER	"LGTM
"
573	301	thockin	2014-06-30 17:00:17	MEMBER	"Sort of.  Having a ""thread"" per pod is not the same as a thread per
container.  Concurrent pulls could be serialized within a pod (so one pod
doesn't hurt others) or even serialized per disk if needed.  But yes, this
is more general.

On Mon, Jun 30, 2014 at 9:35 AM, Daniel Smith notifications@github.com
wrote:

> This is the generalized version of #288
> https://github.com/GoogleCloudPlatform/kubernetes/issues/288.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/301#issuecomment-47554187
> .
"
574	302	monnand	2014-06-30 17:46:31	CONTRIBUTOR	"Oops. I just filed an issue #304. It may be related.

FYI, go's [race detector](http://blog.golang.org/race-detector) is your friends.
"
575	302	monnand	2014-06-30 17:55:16	CONTRIBUTOR	"Not only in kubelet, `pkg/controller` has 4 data races. I will file another issue. Turning on the race detector when running `go test` might be a good choice.
"
576	302	thockin	2014-06-30 17:58:25	MEMBER	"This one in particular is not likely to be something that any tool can
catch - it's higher level.

On Mon, Jun 30, 2014 at 10:46 AM, monnand notifications@github.com wrote:

> Oops. I just filed an issue #304
> https://github.com/GoogleCloudPlatform/kubernetes/issues/304. It may be
> related.
> 
> FYI, go's race detector http://blog.golang.org/race-detector is your
> friends.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/302#issuecomment-47562526
> .
"
577	296	thockin	2014-06-30 18:01:15	MEMBER	"It only tracks it in memory.  Should kubelet ever restart, that info is lost.
"
578	302	monnand	2014-06-30 18:08:35	CONTRIBUTOR	"@thockin At least the race detector detected something. Did not read the message yet, it could be found here #304
"
579	302	thockin	2014-06-30 18:09:32	MEMBER	"Yes, we should look at that race, too :)

On Mon, Jun 30, 2014 at 11:08 AM, monnand notifications@github.com wrote:

> @thockin https://github.com/thockin At least the race detector detected
> something. Did not read the message yet, it could be found here #304
> https://github.com/GoogleCloudPlatform/kubernetes/issues/304
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/302#issuecomment-47565263
> .
"
580	306	monnand	2014-06-30 18:18:00	CONTRIBUTOR	"Changed to mutex. PTAL.
"
581	306	lavalamp	2014-06-30 18:19:17	MEMBER	"LGTM
"
582	306	monnand	2014-06-30 18:20:10	CONTRIBUTOR	"Thank you, @lavalamp.
"
583	280	proppy	2014-06-30 18:29:08	CONTRIBUTOR	"Curious if you considered using `jsonschema` for the validation?

There is a go implementation here:
https://github.com/xeipuuv/gojsonschema

And I did a schema for the old manifest here:
https://github.com/proppy/container-agent/blob/9df44d84241948a20c927e633594961904bf65b4/container_agent/manifest-schema.yaml
"
584	167	brendandburns	2014-06-30 19:10:04	CONTRIBUTOR	"We got your CLA today.

Any chance you can extract the Docker API calls for Image Pulling (rather than shelling out) into a separate PR?

Many thanks!
--brendan
"
585	306	monnand	2014-06-30 19:13:43	CONTRIBUTOR	"Good to merge?
"
586	306	lavalamp	2014-06-30 19:15:32	MEMBER	"Ah, just double checking our shiney new policy. I think this counts as a ""small"" change not needing to LGTMs. :)
"
587	306	monnand	2014-06-30 19:16:11	CONTRIBUTOR	"Thank you!
"
588	294	smarterclayton	2014-06-30 19:39:05	CONTRIBUTOR	"If Pod ID is required, and a ContainerManifest ID is also required, should the pod controller duplicate the pod id into the container manifest id if the container manifest id is not specified?  Then the replication controller should also generate a unique container manifest ID in the template on creation if one is not specified.  

I will make that change in #253 if so
"
589	293	brendandburns	2014-06-30 19:42:16	CONTRIBUTOR	"Ok, I think this qualifies for a 2 person review under the new guidelines @thockin can you hook me up?

(or @smarterclayton or @jjhuff ;)
"
590	280	thockin	2014-06-30 20:01:18	MEMBER	"We need to validate YAML, too.  And more, I am not sure we will never need
to validate other inputs, so I chose to centralize it.  I'm not super happy
with it, though.

On Mon, Jun 30, 2014 at 11:29 AM, Johan Euphrosine <notifications@github.com

> wrote:
> 
> Curious if you considered using jsonschema for the validation?
> 
> There is a go implementation here:
> https://github.com/xeipuuv/gojsonschema
> 
> And I did a schema for the old manifest here:
> 
> https://github.com/proppy/container-agent/blob/9df44d84241948a20c927e633594961904bf65b4/container_agent/manifest-schema.yaml
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/280#issuecomment-47567949
> .
"
591	280	thockin	2014-06-30 20:01:39	MEMBER	"I'm going to break this into a bunch of smaller pulls while I work out the
last few issues.

On Mon, Jun 30, 2014 at 1:00 PM, Tim Hockin thockin@google.com wrote:

> We need to validate YAML, too.  And more, I am not sure we will never need
> to validate other inputs, so I chose to centralize it.  I'm not super happy
> with it, though.
> 
> On Mon, Jun 30, 2014 at 11:29 AM, Johan Euphrosine <
> notifications@github.com> wrote:
> 
> > Curious if you considered using jsonschema for the validation?
> > 
> > There is a go implementation here:
> > https://github.com/xeipuuv/gojsonschema
> > 
> > And I did a schema for the old manifest here:
> > 
> > https://github.com/proppy/container-agent/blob/9df44d84241948a20c927e633594961904bf65b4/container_agent/manifest-schema.yaml
> > 
> > ## 
> > 
> > Reply to this email directly or view it on GitHub
> > https://github.com/GoogleCloudPlatform/kubernetes/pull/280#issuecomment-47567949
> > .
"
592	308	lavalamp	2014-06-30 21:19:18	MEMBER	"LGTM
"
593	308	lavalamp	2014-06-30 21:33:02	MEMBER	"Still LGTM
"
594	280	proppy	2014-06-30 21:42:17	CONTRIBUTOR	"@thockin I know it's a bit weird but `jsonschema` python module was validating `dict`, so it also worked with `yaml` input.

I suspect Go implementation just validate struct (or map) fields using reflection too.
"
595	199	smarterclayton	2014-06-30 21:48:26	CONTRIBUTOR	"Writing up a summary md - does this belong in api/doc, DESIGN.md, or another location?
"
596	293	jjhuff	2014-06-30 21:49:37	CONTRIBUTOR	"I'm about to head out on vacation so this is a perfect time for me to review code:)
At some point in the near future it might make sense for minions to 'register themselves' in etcd (or via http to the master) under /registry/hosts/<fqdn>/status or something.
"
597	293	lavalamp	2014-06-30 21:53:07	MEMBER	"> At some point in the near future it might make sense for minions to 'register themselves' in etcd (or via http to the master) under /registry/hosts//status or something.

Yeah, we've got a couple issues filed about this (security is hard). I think it makes sense to get them from the cloud provider if you're running with a cloud provider, though.
"
598	293	smarterclayton	2014-06-30 21:54:16	CONTRIBUTOR	"+1 - allowing an external source (a different Cloud implementation) to define the list helps for other integration cases by allowing trust delegation.
"
599	308	lavalamp	2014-06-30 22:22:51	MEMBER	"Merging, since this is now a small change :)
"
600	311	lavalamp	2014-06-30 22:24:16	MEMBER	"LGTM
"
601	312	lavalamp	2014-06-30 22:25:02	MEMBER	"LGTM
"
602	309	monnand	2014-06-30 22:34:08	CONTRIBUTOR	"@lavalamp PTAL.
"
603	313	lavalamp	2014-06-30 22:50:51	MEMBER	"Your git history is showing :)

Tip: never merge (or `git pull`). I'm joking, but only a little bit. Use `git fetch` and rebase instead. In fact, do it now to get rid of the ""Merge"" commit up there: `git rebase upstream/master`

After that, run `git rebase -i HEAD~3` and change three of your commits to be `fixup`s. In the future, you can use git commit --amend to modify your last commit.

After that, you should be left with a single commit. You can re-push with `git push -f origin fileserver`.
"
604	245	lavalamp	2014-06-30 23:09:18	MEMBER	"I think the other was Key vs Name in the Env section.
"
605	293	thockin	2014-06-30 23:11:13	MEMBER	"LGTM, modulo comments about empty fqdn.
"
606	309	monnand	2014-06-30 23:11:18	CONTRIBUTOR	"@lavalamp  PTAL, switched to sync.Cond.
"
607	245	thockin	2014-06-30 23:12:36	MEMBER	"For simple fields renames, is it sufficient to define both fields in go and
(once we have validation) canonicalize internally?

On Mon, Jun 30, 2014 at 4:09 PM, Daniel Smith notifications@github.com
wrote:

> I think the other was Key vs Name in the Env section.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/245#issuecomment-47598977
> .
"
608	294	thockin	2014-06-30 23:14:57	MEMBER	"I think so.  Re3plication controller should add a short random
prefix/suffix to each ID

On Mon, Jun 30, 2014 at 12:39 PM, Clayton Coleman notifications@github.com
wrote:

> If Pod ID is required, and a ContainerManifest ID is also required, should
> the pod controller duplicate the pod id into the container manifest id if
> the container manifest id is not specified? Then the replication controller
> should also generate a unique container manifest ID in the template on
> creation if one is not specified.
> 
> I will make that change in #253
> https://github.com/GoogleCloudPlatform/kubernetes/pull/253 if so
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/294#issuecomment-47576577
> .
"
609	286	thockin	2014-06-30 23:15:33	MEMBER	"LGTM
"
610	245	lavalamp	2014-06-30 23:16:55	MEMBER	"We could declare a ""for marshalling"" struct and write code to copy it over. I'd be unhappy to have both field names in our public types with comments saying ""don't use this"" for one of them.
"
611	313	dchen1107	2014-06-30 23:18:46	MEMBER	"I addressed your last comment. PTAL?
"
612	313	lavalamp	2014-06-30 23:22:36	MEMBER	"Awesome. LGTM. Can you squash your commit history (`git rebase -i HEAD~4`; change the last three ""pick"" to ""fixup"") and re-push before I merge?
"
613	245	thockin	2014-06-30 23:22:42	MEMBER	"Brendan and I were talking about that this morning - we both agree that we
should do this, but I want to try to do it cleanly.  In the short term, to
get validation in, I want to have compat.

On Mon, Jun 30, 2014 at 4:17 PM, Daniel Smith notifications@github.com
wrote:

> We could declare a ""for marshalling"" struct and write code to copy it
> over. I'd be unhappy to have both field names in our public types with
> comments saying ""don't use this"" for one of them.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/245#issuecomment-47599582
> .
"
614	304	dchen1107	2014-06-30 23:38:26	MEMBER	"Anyone is working on this one? If no, I plan to take it. Thanks!
"
615	304	lavalamp	2014-06-30 23:39:50	MEMBER	"I think @monnand has already got PRs in progress that fix it?
"
616	313	lavalamp	2014-06-30 23:53:29	MEMBER	"Thanks for the change!
"
617	304	monnand	2014-07-01 00:58:43	CONTRIBUTOR	"@dchen1107 @lavalamp I'm not working on this package, so feel free to change it.
"
618	318	brendandburns	2014-07-01 03:12:07	CONTRIBUTOR	"Generally LGTM, but tests are failing.
"
619	314	brendandburns	2014-07-01 03:21:31	CONTRIBUTOR	"LGTM
"
620	307	brendandburns	2014-07-01 03:24:44	CONTRIBUTOR	"Small comment, LGTM.
"
621	309	monnand	2014-07-01 03:52:16	CONTRIBUTOR	"@brendanburns All comments are address. PTAL.
"
622	274	monnand	2014-07-01 03:55:17	CONTRIBUTOR	"Right now, kubelet could pull data from cAdvisor. However, since cAdvisor currently could not read root container information without lmctfy, we could not get machine level usage percentiles.
"
623	103	brendandburns	2014-07-01 04:15:55	CONTRIBUTOR	"Closing this.  Kubecfg is sync by default, and when sync, the replication controller waits for the replicas to reach hysterisis.

So the pattern is:

kubecfg resize myController 0
kubecfg delete replicationControllers/myController

That shouldn't orphan any pods.
"
624	272	brendandburns	2014-07-01 04:36:51	CONTRIBUTOR	"Closed by #279 
"
625	302	brendandburns	2014-07-01 04:39:53	CONTRIBUTOR	"I thought about this some more.  I don't think that there is a race here.  /.../pods is supposed to contain the complete state for the kubelet.  So if we miss data, it doesn't matter, we get it on the subsequent watch.

If a container was added and then removed, and we miss it, then that is ok, since the current ""desired state"" is removed anyway.

Am I missing something?
"
626	297	brendandburns	2014-07-01 04:43:34	CONTRIBUTOR	"We actually had this a while ago (well byte-wise comparison anyway)

I think this is pretty low on the list of desired optimizations, although refactoring all of the data sources in way that would make this easy to add generically, is probably a good idea.
"
627	302	thockin	2014-07-01 04:45:31	MEMBER	"As long as we periodically force-resync, it's OK.  But there was some
discussion here about reducing the noise by making each source only submit
updates when it believes something has actually changed.  For example, set
an inotify watch, load initial state, while true handle inotify events.

I don't know etcd well enough to say whether it works this way, but I'll
bet 10 bucks it could.

This is not a very important change to make right now, so I am letting it
simmer.  I wanted to file an issue so I don't forget.

On Mon, Jun 30, 2014 at 9:40 PM, brendandburns notifications@github.com
wrote:

> I thought about this some more. I don't think that there is a race here.
> /.../pods is supposed to contain the complete state for the kubelet. So if
> we miss data, it doesn't matter, we get it on the subsequent watch.
> 
> If a container was added and then removed, and we miss it, then that is
> ok, since the current ""desired state"" is removed anyway.
> 
> Am I missing something?
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/302#issuecomment-47616296
> .
"
628	318	thockin	2014-07-01 04:59:14	MEMBER	"LGTM, fix tests.
"
629	284	bgrant0607	2014-07-01 05:44:04	MEMBER	"Both at the apiserver and in kubelet, I was thinking the easiest starting
point would be to add links to list the different types of objects, which
would literally be URLs referring to the corresponding API calls. We could
perhaps conditionalize subsequent hyperlinks based on the mediatype
requested. That would at least make it possible to do through the UI what's
already possible via the CLI.
"
630	284	dchen1107	2014-07-01 05:47:00	MEMBER	"+1 on bgrant0607@

btw, https://${apiserver}/logs does work now. I am working on kubelet one.
But looks like kubelet_server's http page doesn't work at all.

On Mon, Jun 30, 2014 at 10:44 PM, bgrant0607 notifications@github.com
wrote:

> Both at the apiserver and in kubelet, I was thinking the easiest starting
> point would be to add links to list the different types of objects, which
> would literally be URLs referring to the corresponding API calls. We could
> perhaps conditionalize subsequent hyperlinks based on the mediatype
> requested. That would at least make it possible to do through the UI what's
> already possible via the CLI.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/284#issuecomment-47619056
> .
"
631	253	smarterclayton	2014-07-01 14:50:49	CONTRIBUTOR	"Currently I'm using full UUID - in practice and for readability I could drop it down to 64 bits and then do a base64 (omitting - and _).  That would make generated ids immediately more human readable from the apiserver and kubelet, at the risk of moving to a 1 in 10 million chance of a collision with ~5 million containers.
"
632	284	brendandburns	2014-07-01 15:06:19	CONTRIBUTOR	"How are you trying to access it?

By default I don't think that firewalls are set up to show external traffic
to the Kubelet's http server.

Brendan
On Jun 30, 2014 10:47 PM, ""Dawn Chen"" notifications@github.com wrote:

> +1 on bgrant0607@
> 
> btw, https://${apiserver}/logs does work now. I am working on kubelet
> one.
> But looks like kubelet_server's http page doesn't work at all.
> 
> On Mon, Jun 30, 2014 at 10:44 PM, bgrant0607 notifications@github.com
> wrote:
> 
> > Both at the apiserver and in kubelet, I was thinking the easiest
> > starting
> > point would be to add links to list the different types of objects,
> > which
> > would literally be URLs referring to the corresponding API calls. We
> > could
> > perhaps conditionalize subsequent hyperlinks based on the mediatype
> > requested. That would at least make it possible to do through the UI
> > what's
> > already possible via the CLI.
> > 
> > —
> > Reply to this email directly or view it on GitHub
> > <
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/284#issuecomment-47619056>
> > 
> > .
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/284#issuecomment-47619196
> .
"
633	302	brendandburns	2014-07-01 15:10:40	CONTRIBUTOR	"Yes, etcd does support watch.  However, I empirically saw problem where
watches hung and didn't return data, that's why the sync is there.  The
CoreOS folks acknowledged that there were some problems there that are
going to be fixed in subsequent versions...

I agree that ideally we'd just watch all the time, but in practice, I think
resync is valuable insurance.
On Jun 30, 2014 9:45 PM, ""Tim Hockin"" notifications@github.com wrote:

> As long as we periodically force-resync, it's OK. But there was some
> discussion here about reducing the noise by making each source only submit
> updates when it believes something has actually changed. For example, set
> an inotify watch, load initial state, while true handle inotify events.
> 
> I don't know etcd well enough to say whether it works this way, but I'll
> bet 10 bucks it could.
> 
> This is not a very important change to make right now, so I am letting it
> simmer. I wanted to file an issue so I don't forget.
> 
> On Mon, Jun 30, 2014 at 9:40 PM, brendandburns notifications@github.com
> wrote:
> 
> > I thought about this some more. I don't think that there is a race here.
> > /.../pods is supposed to contain the complete state for the kubelet. So
> > if
> > we miss data, it doesn't matter, we get it on the subsequent watch.
> > 
> > If a container was added and then removed, and we miss it, then that is
> > ok, since the current ""desired state"" is removed anyway.
> > 
> > Am I missing something?
> > 
> > ## 
> > 
> > Reply to this email directly or view it on GitHub
> > <
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/302#issuecomment-47616296
> > 
> > .
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/302#issuecomment-47616528
> .
"
634	274	vmarmol	2014-07-01 16:05:18	CONTRIBUTOR	"@monnand I think that's a bug on cAdvisor no? We _should_ be reading stats.
"
635	284	dchen1107	2014-07-01 16:34:38	MEMBER	"Yes, I realized that right after I sent out last message, and plan to
propose to reserve a port for external traffic, so that I can config
firewalls for minons.

On Tue, Jul 1, 2014 at 8:06 AM, brendandburns notifications@github.com
wrote:

> How are you trying to access it?
> 
> By default I don't think that firewalls are set up to show external
> traffic
> to the Kubelet's http server.
> 
> Brendan
> On Jun 30, 2014 10:47 PM, ""Dawn Chen"" notifications@github.com wrote:
> 
> > +1 on bgrant0607@
> > 
> > btw, https://${apiserver}/logs does work now. I am working on kubelet
> > one.
> > But looks like kubelet_server's http page doesn't work at all.
> > 
> > On Mon, Jun 30, 2014 at 10:44 PM, bgrant0607 notifications@github.com
> > wrote:
> > 
> > > Both at the apiserver and in kubelet, I was thinking the easiest
> > > starting
> > > point would be to add links to list the different types of objects,
> > > which
> > > would literally be URLs referring to the corresponding API calls. We
> > > could
> > > perhaps conditionalize subsequent hyperlinks based on the mediatype
> > > requested. That would at least make it possible to do through the UI
> > > what's
> > > already possible via the CLI.
> > > 
> > > —
> > > Reply to this email directly or view it on GitHub
> > > <
> > 
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/284#issuecomment-47619056>
> > 
> > > .
> > 
> > —
> > Reply to this email directly or view it on GitHub
> > <
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/284#issuecomment-47619196>
> > 
> > .
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/284#issuecomment-47667760
> .
"
636	302	thockin	2014-07-01 16:36:21	MEMBER	"Even if we re-sync, we can keep enough state to detect a ""no change""
situation, and thereby reduce noise on the update channel.  But it's low
prio.

On Tue, Jul 1, 2014 at 8:10 AM, brendandburns notifications@github.com
wrote:

> Yes, etcd does support watch. However, I empirically saw problem where
> watches hung and didn't return data, that's why the sync is there. The
> CoreOS folks acknowledged that there were some problems there that are
> going to be fixed in subsequent versions...
> 
> I agree that ideally we'd just watch all the time, but in practice, I
> think
> resync is valuable insurance.
> On Jun 30, 2014 9:45 PM, ""Tim Hockin"" notifications@github.com wrote:
> 
> > As long as we periodically force-resync, it's OK. But there was some
> > discussion here about reducing the noise by making each source only
> > submit
> > updates when it believes something has actually changed. For example,
> > set
> > an inotify watch, load initial state, while true handle inotify events.
> > 
> > I don't know etcd well enough to say whether it works this way, but I'll
> > bet 10 bucks it could.
> > 
> > This is not a very important change to make right now, so I am letting
> > it
> > simmer. I wanted to file an issue so I don't forget.
> > 
> > On Mon, Jun 30, 2014 at 9:40 PM, brendandburns notifications@github.com
> > 
> > wrote:
> > 
> > > I thought about this some more. I don't think that there is a race
> > > here.
> > > /.../pods is supposed to contain the complete state for the kubelet.
> > > So
> > > if
> > > we miss data, it doesn't matter, we get it on the subsequent watch.
> > > 
> > > If a container was added and then removed, and we miss it, then that
> > > is
> > > ok, since the current ""desired state"" is removed anyway.
> > > 
> > > Am I missing something?
> > > 
> > > ## 
> > > 
> > > Reply to this email directly or view it on GitHub
> > > <
> > 
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/302#issuecomment-47616296
> > 
> > > .
> > 
> > ## 
> > 
> > Reply to this email directly or view it on GitHub
> > <
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/302#issuecomment-47616528>
> > 
> > .
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/302#issuecomment-47668313
> .
"
637	284	brendandburns	2014-07-01 16:36:55	CONTRIBUTOR	"Let's be careful here, remember that if you do this, it's exposed to the
world (e.g. the entire internet).  I think that routing it through a
handler on the master might be a more secure approach to this.

--brendan

On Tue, Jul 1, 2014 at 9:34 AM, Dawn Chen notifications@github.com wrote:

> Yes, I realized that right after I sent out last message, and plan to
> propose to reserve a port for external traffic, so that I can config
> firewalls for minons.
> 
> On Tue, Jul 1, 2014 at 8:06 AM, brendandburns notifications@github.com
> wrote:
> 
> > How are you trying to access it?
> > 
> > By default I don't think that firewalls are set up to show external
> > traffic
> > to the Kubelet's http server.
> > 
> > Brendan
> > On Jun 30, 2014 10:47 PM, ""Dawn Chen"" notifications@github.com wrote:
> > 
> > > +1 on bgrant0607@
> > > 
> > > btw, https://${apiserver}/logs does work now. I am working on kubelet
> > > one.
> > > But looks like kubelet_server's http page doesn't work at all.
> > > 
> > > On Mon, Jun 30, 2014 at 10:44 PM, bgrant0607 notifications@github.com
> > > 
> > > wrote:
> > > 
> > > > Both at the apiserver and in kubelet, I was thinking the easiest
> > > > starting
> > > > point would be to add links to list the different types of objects,
> > > > which
> > > > would literally be URLs referring to the corresponding API calls. We
> > > > could
> > > > perhaps conditionalize subsequent hyperlinks based on the mediatype
> > > > requested. That would at least make it possible to do through the UI
> > > > what's
> > > > already possible via the CLI.
> > > > 
> > > > —
> > > > Reply to this email directly or view it on GitHub
> > > > <
> > 
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/284#issuecomment-47619056>
> > 
> > > > .
> > > 
> > > —
> > > Reply to this email directly or view it on GitHub
> > > <
> > 
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/284#issuecomment-47619196>
> > 
> > > .
> > 
> > —
> > Reply to this email directly or view it on GitHub
> > <
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/284#issuecomment-47667760>
> > 
> > .
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/284#issuecomment-47678932
> .
"
638	284	lavalamp	2014-07-01 16:38:43	MEMBER	"Consider the alternative solution of letting apiserver (master) serve an address at which it'll proxy through to a minion. At the cost of extra load on apiserver, you have to worry about fewer firewall rules, which will make this easier to run in alternative places. And you'd have a convenient way to verify that master can talk to kubelets, which is required anyway.
"
639	301	brendandburns	2014-07-01 16:39:11	CONTRIBUTOR	"See #320 
"
640	199	thockin	2014-07-01 16:41:39	MEMBER	"I would go for docs/identifiers.md or something

On Mon, Jun 30, 2014 at 2:48 PM, Clayton Coleman notifications@github.com
wrote:

> Writing up a summary md - does this belong in api/doc, DESIGN.md, or
> another location?
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/199#issuecomment-47591844
> .
"
641	284	lavalamp	2014-07-01 16:42:49	MEMBER	"If Brendan and I independently come up with the same suggestion, it's probably a good one. :)
"
642	253	thockin	2014-07-01 16:56:46	MEMBER	"Are you ready for a thorough review on this one?

On Tue, Jul 1, 2014 at 7:50 AM, Clayton Coleman notifications@github.com
wrote:

> Currently I'm using full UUID - in practice and for readability I could
> drop it down to 64 bits and then do a base64 (omitting - and _). That would
> make generated ids immediately more human readable from the apiserver and
> kubelet, at the risk of moving to a 1 in 10 million chance of a collision
> with ~5 million containers.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/253#issuecomment-47665623
> .
"
643	253	smarterclayton	2014-07-01 16:59:35	CONTRIBUTOR	"General approach and uuid type (full uuid vs shorter random segment) would be helpful
"
644	320	thockin	2014-07-01 17:08:37	MEMBER	"Great change - I'm still learning go idioms, so take my feedback with a grain of salt
"
645	253	thockin	2014-07-01 17:13:29	MEMBER	"It would help a lot for review if you sent a PR just for the Id -> ID change

On Tue, Jul 1, 2014 at 9:59 AM, Clayton Coleman notifications@github.com
wrote:

> General approach and uuid type (full uuid vs shorter random segment) would
> be helpful
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/253#issuecomment-47682170
> .
"
646	293	brendandburns	2014-07-01 17:13:44	CONTRIBUTOR	"I can haz merge?
"
647	293	brendandburns	2014-07-01 17:15:36	CONTRIBUTOR	"oops, forgot to push changes for empty fqdn.  Pushed now.
"
648	293	smarterclayton	2014-07-01 17:15:45	CONTRIBUTOR	"LGTM
"
649	253	smarterclayton	2014-07-01 17:18:50	CONTRIBUTOR	"Done
"
650	318	lavalamp	2014-07-01 17:21:09	MEMBER	"Tests fixed. Broke them while making integration pass, heh.
"
651	293	lavalamp	2014-07-01 17:25:21	MEMBER	"Travis says no merge until you gofmt -s.
"
652	277	brendandburns	2014-07-01 17:28:44	CONTRIBUTOR	"Updated, ptal.

Thanks!
--brendan
"
653	321	brendandburns	2014-07-01 17:32:55	CONTRIBUTOR	"Merging this, travis is borked b/c of gofmt skew.
"
654	321	brendandburns	2014-07-01 17:33:04	CONTRIBUTOR	"(and thanks!)
"
655	322	lavalamp	2014-07-01 17:45:15	MEMBER	"LGTM
"
656	323	lavalamp	2014-07-01 17:47:36	MEMBER	"Do we really need three targets? Can we get rid of 1.2 or tip?
"
657	324	vmarmol	2014-07-01 17:50:07	CONTRIBUTOR	"Tested with kube-up/down. Let me know if there is anything else I should test.
"
658	253	thockin	2014-07-01 17:51:21	MEMBER	"Directionally good.

On Tue, Jul 1, 2014 at 10:18 AM, Clayton Coleman notifications@github.com
wrote:

> Done
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/253#issuecomment-47684401
> .
"
659	320	brendandburns	2014-07-01 17:52:20	CONTRIBUTOR	"rebased.
"
660	323	lavalamp	2014-07-01 17:55:47	MEMBER	"Yeah, let's please remove tip. All changes are failing gofmt in tip...
"
661	323	proppy	2014-07-01 17:56:40	CONTRIBUTOR	"@lavalamp #322 already disable gofmt for `tip`, do you also want to disable build and test for `tip`?
"
662	322	lavalamp	2014-07-01 17:57:11	MEMBER	"Travis failed on go1.2 and the error message isn't helpful...
"
663	322	proppy	2014-07-01 17:58:55	CONTRIBUTOR	"Yes, previous version was printing the name of the file that were not formatted.
"
664	323	lavalamp	2014-07-01 18:00:00	MEMBER	"@proppy Yeah, let's disable tip here and @brendandburns can add it back once he gets #322 working.
"
665	323	proppy	2014-07-01 18:01:31	CONTRIBUTOR	"PTAL
"
666	323	lavalamp	2014-07-01 18:03:48	MEMBER	"Merging this. Hopefully our builds will work again!
"
667	322	lavalamp	2014-07-01 18:06:02	MEMBER	"@proppy turned off building at tip, so rebase, turn tip back on, and fix this at your leisure. :)
"
668	194	brendandburns	2014-07-01 18:14:26	CONTRIBUTOR	"Closed by #319 
"
669	318	lavalamp	2014-07-01 18:15:22	MEMBER	"Can we merge this soon?
"
670	324	lavalamp	2014-07-01 18:20:23	MEMBER	"LGTM
"
671	309	monnand	2014-07-01 18:22:33	CONTRIBUTOR	"@lavalamp All comments addressed. PTAL.
"
672	284	dchen1107	2014-07-01 18:23:17	MEMBER	"I just found my last message from @google.com was lost from github.com:

On Tue, Jul 1, 2014 at 9:44 AM, Dawn Chen dawnchen@google.com wrote:

Even logs? Container / pod's stats / states should be easy and master is going to collect some of them anyway, but not sure about logs here.
"
673	284	brendandburns	2014-07-01 18:30:50	CONTRIBUTOR	"No, you don't need to send the data to the master.  Just gateway the HTTP
traffic, from master to minion.

e.g.

http://master/minion?id=foo

will delegate an HTTP call back out to the minion, and display the results.

Turn part of the master into an http proxy.

--brendan

On Tue, Jul 1, 2014 at 11:23 AM, Dawn Chen notifications@github.com wrote:

> I just found my last message from @google.com was lost from github.com:
> 
> On Tue, Jul 1, 2014 at 9:44 AM, Dawn Chen dawnchen@google.com wrote:
> 
> Even logs? Container / pod's stats / states should be easy and master is
> going to collect some of them anyway, but not sure about logs here.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/284#issuecomment-47692076
> .
"
674	309	lavalamp	2014-07-01 18:31:43	MEMBER	"Thanks for the change! Will merge when travis finishes.
"
675	324	brendandburns	2014-07-01 18:35:23	CONTRIBUTOR	"LGTM, modulo the version comment.
"
676	309	monnand	2014-07-01 18:37:56	CONTRIBUTOR	"Thank you, @lavalamp !
"
677	322	brendandburns	2014-07-01 18:40:19	CONTRIBUTOR	"Fixed, ptal.
"
678	309	lavalamp	2014-07-01 18:41:22	MEMBER	"Oh, we should have squashed this to get rid of the intermediate commits. Oops, next time :)
"
679	318	thockin	2014-07-01 18:45:35	MEMBER	"""We can’t automatically merge this pull request.""
"
680	324	vmarmol	2014-07-01 18:55:17	CONTRIBUTOR	"Updated version, PTAL
"
681	318	lavalamp	2014-07-01 19:10:14	MEMBER	"Argh, I did that to myself by merging Brendan's change. Rebased.
"
682	318	thockin	2014-07-01 19:23:09	MEMBER	"""x Failed — The Travis CI build failed ""
"
683	318	lavalamp	2014-07-01 19:28:20	MEMBER	"It's passing for me locally. Investigating.
"
684	312	brendandburns	2014-07-01 20:14:27	CONTRIBUTOR	"LGTM, looks like it needs a rebase, though...
"
685	326	brendandburns	2014-07-01 20:17:50	CONTRIBUTOR	"LGTM, modulo Daniel's comment.
"
686	193	brendandburns	2014-07-01 20:24:48	CONTRIBUTOR	"This is closed by a combination of the MinionRegistry, the new CloudMinionRegistry and flags in the default GCE set up.

On GCE, if a VM is deleted from the pool, its tasks will now be moved to a different machine.
"
687	193	lavalamp	2014-07-01 20:26:01	MEMBER	"I think that's only true for pods controlled by a replication controller?
"
688	325	monnand	2014-07-01 20:32:02	CONTRIBUTOR	"I think this PR should be P0 because it fixes a break on the CI test. It LGTM, but I do not have permission to merge. Anyone? 
"
689	325	lavalamp	2014-07-01 20:38:21	MEMBER	"It only matters when etcd is flaking, so sadly it will not fix any flaky tests, just improve behavior when they do flake.
"
690	300	brendandburns	2014-07-01 20:39:47	CONTRIBUTOR	"This is now /etc/kubernetes/manifests
"
691	288	brendandburns	2014-07-01 20:40:40	CONTRIBUTOR	"Closing this for now, we could do more, but I think we've gotten rid of the long pole.  Re-open if you disagree.
"
692	326	thockin	2014-07-01 20:49:48	MEMBER	"Done

On Tue, Jul 1, 2014 at 1:17 PM, brendandburns notifications@github.com
wrote:

> LGTM, modulo Daniel's comment.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/326#issuecomment-47704796
> .
"
693	312	thockin	2014-07-01 20:59:33	MEMBER	"rebased.
"
694	318	lavalamp	2014-07-01 21:20:13	MEMBER	"I see what's going on. #277 and this PR don't play well together. I don't yet understand how #277 (which waits for containerInfo to give confirmation that containers are running) passes integration by itself, since even before this change that path shouldn't be working in the integration test.
"
695	318	lavalamp	2014-07-01 22:39:00	MEMBER	"OK. I just went ahead and fixed it for reals. Kubelet now returns info for a whole _pod_ and not the bizarre thing we were doing before with passing a pod id to /containerInfo.
"
696	332	brendandburns	2014-07-01 22:39:14	CONTRIBUTOR	"Thanks (and sorry we missed that) can you sign our CLA as described in CONTRIB.md and we'll merge this in.

Thanks again!
--brendan
"
697	318	lavalamp	2014-07-01 22:41:01	MEMBER	"Note that I'm assuming that pod.ID == manifest.ID. Hopefully that will be true in the future.
"
698	329	lavalamp	2014-07-01 22:42:21	MEMBER	"Can you rebase so we can see what's new in this PR?
"
699	329	thockin	2014-07-01 22:59:46	MEMBER	"Done
"
700	301	vmarmol	2014-07-01 23:11:14	CONTRIBUTOR	"I'd like to assign this to myself since I'm working on making different syncs concurrent, but I don't have write access :)
"
701	199	smarterclayton	2014-07-01 23:20:31	CONTRIBUTOR	"Referenced pull includes a summary of this discussion, open questions:
- implication 3-1 (agree/disagree)?
- I defined ""pod unique name"" and ""pod unique identifier"" to represent thockin's kubelet items 3 and 4
"
702	334	brendandburns	2014-07-01 23:21:57	CONTRIBUTOR	"LGTM, I'll leave for @thockin and others to take a look at before we merge.
"
703	329	brendandburns	2014-07-01 23:24:12	CONTRIBUTOR	"LGTM, I'll let you decide about my comment.  If you want to punt, let's add a TODO and I'm happy to merge.
"
704	318	thockin	2014-07-01 23:25:35	MEMBER	"x Failed — The Travis CI build failed
"
705	318	lavalamp	2014-07-01 23:27:43	MEMBER	"Yeah, once again, it worked locally. Sigh.
"
706	329	thockin	2014-07-01 23:31:04	MEMBER	"TODO added and pushed
"
707	334	lavalamp	2014-07-01 23:32:45	MEMBER	"LGTM
"
708	330	thockin	2014-07-01 23:43:08	MEMBER	"rebased
"
709	318	lavalamp	2014-07-02 00:47:05	MEMBER	"OK, I'm not sure travis _should_ be passing, but it _is_, and so are my local runs. Let's get this approved and merged and then we can investigate why the mere presence of a pod's container's info (sans state{running=true}) is sufficient to convince the system that the pod has been created.

Note that this change is pretty different than it was when it got those LGTMs, so look carefully...
"
710	330	lavalamp	2014-07-02 00:51:16	MEMBER	"LGTM. Travis failure is a flake.
"
711	304	monnand	2014-07-02 01:05:04	CONTRIBUTOR	"I think this is fixed by #331. Thanks @dchen1107 
"
712	318	brendandburns	2014-07-02 05:13:34	CONTRIBUTOR	"This (still) looks ok to me, and since its been hanging out there forever, I'm going to merge it.
"
713	318	thockin	2014-07-02 05:14:08	MEMBER	"You beat me by less than a minute.  LGTM
"
714	333	brendandburns	2014-07-02 05:15:04	CONTRIBUTOR	"rebase?
"
715	333	thockin	2014-07-02 05:17:56	MEMBER	"Done
"
716	336	brendandburns	2014-07-02 16:17:27	CONTRIBUTOR	"Hey David
Thanks for the PR! we actually got an identical PR from another contributer that I merged, since it was on top of the list...

If you plan on sending more PRs can you sign our CLA (as described in the CONTRIB.md file)

Then we'll be able to easily merge PRs in the future.

Many thanks!
--brendan
"
717	337	lavalamp	2014-07-02 16:32:28	MEMBER	"Assigning to @bgrant0607 because he knows all about the /api directory. :)
"
718	339	lavalamp	2014-07-02 17:31:38	MEMBER	"Thanks!
"
719	340	lavalamp	2014-07-02 18:28:37	MEMBER	"Thanks for catching!
"
720	337	bgrant0607	2014-07-02 18:32:10	MEMBER	"LGTM.
"
721	284	monnand	2014-07-02 18:42:27	CONTRIBUTOR	"We are working on the backend storage for cAdvisor now. I just submitted a PR google/cadvisor#74, which will use [influxdb](http://influxdb.com/) as a backend storage to store stats data of all containers tracked by cAdvisor. I think this approach would be better than redirecting all stats to master for the following reasons:
- We could reduce the workload of the master, which is a very important component for the cluster.
- In the future, cAdvisor could support multiple backend storage and users could just check containers' history by only querying those storage components. 
- The storage software (influxdb, mysql, postgres, etc.) itself could handle all security issues and users are more familiar about how to build a safe mysql/postgres/influxdb environment.
- Specific to [influxdb](http://influxdb.com/), it supports a SQL-like language and could display nice graphs based on the results of queries. This feature itself could save a lot of time for us (assuming that most of us are not quite good at UI.)

Because stats data may be huge, it might be better to let another storage service to handle it. For other data, I think going through master node would be fine.
"
722	328	monnand	2014-07-02 20:22:23	CONTRIBUTOR	"@lavalamp @brendanburns @vmarmol All comments are addressed. PTAL.
"
723	307	lavalamp	2014-07-02 20:57:51	MEMBER	"Comment addressed, and made these work. Test in integration test.

I think it's pretty cool, this should make updates atomic for all apiserver resources. 
"
724	310	lavalamp	2014-07-02 20:58:35	MEMBER	"Can we rebase and turn this on? Or are there more races to fix?
"
725	328	monnand	2014-07-02 21:47:58	CONTRIBUTOR	"@lavalamp PTAL. I cleaned the path before split it. But still, we need to drop empty strings.
"
726	310	monnand	2014-07-02 21:51:44	CONTRIBUTOR	"@lavalamp I think we can. But someone need to refresh the CI to make sure it works. I do not have the permission.
"
727	342	lavalamp	2014-07-02 22:01:12	MEMBER	"LGTM
"
728	328	monnand	2014-07-02 22:21:04	CONTRIBUTOR	"@lavalamp PTAL.
"
729	342	thockin	2014-07-02 22:31:47	MEMBER	"I'm going to self-merge this - I call it trivial.
"
730	307	lavalamp	2014-07-02 22:42:58	MEMBER	"Comments addressed. PTAL
"
731	343	lavalamp	2014-07-02 23:18:25	MEMBER	"Perhaps squash the ""pub"" commit before we merge? `rebase -i HEAD~2` and use ""fixup""-- there may be easier ways, but that works.
"
732	343	thockin	2014-07-02 23:18:57	MEMBER	"Squashed.  Forgot to do that, sorry
"
733	343	lavalamp	2014-07-02 23:25:16	MEMBER	"Thanks for the change!
"
734	345	lavalamp	2014-07-02 23:40:58	MEMBER	"Aside from a few comments, this LGTM. Thanks for this change, it should go a long ways towards making our IDs sensible!
"
735	328	lavalamp	2014-07-02 23:43:57	MEMBER	"This is looking good, I think. Can you `git rebase -i HEAD~7` and change the intermediate commits to `fixup` so our commit history looks pretty?
"
736	310	lavalamp	2014-07-02 23:45:16	MEMBER	"@monnand, I think if you just

git fetch upstream
git rebase upstream/master
git push -f origin race-detector

It will auto-refresh and travis will do another build.
"
737	345	brendandburns	2014-07-02 23:46:14	CONTRIBUTOR	"Thanks for the PR!  LGTM, modulo Daniel's comments.
"
738	328	monnand	2014-07-02 23:47:39	CONTRIBUTOR	"@lavalamp Sure. I'm working on it.
"
739	328	monnand	2014-07-02 23:56:14	CONTRIBUTOR	"@lavalamp Done. I squashed 7 recent commits. Do you want me to squash all comments?
"
740	328	monnand	2014-07-03 00:01:01	CONTRIBUTOR	"@lavalamp I think you may want me to squash all comments into single one. Done.
"
741	310	monnand	2014-07-03 00:02:45	CONTRIBUTOR	"rebased. But nothing happen. Let me add a comment to the sell script and it will trigger the CI.
"
742	328	lavalamp	2014-07-03 00:03:48	MEMBER	"Thanks, LGTM! I'll let @thockin or @brendandburns give a final look and they can merge.
"
743	328	monnand	2014-07-03 00:04:51	CONTRIBUTOR	"@lavalamp Thank you!
"
744	310	monnand	2014-07-03 00:09:18	CONTRIBUTOR	"Don't merge it. Any recent commit changed the controller package? There are new data races in that package.
"
745	310	monnand	2014-07-03 00:34:56	CONTRIBUTOR	"I checked it and the reason is using `sync.Cond`. I send another PR #348 to fix this.
"
746	348	monnand	2014-07-03 00:37:00	CONTRIBUTOR	"In this PR, I also turned on the race detector. If the CI passed, then it means there's no data races detected by the race detector. Once this PR merged, the race detector will be turned on by default. #310 will not be necessary.
"
747	348	monnand	2014-07-03 00:46:39	CONTRIBUTOR	"There's another data race in kubelet package introduced by 1798e0fe (in kubelet.go, line 705). I'm going to fix it.
"
748	348	monnand	2014-07-03 01:07:51	CONTRIBUTOR	"Fixed data race introduced by 1798e0f. Let's see what CI say.
"
749	348	monnand	2014-07-03 01:15:25	CONTRIBUTOR	"OK. I did not notice there's a `proxy/config` package. There's data race.
"
750	346	smarterclayton	2014-07-03 02:10:00	CONTRIBUTOR	"LGTM
"
751	348	monnand	2014-07-03 05:16:25	CONTRIBUTOR	"God. The data race in `proxy/config` is a hard one. `NewServiceConfig()` returns a value, not a pointer. It took me several hours to figure it out that two objects are sharing same map! 

Besides, there's another easy-to-fix data race.

Lessons learned: Do not over use goroutine and channels. They are awesome, but in most cases, we don't need to pass messages around. Go is not erlang, there's side effect and it's hard to debug --- even with a race detector.

On my machine, all test passed with race detector turned on. 
"
752	348	monnand	2014-07-03 05:36:01	CONTRIBUTOR	"CI only failed on go 1.2. However, I was not able to reproduce it even with go1.2. Anyone wants to take a look? Probably we should remove Go 1.2 support in .travis.yml?  ping @brendanburns @lavalamp @thockin 
"
753	348	monnand	2014-07-03 05:57:04	CONTRIBUTOR	"This is interesting!

With go 1.2 tool chain, if I run: `go test -race -cover -coverprofile=tmp.out github.com/GoogleCloudPlatform/kubernetes/pkg/apiserve`, it will report data race.

If I run `go test -race github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver`, it will pass the test.

In short, with go1.2, do not run race detector with code coverage program together.

I don't know if it is a bug in our code, or a bug in race detector, or a bug in the code coverage program. I tried to understand the race detector's message, but got nothing useful. I think  it's unlikely to be a bug in our code. Do we need to change the test script? Or simply ignore go 1.2?
"
754	328	monnand	2014-07-03 05:59:15	CONTRIBUTOR	"Thank you @brendanburns !

BTW, you may want to take a look at #348. It is hilarious. (read my last comment.)
"
755	348	brendandburns	2014-07-03 06:02:41	CONTRIBUTOR	"Let's optionalize coverage, and turn it off in Travis.

Users can opt-in to turning it on, when they want to see coverage stats.

(also perhaps send a bug report to the go team, although given that its in
1.2, it might be off their roadmap to fix it)

--brendan

On Wed, Jul 2, 2014 at 10:57 PM, monnand notifications@github.com wrote:

> This is interesting!
> 
> With go 1.2 tool chain, if I run: go test -race -cover
> -coverprofile=tmp.out
> github.com/GoogleCloudPlatform/kubernetes/pkg/apiserve, it will report
> data race.
> 
> If I run go test -race
> github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver, it will pass the
> test. I don't know if it is a bug in our code, or a bug in race detector,
> or a bug in the code coverage program. I tried to understand the race
> detector's message, but got nothing useful. I think it's unlikely to be a
> bug in our code. Do we need to change the test script? Or simply ignore go
> 1.2?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/348#issuecomment-47869545
> .
"
756	348	monnand	2014-07-03 06:09:10	CONTRIBUTOR	"@brendandburns Turned off the coverage. Not quite familiar with shell script. PTAL.

I think we don't need to file this bug to Go team because it is fixed in Go 1.3. And Go only maintain one stable branch.
"
757	350	monnand	2014-07-03 06:09:48	CONTRIBUTOR	"LGTM. But I think we need another person's approval.
"
758	350	brendandburns	2014-07-03 06:15:30	CONTRIBUTOR	"I'm going to merge this now, this is a bad enough behavior (it's killing the cadvisor pods, for example) that I'd like to get the fix in before the long weekend.
"
759	348	monnand	2014-07-03 06:16:30	CONTRIBUTOR	"@brendanburns Turning off the coverage does not help. But this time, it is definitely a bug in go. It reports an error:

```
/tmp/go-build776683792/github.com/GoogleCloudPlatform/kubernetes/pkg/labels/_test/_testmain.go:5: can't find import: ""regexp""
FAIL    github.com/GoogleCloudPlatform/kubernetes/pkg/labels [build failed]
```

I have same problem when I use travis with cAdvisor. I think we may want to only use 1.3 and tip.
"
760	348	monnand	2014-07-03 06:33:57	CONTRIBUTOR	"@brendanburns I removed go 1.2 from .travis.yml and added the coverage back, which is useful and un-harmful now. PTAL. And we passed the CI with race detector turning on! :+1: 
"
761	345	monnand	2014-07-03 06:37:51	CONTRIBUTOR	"I have a small request: Would you please run the race detector first? Or could we wait for #348 getting merged?

I'm sorry, after fixing data races for two days, I think it might be better to make sure the code could pass the race detector before merging it.
"
762	310	monnand	2014-07-03 06:39:34	CONTRIBUTOR	"#348 is ready to review. Once #348 is merged, I will close this issue because #348 has turned the race detector on.
"
763	302	tristanz	2014-07-03 08:33:18	NONE	"I believe this is exactly the purpose of `waitIndex` in etcd.  You can to an initial sync and then issue each watch with the last `modifiedIndex` as the new `waitIndex`.  This should ensure no missed events.  There's no need to re-sync.  See:

https://godoc.org/github.com/coreos/go-etcd/etcd#Client.Watch

The only caveat  is etcd's snapshot policy, but the default is to always keep a good deal of history for this exact purpose and you can tune it as desired.

https://github.com/coreos/etcd/issues/285
"
764	336	dgageot	2014-07-03 10:16:05	NONE	"CLA signed for next time
"
765	167	discordianfish	2014-07-03 12:05:21	CONTRIBUTOR	"OK, I've created #351 
"
766	351	thockin	2014-07-03 15:27:22	MEMBER	"I have two questions

1) Does the API correctly handle pulling an image that is already being pulled?  The CLI recognizes this and blocks the caller until it is done, which is simple to handle.

2) Why is this better than shelling out?  Other than cleanliness, shelling out has the nice property that it is less code.  I have this same question in general - why bother with the API and the library deps to use it, when I can just assemble a commandline and exec it?
"
767	351	brendandburns	2014-07-03 16:30:13	CONTRIBUTOR	"Personally I like using the api, since it provides a structured response,
and higher granularity input.

It also provides better future proofing.  APIs have deprecation policies,
etc.  CLI flags and options, not so much.

I haven't looked at the PR (on my phone) but can we make certain that the
bits of code derived from the docker code, are clearly marked and isolated
in a separate file, with a clear copyright header?

Thanks
Brendan
On Jul 3, 2014 8:27 AM, ""Tim Hockin"" notifications@github.com wrote:

> I have two questions
> 
> 1) Does the API correctly handle pulling an image that is already being
> pulled? The CLI recognizes this and blocks the caller until it is done,
> which is simple to handle.
> 
> 2) Why is this better than shelling out? Other than cleanliness, shelling
> out has the nice property that it is less code. I have this same question
> in general - why bother with the API and the library deps to use it, when I
> can just assemble a commandline and exec it?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/351#issuecomment-47944655
> .
"
768	345	smarterclayton	2014-07-03 16:40:17	CONTRIBUTOR	"@monnand  no additional data races reported (other than the existing one in TestWatchControllers)
"
769	345	thockin	2014-07-03 16:40:52	MEMBER	"I'm unclear on what exactly JSONBase.ID is supposed to represent.  Is it a globally unique ID for each api-managed object?  Is it a transaction ID?  Something in-between?

I'm also finding it hard to think about this change because I am still mulling the other discussion about the different forms of name and ID.  It feels weird that ID is in JSONBase for some objects and not others.

I'm not blocking this change at this point, but I feel like I don't quite get it yet.  Something somewhere has not clicked for me :)
"
770	351	smarterclayton	2014-07-03 16:47:21	CONTRIBUTOR	"Actually, with Docker the CLI deprecation policy has been much stronger than the API deprecation policy.  With 1.0 in place the API now has a much stronger change policy, but I would say they're equally stable now.
"
771	351	discordianfish	2014-07-03 17:48:54	CONTRIBUTOR	"Ok, I discussed that parsing in #docker-dev and it seems it's not necessary to do that because the 'fromImage' parameter (which is called, a bit misfortune, 'Repository' in go-dockerclient) gets parsed server side for repo and so on. So I've removed the parsing.

The docker API now is stable, nobody should run docker < 1.0 already because the security fixes in 1.0.
Regarding the parallel pulls, I will look into that tomorrow. I also need to verify that the fromImage parameter can include tags or whether we need to parse the tag from the name first. So hold off with merging for now.
"
772	348	monnand	2014-07-03 21:22:26	CONTRIBUTOR	"Thank you, @brendanburns 
"
773	345	brendandburns	2014-07-03 21:25:25	CONTRIBUTOR	"I intended ID to be a unique GUID that identifies a particular type of resource in the system.

So all Pods should have unique ID, all replication controllers should have unique ID (though a replication controller and a pod might have the same id, since they're in different namespaces)

as far as I intended, it is not a transaction ID, except in the case of an ServerOp, and even their it is a GUID, its just a GUID for an object that represents a transaction.

I think its different than the name of a container inside a pod, for example, since that is a semantic construct used for identifying (and possibly for DNS allocation, etc), but will never be used by the API itself to reference to anything.

For a while, I actually had ID (GUID) and Name (semantic identifier) but that actually got too confusing, so I ripped out Name...
"
774	310	monnand	2014-07-03 21:30:48	CONTRIBUTOR	"#348 has merged. I'll close this issue now.
"
775	353	brendandburns	2014-07-04 03:44:11	CONTRIBUTOR	"Hey Clayton,
I think in general, I was trying to have it only return when it was ""done"" but as you point out, there are lots of problems with this, I think that falling back to ""in our control"" is perhaps the correct semantic.

We've been thinking along similar lines, namely that we should only do acceptance testing (e.g. ""is this pod feasible"") before returning 200 and stopping waiting, instead of waiting for the task to go fully running.

I took a rough design that @lavalamp proposed, and pasted it into:

https://github.com/GoogleCloudPlatform/kubernetes/issues/354

So if that design makes sense to you, and achieves the the same goals as what you are suggesting (I think it does) let's move towards that.  The first thing might to roll back the other PR, and then move forward from the previous state.

Thanks
--brendan
"
776	355	brendandburns	2014-07-04 06:25:22	CONTRIBUTOR	"I'm going to merge this, it's passed once, and it passes on my machine, and (more importantly) it makes the e2e tests pass.

We need to get rid of nginx, I think.  Its doing retries in bad places, and making the e2e unreliable.
"
777	351	discordianfish	2014-07-04 12:55:02	CONTRIBUTOR	"So the parallel pulls should behave the same: It blocks until the pull finished, then returns:
https://github.com/dotcloud/docker/blob/master/server/server.go#L1365

But the tags need to provided explicitly. So if Pull(image string) should support tags (user/image:tag), we need to either parse the tag or extend kubernetes container structs to support a tags attribute.

Parsing the tag is rather ugly because we basically need to parse everything which, as you saw earlier, is pretty complex.

So to me the right way to solve this is either:
1. Make Docker parse the tag from the string
2. Add tag to kubernetes container representation

What do you think?
"
778	341	meirf	2014-07-04 20:08:52	CONTRIBUTOR	"I've started working on this. I will submit a PR over the next day or so.
"
779	351	thockin	2014-07-05 02:57:21	MEMBER	"Where do you see tag in the docker API?   Maybe I'm looking in the wrong
place - I don't see them mentioned at all..

I'm in favor of (1) - this is not something the client should be expected
to parse, I think.

On Fri, Jul 4, 2014 at 5:55 AM, discordianfish notifications@github.com
wrote:

> So the parallel pulls should behave the same: It blocks until the pull
> finished, then returns:
> https://github.com/dotcloud/docker/blob/master/server/server.go#L1365
> 
> But the tags need to provided explicitly. So if Pull(image string) should
> support tags (user/image:tag), we need to either parse the tag or extend
> kubernetes container structs to support a tags attribute.
> 
> Parsing the tag is rather ugly because we basically need to parse
> everything which, as you saw earlier, is pretty complex.
> 
> So to me the right way to solve this is either:
> 1. Make Docker parse the tag from the string
> 2. Add tag to kubernetes container representation
> 
> What do you think?
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/351#issuecomment-48041114
> .
"
780	310	thockin	2014-07-05 03:06:54	MEMBER	"Is current master supposed to test cleanly?  It doesn't for me.  Lots of
races..

On Thu, Jul 3, 2014 at 2:30 PM, monnand notifications@github.com wrote:

> Closed #310 https://github.com/GoogleCloudPlatform/kubernetes/pull/310.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/310#event-138110355
> .
"
781	350	thockin	2014-07-05 03:19:40	MEMBER	"LGTM.  That is a really nefarious bug.  Why the language allows it is baffling to me (and according to google, many others, too)
"
782	345	thockin	2014-07-05 03:35:25	MEMBER	"Should we use JSONBase.ID  instead of structure-specific IDs with
structure-specific semantics (e.g. ""this ID survives pod moves and
restarts"" vs ""this ID is changed on every reschedule"") ?

Or put another way, the ContainerManifest does not include JSONBase.
 Should it?

On Thu, Jul 3, 2014 at 2:25 PM, brendandburns notifications@github.com
wrote:

> I intended ID to be a unique GUID that identifies a particular type of
> resource in the system.
> 
> So all Pods should have unique ID, all replication controllers should have
> unique ID (though a replication controller and a pod might have the same
> id, since they're in different namespaces)
> 
> as far as I intended, it is not a transaction ID, except in the case of an
> ServerOp, and even their it is a GUID, its just a GUID for an object that
> represents a transaction.
> 
> I think its different than the name of a container inside a pod, for
> example, since that is a semantic construct used for identifying (and
> possibly for DNS allocation, etc), but will never be used by the API itself
> to reference to anything.
> 
> For a while, I actually had ID (GUID) and Name (semantic identifier) but
> that actually got too confusing, so I ripped out Name...
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/345#issuecomment-47986654
> .
"
783	345	smarterclayton	2014-07-05 17:35:08	CONTRIBUTOR	"[This implies](https://github.com/GoogleCloudPlatform/kubernetes/pull/345#discussion_r14491699) to me that ContainerManifest should not have an ID - it was a temporary issue that the presence of a pod uuid resolves, so I marked it as deprecated in the latest commit.  Now, if ContainerManifest becomes an API resource as commented in another issue, I think we would want to make it a JSON base.
"
784	350	brendandburns	2014-07-05 23:40:01	CONTRIBUTOR	"Fwiw, this style of bug can bite you in C++ too (in fact it has bitten me)
when you do async RPCs inside a loop using a loop local request object.

I agree go makes it more likely due to its idioms.

Brendan
On Jul 4, 2014 8:19 PM, ""Tim Hockin"" notifications@github.com wrote:

> LGTM. That is a really nefarious bug. Why the language allows it is
> baffling to me (and according to google, many others, too)
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/350#issuecomment-48077407
> .
"
785	361	verdverm	2014-07-06 23:11:35	NONE	"#267 Allow minions to be securely registered/deregistered
"
786	360	thockin	2014-07-07 03:25:11	MEMBER	"Caveat: I am not super familiar with the selector parsing, but I was interested, so I looked.

As a general rule, I have found that little embedded DSLs like this are almost never worth the time to spec and parse properly, specially when simply open-coded like this.  I find the proposed syntax of << >> and $ to be sort of non-obvious (what is wrong with parentheses and commas?).  It's also not obvious from this reading whether a label value can have embedded metacharacters (<, >, space, quotes, etc) and what the escaping rules would be.

If this has to be a grammar, it should be dead obvious.  But does it have to be?  Can it be a structured message instead?
"
787	345	thockin	2014-07-07 03:39:35	MEMBER	"Yeah, we have some confusion between what is a pod and what is a manifest.
 From Kubelet's point of view a manifest IS a pod, and it is a resource.
 From apiserver's point of view it's just a piece of a larger resource.

Brendan and I were discussing separating the objects based on point-of-view
(API Server vs Kubelet) but I have not done so yet.  If
ContainerManifest.ID is deprecated, what do I use to identify a pod on a
kubelet?

Got time to do a hangout this week maybe?

On Sat, Jul 5, 2014 at 10:35 AM, Clayton Coleman notifications@github.com
wrote:

> This implies
> https://github.com/GoogleCloudPlatform/kubernetes/pull/345#discussion_r14491699
> to me that ContainerManifest should not have an ID - it was a temporary
> issue that the presence of a pod uuid resolves, so I marked it as
> deprecated in the latest commit. Now, if ContainerManifest becomes an API
> resource as commented in another issue, I think we would want to make it a
> JSON base.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/345#issuecomment-48091958
> .
"
788	345	smarterclayton	2014-07-07 03:53:50	CONTRIBUTOR	"Sure - I had started to take a stab at what it would take to abstract the kubelet from a direct etcd dependency in #356 and noted the same thing.  Would be good to go over the options in a hangout.  
"
789	345	thockin	2014-07-07 04:05:59	MEMBER	"You're east-coast time, right?

Tuesday afternoon (your time, after 11:00a California time) is open for
me...

For anyone else, this is not a private meeting, but we would like to focus
on progress as much as possible :)

On Sun, Jul 6, 2014 at 8:53 PM, Clayton Coleman notifications@github.com
wrote:

> Sure - I had started to take a stab at what it would take to abstract the
> kubelet from a direct etcd dependency in #356
> https://github.com/GoogleCloudPlatform/kubernetes/pull/356 and noted
> the same thing. Would be good to go over the options in a hangout.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/345#issuecomment-48138980
> .
"
790	345	verdverm	2014-07-07 05:55:05	NONE	"I'd be down to join a hangout, though I'm not sure how much I have to contribute at this point
"
791	351	discordianfish	2014-07-07 10:26:06	CONTRIBUTOR	"@thockin See ""Create an image"" here: http://docs.docker.com/reference/api/docker_remote_api_v1.12/#22-images
But the API docs are a bit misleading in that one, so I've created https://github.com/dotcloud/docker/issues/6837

And to me it also looks like the server should parse that, given it parses all other parameters from the name as well. But maybe there are reasons for not doing it. I'll discuss that, but if this gets changed kubernetes would require at least some future version of Docker. Would that be okay? Or should this be rather backward compatible? In that case we still need to parse the tag to support older docker daemons.
"
792	360	meirf	2014-07-07 11:57:00	CONTRIBUTOR	"@thockin, thank you for your feedback 

The original parsing used simple `split`ting so I tried to keep it. This forced a departure from using commas as a value separator in the set since the comma was already used a selector separator. Parsing might instead be done using regular expressions with capturing parentheses, but I agree that a different approach should be considered than this one-off encoding/parsing if set functionality is added.

What type of structured message do you have in mind?
"
793	345	smarterclayton	2014-07-07 13:29:10	CONTRIBUTOR	"Let's try 2pm EDT on Tuesday
"
794	359	smarterclayton	2014-07-07 13:31:16	CONTRIBUTOR	"Fixed
"
795	351	discordianfish	2014-07-07 15:18:53	CONTRIBUTOR	"@thockin I've opened https://github.com/dotcloud/docker/issues/6876 to track adding parsing of the tag to the daemon. But since it probably make sense to support already existing Docker versions, I've added the parsing to kubernetes for :). See updated code.
"
796	363	brendandburns	2014-07-07 16:59:05	CONTRIBUTOR	"Hey Brian,
Any chance you have cycles to fill this in somewhat?  If not I can definitely take a crack at it.

Thanks
--brendan
"
797	362	brendandburns	2014-07-07 17:01:03	CONTRIBUTOR	"Thanks for the report.  This looks like its in the Google Cloud SDK, I'll forward the report on to them and see if they know about anything.

Will report back when I know more.

Thanks again!
--brendan
"
798	346	brendandburns	2014-07-07 17:02:37	CONTRIBUTOR	"Friendly monday morning ping ;) @thockin you're my only hope ;)
"
799	346	thockin	2014-07-07 17:06:58	MEMBER	"LGTM, modulo one question on Manifest.ID
"
800	346	brendandburns	2014-07-07 17:10:46	CONTRIBUTOR	"Comment addressed, ptal.

--brendan
"
801	364	brendandburns	2014-07-07 17:14:16	CONTRIBUTOR	"Ugh, ok, I bit the bullet and moved integration test back to being sync: true.  I don't like this, but I'd rather get to green and then work from there.
"
802	351	thockin	2014-07-07 17:30:30	MEMBER	"I find no ""Creating"" anywhere on the page at
http://docs.docker.com/reference/api/docker_remote_api_v1.12/#22-images

?

On Mon, Jul 7, 2014 at 3:26 AM, discordianfish notifications@github.com
wrote:

> @thockin https://github.com/thockin See ""Create an image"" here:
> http://docs.docker.com/reference/api/docker_remote_api_v1.12/#22-images
> But the API docs are a bit misleading in that one, so I've created
> dotcloud/docker#6837 https://github.com/dotcloud/docker/issues/6837
> 
> And to me it also looks like the server should parse that, given it parses
> all other parameters from the name as well. But maybe there are reasons for
> not doing it. I'll discuss that, but if this gets changed kubernetes would
> require at least some future version of Docker. Would that be okay? Or
> should this be rather backward compatible? In that case we still need to
> parse the tag to support older docker daemons.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/351#issuecomment-48162607
> .
"
803	351	discordianfish	2014-07-07 17:39:57	CONTRIBUTOR	"@thockin Not ""Creating"", but ""Create"" :) But yes it's rather hard to navigate the docs..
"
804	310	monnand	2014-07-07 17:46:41	CONTRIBUTOR	"@thockin Merging #348 would have fixed all detect-able races. But I'm not sure if any PR after #348 introduced new races. The currently master breaks the CI because of some time out error. I do not have if it is related to the races you mentioned.
"
805	310	thockin	2014-07-07 17:51:56	MEMBER	"When I run tests on current tip, I get errors

$ ./hack/test-go.sh
ok   github.com/GoogleCloudPlatform/kubernetes/pkg/api 1.030s coverage:
81.0% of statements
I0707 10:51:01.132286 31033 logger.go:84] GET /prefix/version/simple:

# (462.796us) 200

WARNING: DATA RACE
Write by goroutine 14:
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.func·004()
http://github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.func%C2%B7004()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/operation.go:35
+0x45
  github.com/GoogleCloudPlatform/kubernetes/pkg/util.func·001()
http://github.com/GoogleCloudPlatform/kubernetes/pkg/util.func%C2%B7001()
      /home/thockin/src/kubernetes/output/go/src/
github.com/GoogleCloudPlatform/kubernetes/pkg/util/util.go:56 +0x5d
  github.com/GoogleCloudPlatform/kubernetes/pkg/util.Forever()
      /home/thockin/src/kubernetes/output/go/src/
github.com/GoogleCloudPlatform/kubernetes/pkg/util/util.go:57 +0x79

Previous write by goroutine 7:
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.func·004()
http://github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.func%C2%B7004()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/operation.go:35
+0x45
  github.com/GoogleCloudPlatform/kubernetes/pkg/util.func·001()
http://github.com/GoogleCloudPlatform/kubernetes/pkg/util.func%C2%B7001()
      /home/thockin/src/kubernetes/output/go/src/
github.com/GoogleCloudPlatform/kubernetes/pkg/util/util.go:56 +0x5d
  github.com/GoogleCloudPlatform/kubernetes/pkg/util.Forever()
      /home/thockin/src/kubernetes/output/go/src/
github.com/GoogleCloudPlatform/kubernetes/pkg/util/util.go:57 +0x79

Goroutine 14 (running) created at:
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.NewOperations()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/operation.go:35
+0x15f
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.New()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:75
+0x4b
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.TestErrorList()
      /home/thockin/src/kubernetes/output/go/src/
github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/apiserver_test.go:155
+0x16c
  testing.tRunner()
      /usr/local/go/src/pkg/testing/testing.go:391 +0x10f

Goroutine 7 (running) created at:
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.NewOperations()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/operation.go:35
+0x15f
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.New()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:75
+0x4b
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.TestSimpleList()
      /home/thockin/src/kubernetes/output/go/src/
github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/apiserver_test.go:138
+0x127
  testing.tRunner()

#       /usr/local/go/src/pkg/testing/testing.go:391 +0x10f

# 

WARNING: DATA RACE
Write by goroutine 14:

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*Operations).expire()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/operation.go:89
+0x47
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.func·004()
http://github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.func%C2%B7004()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/operation.go:35
+0x7d
  github.com/GoogleCloudPlatform/kubernetes/pkg/util.func·001()
http://github.com/GoogleCloudPlatform/kubernetes/pkg/util.func%C2%B7001()
      /home/thockin/src/kubernetes/output/go/src/
github.com/GoogleCloudPlatform/kubernetes/pkg/util/util.go:56 +0x5d
  github.com/GoogleCloudPlatform/kubernetes/pkg/util.Forever()
      /home/thockin/src/kubernetes/output/go/src/
github.com/GoogleCloudPlatform/kubernetes/pkg/util/util.go:57 +0x79

Previous write by goroutine 7:

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*Operations).expire()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/operation.go:89
+0x47
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.func·004()
http://github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.func%C2%B7004()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/operation.go:35
+0x7d
  github.com/GoogleCloudPlatform/kubernetes/pkg/util.func·001()
http://github.com/GoogleCloudPlatform/kubernetes/pkg/util.func%C2%B7001()
      /home/thockin/src/kubernetes/output/go/src/
github.com/GoogleCloudPlatform/kubernetes/pkg/util/util.go:56 +0x5d
  github.com/GoogleCloudPlatform/kubernetes/pkg/util.Forever()
      /home/thockin/src/kubernetes/output/go/src/
github.com/GoogleCloudPlatform/kubernetes/pkg/util/util.go:57 +0x79

Goroutine 14 (running) created at:
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.NewOperations()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/operation.go:35
+0x15f
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.New()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:75
+0x4b
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.TestErrorList()
      /home/thockin/src/kubernetes/output/go/src/
github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/apiserver_test.go:155
+0x16c
  testing.tRunner()
      /usr/local/go/src/pkg/testing/testing.go:391 +0x10f

Goroutine 7 (running) created at:
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.NewOperations()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/operation.go:35
+0x15f
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.New()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:75
+0x4b
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.TestSimpleList()
      /home/thockin/src/kubernetes/output/go/src/
github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/apiserver_test.go:138
+0x127
  testing.tRunner()

#       /usr/local/go/src/pkg/testing/testing.go:391 +0x10f

# 

WARNING: DATA RACE
Write by goroutine 14:

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*Operations).expire()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/operation.go:101
+0x277
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.func·004()
http://github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.func%C2%B7004()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/operation.go:35
+0x7d
  github.com/GoogleCloudPlatform/kubernetes/pkg/util.func·001()
http://github.com/GoogleCloudPlatform/kubernetes/pkg/util.func%C2%B7001()
      /home/thockin/src/kubernetes/output/go/src/
github.com/GoogleCloudPlatform/kubernetes/pkg/util/util.go:56 +0x5d
  github.com/GoogleCloudPlatform/kubernetes/pkg/util.Forever()
      /home/thockin/src/kubernetes/output/go/src/
github.com/GoogleCloudPlatform/kubernetes/pkg/util/util.go:57 +0x79

Previous write by goroutine 7:

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*Operations).expire()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/operation.go:101
+0x277
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.func·004()
http://github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.func%C2%B7004()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/operation.go:35
+0x7d
  github.com/GoogleCloudPlatform/kubernetes/pkg/util.func·001()
http://github.com/GoogleCloudPlatform/kubernetes/pkg/util.func%C2%B7001()
      /home/thockin/src/kubernetes/output/go/src/
github.com/GoogleCloudPlatform/kubernetes/pkg/util/util.go:56 +0x5d
  github.com/GoogleCloudPlatform/kubernetes/pkg/util.Forever()
      /home/thockin/src/kubernetes/output/go/src/
github.com/GoogleCloudPlatform/kubernetes/pkg/util/util.go:57 +0x79

Goroutine 14 (running) created at:
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.NewOperations()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/operation.go:35
+0x15f
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.New()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:75
+0x4b
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.TestErrorList()
      /home/thockin/src/kubernetes/output/go/src/
github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/apiserver_test.go:155
+0x16c
  testing.tRunner()
      /usr/local/go/src/pkg/testing/testing.go:391 +0x10f

Goroutine 7 (running) created at:
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.NewOperations()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/operation.go:35
+0x15f
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.New()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:75
+0x4b
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.TestSimpleList()
      /home/thockin/src/kubernetes/output/go/src/
github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/apiserver_test.go:138
+0x127
  testing.tRunner()

#       /usr/local/go/src/pkg/testing/testing.go:391 +0x10f

I0707 10:51:01.135193 31033 logger.go:84] GET /prefix/version/simple:
(147.07us) 500
goroutine 17 [running]:
github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(_respLogger).WriteHeader(0xc210051e40,
0x1f4)
 github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/logger.go:104
+0x10b
github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(_ApiServer).error(0xc21007fa50,
0x7fc077b06e80, 0xc21001edc0, 0x7fc077b08b00, 0xc210051e40)

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:178
+0x73
github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*ApiServer).handleREST(0xc21007fa50,
0xc21004c0d0, 0x1, 0x1, 0xc2100474d0, ...)

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:259
+0x6b8
github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*ApiServer).ServeHTTP(0xc21007fa50,
0x7fc077b08ac8, 0xc21000fd20, 0xc2100910d0)

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:153
+0xdbd
net/http/httptest.(_waitGroupHandler).ServeHTTP(0xc21004ce40,
0x7fc077b08ac8, 0xc21000fd20, 0xc2100910d0)
/usr/local/go/src/pkg/net/http/httptest/server.go:200 +0xe0
net/http.serverHandler.ServeHTTP(0xc21001fdc0, 0x7fc077b08ac8,
0xc21000fd20, 0xc2100910d0)
/usr/local/go/src/pkg/net/http/server.go:1597 +0x1cb
net/http.(_conn).serve(0xc210059600)
 /usr/local/go/src/pkg/net/http/server.go:1167 +0xc01
created by net/http.(*Server).Serve
/usr/local/go/src/pkg/net/http/server.go:1644 +0x2c1
I0707 10:51:01.137497 31033 logger.go:84] GET /prefix/version/simple:
(190.068us) 200
I0707 10:51:01.140352 31033 logger.go:84] GET /prefix/version/simple/id:
(139.615us) 200
I0707 10:51:01.143403 31033 logger.go:84] DELETE /prefix/version/simple/id:
(744.879us) 202
I0707 10:51:01.146837 31033 logger.go:84] PUT /prefix/version/simple/id:

# (901.881us) 202

WARNING: DATA RACE
Write by goroutine 51:
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.func·001()
http://github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.func%C2%B7001()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:34
+0x4e

Previous write by goroutine 41:
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.func·001()
http://github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.func%C2%B7001()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:34
+0x4e

Goroutine 51 (running) created at:
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.MakeAsync()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:58
+0x113

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*SimpleRESTStorage).Update()
      /home/thockin/src/kubernetes/output/go/src/
github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/apiserver_test.go:121
+0x1f9

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*ApiServer).handleREST()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:351
+0x1249

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*ApiServer).ServeHTTP()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:153
+0xdbc
  net/http/httptest.(_waitGroupHandler).ServeHTTP()
      /usr/local/go/src/pkg/net/http/httptest/server.go:200 +0xdf
  net/http.serverHandler.ServeHTTP()
      /usr/local/go/src/pkg/net/http/server.go:1597 +0x1ca
  net/http.(_conn).serve()
      /usr/local/go/src/pkg/net/http/server.go:1167 +0xc00

Goroutine 41 (finished) created at:
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.MakeAsync()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:58
+0x113

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*SimpleRESTStorage).Delete()
      /home/thockin/src/kubernetes/output/go/src/
github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/apiserver_test.go:89
+0x1a0

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*ApiServer).handleREST()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:322
+0x31d

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*ApiServer).ServeHTTP()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:153
+0xdbc
  net/http/httptest.(_waitGroupHandler).ServeHTTP()
      /usr/local/go/src/pkg/net/http/httptest/server.go:200 +0xdf
  net/http.serverHandler.ServeHTTP()
      /usr/local/go/src/pkg/net/http/server.go:1597 +0x1ca
  net/http.(_conn).serve()

#       /usr/local/go/src/pkg/net/http/server.go:1167 +0xc00

# 

WARNING: DATA RACE
Write by goroutine 51:
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.func·001()
http://github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.func%C2%B7001()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:52
+0x2e3

Previous write by goroutine 41:
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.func·001()
http://github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.func%C2%B7001()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:52
+0x2e3

Goroutine 51 (running) created at:
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.MakeAsync()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:58
+0x113

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*SimpleRESTStorage).Update()
      /home/thockin/src/kubernetes/output/go/src/
github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/apiserver_test.go:121
+0x1f9

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*ApiServer).handleREST()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:351
+0x1249

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*ApiServer).ServeHTTP()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:153
+0xdbc
  net/http/httptest.(_waitGroupHandler).ServeHTTP()
      /usr/local/go/src/pkg/net/http/httptest/server.go:200 +0xdf
  net/http.serverHandler.ServeHTTP()
      /usr/local/go/src/pkg/net/http/server.go:1597 +0x1ca
  net/http.(_conn).serve()
      /usr/local/go/src/pkg/net/http/server.go:1167 +0xc00

Goroutine 41 (finished) created at:
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.MakeAsync()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:58
+0x113

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*SimpleRESTStorage).Delete()
      /home/thockin/src/kubernetes/output/go/src/
github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/apiserver_test.go:89
+0x1a0

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*ApiServer).handleREST()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:322
+0x31d

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*ApiServer).ServeHTTP()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:153
+0xdbc
  net/http/httptest.(_waitGroupHandler).ServeHTTP()
      /usr/local/go/src/pkg/net/http/httptest/server.go:200 +0xdf
  net/http.serverHandler.ServeHTTP()
      /usr/local/go/src/pkg/net/http/server.go:1597 +0x1ca
  net/http.(_conn).serve()

#       /usr/local/go/src/pkg/net/http/server.go:1167 +0xc00

# 

WARNING: DATA RACE
Write by goroutine 52:

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*Operation).wait()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/operation.go:106
+0x3c

Previous write by goroutine 42:

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*Operation).wait()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/operation.go:106
+0x3c

Goroutine 52 (running) created at:

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*Operations).NewOperation()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/operation.go:48
+0x15e

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*ApiServer).finishReq()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:190
+0x8f

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*ApiServer).handleREST()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:358
+0x134c

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*ApiServer).ServeHTTP()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:153
+0xdbc
  net/http/httptest.(_waitGroupHandler).ServeHTTP()
      /usr/local/go/src/pkg/net/http/httptest/server.go:200 +0xdf
  net/http.serverHandler.ServeHTTP()
      /usr/local/go/src/pkg/net/http/server.go:1597 +0x1ca
  net/http.(_conn).serve()
      /usr/local/go/src/pkg/net/http/server.go:1167 +0xc00

Goroutine 42 (finished) created at:

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*Operations).NewOperation()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/operation.go:48
+0x15e

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*ApiServer).finishReq()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:190
+0x8f

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*ApiServer).handleREST()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:329
+0x420

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*ApiServer).ServeHTTP()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:153
+0xdbc
  net/http/httptest.(_waitGroupHandler).ServeHTTP()
      /usr/local/go/src/pkg/net/http/httptest/server.go:200 +0xdf
  net/http.serverHandler.ServeHTTP()
      /usr/local/go/src/pkg/net/http/server.go:1597 +0x1ca
  net/http.(_conn).serve()

#       /usr/local/go/src/pkg/net/http/server.go:1167 +0xc00

# 

WARNING: DATA RACE
Write by goroutine 53:

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*Operations).insert()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/operation.go:54
+0x3c

Previous write by goroutine 43:

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*Operations).insert()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/operation.go:54
+0x3c

Goroutine 53 (running) created at:

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*Operations).NewOperation()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/operation.go:49
+0x17f

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*ApiServer).finishReq()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:190
+0x8f

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*ApiServer).handleREST()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:358
+0x134c

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*ApiServer).ServeHTTP()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:153
+0xdbc
  net/http/httptest.(_waitGroupHandler).ServeHTTP()
      /usr/local/go/src/pkg/net/http/httptest/server.go:200 +0xdf
  net/http.serverHandler.ServeHTTP()
      /usr/local/go/src/pkg/net/http/server.go:1597 +0x1ca
  net/http.(_conn).serve()
      /usr/local/go/src/pkg/net/http/server.go:1167 +0xc00

Goroutine 43 (finished) created at:

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*Operations).NewOperation()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/operation.go:49
+0x17f

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*ApiServer).finishReq()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:190
+0x8f

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*ApiServer).handleREST()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:329
+0x420

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*ApiServer).ServeHTTP()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:153
+0xdbc
  net/http/httptest.(_waitGroupHandler).ServeHTTP()
      /usr/local/go/src/pkg/net/http/httptest/server.go:200 +0xdf
  net/http.serverHandler.ServeHTTP()
      /usr/local/go/src/pkg/net/http/server.go:1597 +0x1ca
  net/http.(_conn).serve()

#       /usr/local/go/src/pkg/net/http/server.go:1167 +0xc00

# 

WARNING: DATA RACE
Write by goroutine 51:
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.func·001()
http://github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.func%C2%B7001()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:55
+0x286

Previous write by goroutine 41:
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.func·001()
http://github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.func%C2%B7001()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:55
+0x286

Goroutine 51 (running) created at:
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.MakeAsync()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:58
+0x113

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*SimpleRESTStorage).Update()
      /home/thockin/src/kubernetes/output/go/src/
github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/apiserver_test.go:121
+0x1f9

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*ApiServer).handleREST()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:351
+0x1249

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*ApiServer).ServeHTTP()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:153
+0xdbc
  net/http/httptest.(_waitGroupHandler).ServeHTTP()
      /usr/local/go/src/pkg/net/http/httptest/server.go:200 +0xdf
  net/http.serverHandler.ServeHTTP()
      /usr/local/go/src/pkg/net/http/server.go:1597 +0x1ca
  net/http.(_conn).serve()
      /usr/local/go/src/pkg/net/http/server.go:1167 +0xc00

Goroutine 41 (finished) created at:
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.MakeAsync()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:58
+0x113

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*SimpleRESTStorage).Delete()
      /home/thockin/src/kubernetes/output/go/src/
github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/apiserver_test.go:89
+0x1a0

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*ApiServer).handleREST()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:322
+0x31d

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*ApiServer).ServeHTTP()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:153
+0xdbc
  net/http/httptest.(_waitGroupHandler).ServeHTTP()
      /usr/local/go/src/pkg/net/http/httptest/server.go:200 +0xdf
  net/http.serverHandler.ServeHTTP()
      /usr/local/go/src/pkg/net/http/server.go:1597 +0x1ca
  net/http.(_conn).serve()

#       /usr/local/go/src/pkg/net/http/server.go:1167 +0xc00

I0707 10:51:01.149748 31033 logger.go:84] GET /foobar: (341.377us) 404
goroutine 58 [running]:
github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(_respLogger).WriteHeader(0xc2100ba900,
0x194)
 github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/logger.go:104
+0x10b
github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(_ApiServer).notFound(0xc2100bced0,
0xc2100398f0, 0x7fc077b08b00, 0xc2100ba900)

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:159
+0x70
github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*ApiServer).ServeHTTP(0xc2100bced0,
0x7fc077b08ac8, 0xc21000f500, 0xc2100398f0)

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:128
+0x77f
net/http/httptest.(_waitGroupHandler).ServeHTTP(0xc2100c3800,
0x7fc077b08ac8, 0xc21000f500, 0xc2100398f0)
/usr/local/go/src/pkg/net/http/httptest/server.go:200 +0xe0
net/http.serverHandler.ServeHTTP(0xc2100a9550, 0x7fc077b08ac8,
0xc21000f500, 0xc2100398f0)
/usr/local/go/src/pkg/net/http/server.go:1597 +0x1cb
net/http.(_conn).serve(0xc210059b80)
 /usr/local/go/src/pkg/net/http/server.go:1167 +0xc01
created by net/http.(_Server).Serve
/usr/local/go/src/pkg/net/http/server.go:1644 +0x2c1
I0707 10:51:01.152165 31033 logger.go:84] GET /prefix/version: (329.537us)
404
goroutine 65 [running]:
github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(_respLogger).WriteHeader(0xc21009f5a0,
0x194)
 github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/logger.go:104
+0x10b
github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*ApiServer).notFound(0xc21009b1b0,
0xc2100d4270, 0x7fc077b08b00, 0xc21009f5a0)

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:159
+0x70
github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*ApiServer).ServeHTTP(0xc21009b1b0,
0x7fc077b08ac8, 0xc21000f640, 0xc2100d4270)

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:135
+0x8fd
net/http/httptest.(_waitGroupHandler).ServeHTTP(0xc2100c3d60,
0x7fc077b08ac8, 0xc21000f640, 0xc2100d4270)
/usr/local/go/src/pkg/net/http/httptest/server.go:200 +0xe0
net/http.serverHandler.ServeHTTP(0xc2100a95f0, 0x7fc077b08ac8,
0xc21000f640, 0xc2100d4270)
/usr/local/go/src/pkg/net/http/server.go:1597 +0x1cb
net/http.(_conn).serve(0xc210059c80)
 /usr/local/go/src/pkg/net/http/server.go:1167 +0xc01
created by net/http.(_Server).Serve
/usr/local/go/src/pkg/net/http/server.go:1644 +0x2c1
I0707 10:51:01.154562 31033 logger.go:84] GET /prefix/version/foobar:
(330.374us) 404
goroutine 72 [running]:
github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(_respLogger).WriteHeader(0xc21009f900,
0x194)
 github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/logger.go:104
+0x10b
github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*ApiServer).notFound(0xc21009bf00,
0xc2100d4680, 0x7fc077b08b00, 0xc21009f900)

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:159
+0x70
github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*ApiServer).ServeHTTP(0xc21009bf00,
0x7fc077b08ac8, 0xc21000f820, 0xc2100d4680)

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:149
+0xcf3
net/http/httptest.(_waitGroupHandler).ServeHTTP(0xc21009c320,
0x7fc077b08ac8, 0xc21000f820, 0xc2100d4680)
/usr/local/go/src/pkg/net/http/httptest/server.go:200 +0xe0
net/http.serverHandler.ServeHTTP(0xc2100a9690, 0x7fc077b08ac8,
0xc21000f820, 0xc2100d4680)
/usr/local/go/src/pkg/net/http/server.go:1597 +0x1cb
net/http.(_conn).serve(0xc210059d80)
 /usr/local/go/src/pkg/net/http/server.go:1167 +0xc01
created by net/http.(*Server).Serve
/usr/local/go/src/pkg/net/http/server.go:1644 +0x2c1

'foobar' has no storage object
I0707 10:51:01.157623 31033 logger.go:84] POST /prefix/version/foo:
(887.762us) 202
E0707 10:51:01.158720 31033 apiserver.go:233] Failed to parse:
&errors.errorString{s:""time: invalid duration not a timeout""} 'not a
timeout'
I0707 10:51:01.361860 31033 logger.go:84] POST
/prefix/version/foo?sync=true: (201.10636ms) 200
I0707 10:51:01.565104 31033 logger.go:84] POST

# /prefix/version/foo?sync=true&timeout=200ms: (200.864504ms) 202

WARNING: DATA RACE
Write by goroutine 113:

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*Operation).WaitFor()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/operation.go:119
+0x3c
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.func·013()
http://github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.func%C2%B7013()
      /home/thockin/src/kubernetes/output/go/src/
github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/operation_test.go:56
+0x61

Previous write by goroutine 108:

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*Operation).WaitFor()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/operation.go:119
+0x3c
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.TestOperation()
      /home/thockin/src/kubernetes/output/go/src/
github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/operation_test.go:61
+0x59e
  testing.tRunner()
      /usr/local/go/src/pkg/testing/testing.go:391 +0x10f

Goroutine 113 (running) created at:
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.TestOperation()
      /home/thockin/src/kubernetes/output/go/src/
github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/operation_test.go:58
+0x558
  testing.tRunner()
      /usr/local/go/src/pkg/testing/testing.go:391 +0x10f

Goroutine 108 (running) created at:
  testing.RunTests()
      /usr/local/go/src/pkg/testing/testing.go:471 +0xb3c
  testing.Main()
      /usr/local/go/src/pkg/testing/testing.go:403 +0xa2
  main.main()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/_testmain.go:125

# +0x19a

# PASS

WARNING: DATA RACE
Read by main goroutine:
  testing.coverReport()
      /usr/local/go/src/pkg/testing/cover.go:66 +0x43a
  testing.after()
      /usr/local/go/src/pkg/testing/testing.go:558 +0x8b6
  testing.Main()
      /usr/local/go/src/pkg/testing/testing.go:412 +0x1e9
  main.main()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/_testmain.go:125
+0x19a

Previous write by goroutine 122:

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*Operation).WaitFor()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/operation.go:124
+0x119
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.func·013()
http://github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.func%C2%B7013()
      /home/thockin/src/kubernetes/output/go/src/
github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/operation_test.go:56
+0x61

Goroutine 122 (finished) created at:
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.TestOperation()
      /home/thockin/src/kubernetes/output/go/src/
github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/operation_test.go:58
+0x558
  testing.tRunner()

#       /usr/local/go/src/pkg/testing/testing.go:391 +0x10f

coverage: 66.4% of statements
Found 10 data race(s)
FAIL github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver 2.055s

On Mon, Jul 7, 2014 at 10:46 AM, monnand notifications@github.com wrote:

> @thockin https://github.com/thockin Merging #348
> https://github.com/GoogleCloudPlatform/kubernetes/pull/348 would have
> fixed all detect-able races. But I'm not sure if any PR after #348
> https://github.com/GoogleCloudPlatform/kubernetes/pull/348 introduced
> new races. The currently master breaks the CI because of some time out
> error. I do not have if it is related to the races you mentioned.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/310#issuecomment-48212153
> .
"
806	310	monnand	2014-07-07 17:59:39	CONTRIBUTOR	"@thockin Aha, this is a quite familiar error. You are using go 1.2, right? There's a bug in go 1.2's race detector and I was bitten by it in #348. Use go 1.3 and it will be fixed.
"
807	360	thockin	2014-07-07 18:03:40	MEMBER	"Structured like:

type Selector struct {
    Requirements []Requirement
}

type Requirement struct {
    key string
   // comparator is something like an enum of exists, in, not in
    comparator Comparator
    // Only one of the following can be used at once.  Maybe use pointers?
 or some other union-like construct
    str_values []string
    int_values []int
    range_values []IntRange
}

So instead of saying

selector = ""x=<<foo$bar>>""

you would say

selector = Selector{
    Requirements: []Requirement{
        key: ""x""
        comparator: IN
        str_values: []string{""foo"", ""bar""}
    }
}

basically an AST, rather than a string to be parsed

On Mon, Jul 7, 2014 at 4:57 AM, meirf notifications@github.com wrote:

> @thockin https://github.com/thockin, thank you for your feedback
> 
> The original parsing used simple splitting so I tried to keep it. This
> forced a departure from using commas as a value separator in the set since
> the comma was already used a selector separator. Parsing might instead be
> done using regular expressions with capturing parentheses, but I agree that
> a different approach should be considered than this one-off
> encoding/parsing.
> 
> What type of structured message do you have in mind?
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/360#issuecomment-48169382
> .
"
808	310	thockin	2014-07-07 18:06:15	MEMBER	"Do we have an official statement of which versions of Go are supported and
not supported?  If 1.2 doesn't work, we need such a statement.  Better yet,
we need the tools to return an error, rather than try to run.

On Mon, Jul 7, 2014 at 10:59 AM, monnand notifications@github.com wrote:

> @thockin https://github.com/thockin Aha, this is a quite familiar
> error. You are using go 1.2, right? There's a bug in go 1.2's race detector
> and I was bitten by it in #348
> https://github.com/GoogleCloudPlatform/kubernetes/pull/348. Use go 1.3
> and it will be fixed.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/310#issuecomment-48214762
> .
"
809	345	thockin	2014-07-07 18:35:36	MEMBER	"I will post a hangout link on this thread.

Proposed topics:

API structure split: master vs slave
API versioning
IDs and Names

On Mon, Jul 7, 2014 at 6:29 AM, Clayton Coleman notifications@github.com
wrote:

> Let's try 2pm EDT on Tuesday
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/345#issuecomment-48177702
> .
"
810	361	brendandburns	2014-07-07 18:57:31	CONTRIBUTOR	"This is actually possible right now (on GCE at least), take a look at pkg/registry/minion_registry.go
"
811	158	rgarcia	2014-07-07 19:48:36	NONE	"It'd be great to have more directions for getting the example app running on a local cluster. I was able to get `hack/local-up-cluster.sh` working, but then failed to get the [guestbook example](https://github.com/GoogleCloudPlatform/kubernetes/blob/master/examples/guestbook/guestbook.md) up and running. `kubecfg.sh` failed due to `gcutil` missing, but even after installing that there were other gcutil-related errors.

Overall a frustrating kicking of the tires :-/
"
812	367	verdverm	2014-07-07 20:32:51	NONE	"I have noticed the FirstFit (default?) scheduler  co-locates pods when there are open machines available. Each of these machines has a single cpu.

It would be nice to use information about available cpu and a pod's expected cpu requirements

sed 's/cpu/other_machine_stat/'
"
813	367	monnand	2014-07-07 20:34:47	CONTRIBUTOR	"Currently, kubelet could get stats from cAdvisor which would be useful for scheduler. It could provide different percentiles of CPU and memory usage of a container (including the root container, i.e. the machine).
"
814	367	thockin	2014-07-07 20:35:04	MEMBER	"That's just ""scheduling"", as opposed to machine constraints, though very
coarsely they feel similar :)

On Mon, Jul 7, 2014 at 1:32 PM, Tony Worm notifications@github.com wrote:

> I have noticed the FirstFit (default?) scheduler co-locates pods when
> there are open machines available. Each of these machines has a single cpu.
> 
> It would be nice to use information about available cpu and a pod's
> expected cpu requirements
> 
> sed 's/cpu/other_machine_stat/'
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/367#issuecomment-48236654
> .
"
815	367	timothysc	2014-07-07 20:42:42	MEMBER	"Labels ""seems"" like the ideal place to enable a rank & requirements to define constraints.  However labels would need to be regularly published by minions. 

e.g.
rank = memory
requirements = gpu & clusterXYZ

I have a couple of concerns here: 
1. This treads into the full scale scheduling world. 
2. Config syntax = ?, DSL? ... 
"
816	367	thockin	2014-07-07 20:47:54	MEMBER	"Let's worry about semantics before syntax.  We have a similar issue open
for label selectors in general - we can discuss syntax there.

On Mon, Jul 7, 2014 at 1:42 PM, Timothy St. Clair notifications@github.com
wrote:

> Labels ""seems"" like the ideal place to enable a rank & requirements to
> define constraints. For example:
> 
> rank = memory
> requirements = gpu & clusterXYZ
> 
> I have a couple of concerns here:
> 1. This treads into the full scale scheduling world.
> 2. Config syntax = ?, DSL? ...
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/367#issuecomment-48237875
> .
"
817	367	timothysc	2014-07-07 20:56:12	MEMBER	"FWIW I often view constraints as a SQL query on a nvp store. 

SELECT Resources 
FROM Pool 
WHERE Requirements
ORDER BY Rank 
...

The hardest part are the 'fields' in an nvp store. 
"
818	366	brendandburns	2014-07-07 21:09:55	CONTRIBUTOR	"Ok, I think this is ready to merge.  I had to cut down the number of iterations to 5 to make it pass reliably (I hope..)

Let's get this in and get back to green.

Thanks!
--brendan
"
819	317	bgrant0607	2014-07-07 21:28:14	MEMBER	"See also #160 . @vmarmol 
"
820	317	vmarmol	2014-07-07 21:30:41	CONTRIBUTOR	"@dchen1107 is doing some work in this area as 
"
821	367	bgrant0607	2014-07-07 21:34:12	MEMBER	"Scheduling based on resources and constraints are 2 significantly different issues.

We have several issues open about resource (and QoS) awareness: #147 , #160 , #168 , #274 , #317.

Constraint syntax/semantics: We should start with the proposed label selector mechanism, #341 .
"
822	366	monnand	2014-07-07 22:32:49	CONTRIBUTOR	"Is it possible to run race detector for integration test as well? I think we could use either `go build -race` or `go run -race`.
"
823	160	monnand	2014-07-07 22:40:25	CONTRIBUTOR	"Currently, #328 and #174 could let kubelet retrieve some basic stats info from cAdvisor. On cAdvisor side, google/cadvisor#74 could put all stats into [influxdb](http://influxdb.com). 

If we want to display the data, we could write a separate UI program to retrieve stats from the backend storage (There is only [influxdb](http://influxdb.com) now) directly (reading the influxdb) or indirectly (retrieving from cAdvisor). Or, we could put this feature into the api server.
"
824	363	bgrant0607	2014-07-08 04:42:33	MEMBER	"Yes, I can take this one.
"
825	358	thockin	2014-07-08 04:50:11	MEMBER	"Ping - 3 commits for 3 logical changes.
"
826	368	brendandburns	2014-07-08 04:53:56	CONTRIBUTOR	"What is necessary to activate this, is it a kernel parameter?  Do we need to build in a different module?

Thanks
--brendan
"
827	369	brendandburns	2014-07-08 04:56:50	CONTRIBUTOR	"Thanks for the PR!

It actually had an unexpected effect of making me realize this section is no longer necessary and should be deleted, we now supply an htpasswd tool in ./third_party/

Any chance you could update this PR to delete that whole section.

And also, can you sign our CLA as described in CONTRIB.md

Thanks!
--brendan
"
828	358	brendandburns	2014-07-08 05:03:24	CONTRIBUTOR	"Small stuff.  Largely a preference for for-each style loops:

for _, obj := range foo {

vs

for ix := range foo {
  obj := foo[ix]
  ...
}
"
829	167	philips	2014-07-08 15:43:02	CONTRIBUTOR	"@jbeda FWIW, read replicas are part of the motivation for our work on an updated raft implementation. They are coming after we get that work in: https://github.com/coreos/etcd/pull/874
"
830	372	smarterclayton	2014-07-08 17:00:23	CONTRIBUTOR	"LGTM
"
831	167	discordianfish	2014-07-08 17:24:34	CONTRIBUTOR	"Do you guys have interest in anything else from this PR? I still would like to see a easy way to deploy kubernetes as docker images itself. Happy to help if you have concrete suggestions.
"
832	157	brendandburns	2014-07-08 17:42:27	CONTRIBUTOR	"@roberthbailey will handle this one.
"
833	345	thockin	2014-07-08 17:53:45	MEMBER	"https://plus.google.com/hangouts/_/grkrl7elarpergdg6ksepxjclea

I hope this works - never set up a public hangout before.

On Mon, Jul 7, 2014 at 11:35 AM, Tim Hockin thockin@google.com wrote:

> I will post a hangout link on this thread.
> 
> Proposed topics:
> 
> API structure split: master vs slave
> API versioning
> IDs and Names
> 
> On Mon, Jul 7, 2014 at 6:29 AM, Clayton Coleman notifications@github.com
> wrote:
> 
> > Let's try 2pm EDT on Tuesday
> > 
> > ## 
> > 
> > Reply to this email directly or view it on GitHub
> > https://github.com/GoogleCloudPlatform/kubernetes/pull/345#issuecomment-48177702
> > .
"
834	345	verdverm	2014-07-08 18:01:35	NONE	"Link says the party is over
"
835	345	thockin	2014-07-08 18:04:22	MEMBER	"try

https://plus.google.com/hangouts/_/g6eqprge2izsz3xmvubuajky2ua

On Tue, Jul 8, 2014 at 11:01 AM, Tony Worm notifications@github.com wrote:

> Link says the party is over
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/345#issuecomment-48376615
> .
"
836	341	timothysc	2014-07-08 18:16:54	MEMBER	"If folks intend to use labels for constraint matching, there will likely need to be comparison operators. 
"
837	147	timothysc	2014-07-08 18:33:50	MEMBER	"How does QoS tiers relate to resource requests?
"
838	168	timothysc	2014-07-08 19:47:30	MEMBER	"+1.  
"
839	274	timothysc	2014-07-08 19:50:46	MEMBER	"imho appending cAdvisor stats as a ""Kublet Label"" would provide uniform mechanics for viewing resources as well as constraint matching.  
"
840	160	timothysc	2014-07-08 19:54:27	MEMBER	"Why can't the kublet just regularly send a 'nvp thunk' including cAdvisor data to a store which the scheduler can then use well established variables for initial algorithm, then extra discrimination via constraint. 
"
841	372	brendandburns	2014-07-08 19:55:38	CONTRIBUTOR	"Thanks for the clean up.

Closes #35 
"
842	358	brendandburns	2014-07-08 19:58:24	CONTRIBUTOR	"Ok, I'm ok w/ leaving the iteration as is.  (are you certain the compiler is eliding the copy in this case, anyway?  I mean you're calling range ... which presumably has a multi-value return, even if you're dropping one of the values.

but LGTM, rebase and I'm ok to submit.
"
843	187	derekwaynecarr	2014-07-08 20:15:14	MEMBER	"I am going to try and take a look at this.
"
844	345	thockin	2014-07-08 20:15:37	MEMBER	"Notes from this discussion:
- API structure split: master vs slave

We all agreed in general that we should differentiate top-level API
structures between the API server and the Kubelet.  Common structures like
ContainerManifest will stay common and shared, but (for example) there
probably needs to be a different ""Pod"" structure for master and slave.

Google will find someone to work on this issue.
- API versioning

We all agreed in general that we need to start versioning the APIs and
having an explicit point in the processing logic where we convert from
versioned external structures to internal structures.  We will try to avoid
explicitly versioning internal structures for simplicity, but nobody was
deeply against it, if need be.  If we find ourselves with truly
incompatible behavior, we will need to introduce them carefully with
explicit notes of API breakage.

Nobody volunteered to work on this, so Google will find someone if possible
- IDs and names

We agreed on the following:
- ""Pod ID"" that exists for the lifetime of the pod in k8s
- ""Pod Instance ID"" that exists for the lifetime of a pod on a host
- ""Container attempt ID"" that is assigned to each run of a container
  (docker's ID)

We did not discuss, but previously agreed on:
- ""Pod namespace"" that represents the origin of the pod
- ""Pod name""
- ""Container name""

We agreed to not have the following:
- A generation number that changes on stop/start cycles.

I'll write this up more fully in PR #349.  Clayton is working on this.

Thanks everyone.  way faster than email.

Tim

On Mon, Jul 7, 2014 at 11:35 AM, Tim Hockin thockin@google.com wrote:

> I will post a hangout link on this thread.
> 
> Proposed topics:
> 
> API structure split: master vs slave
> API versioning
> IDs and Names
> 
> On Mon, Jul 7, 2014 at 6:29 AM, Clayton Coleman notifications@github.com
> wrote:
> 
> > Let's try 2pm EDT on Tuesday
> > 
> > ## 
> > 
> > Reply to this email directly or view it on GitHub
> > https://github.com/GoogleCloudPlatform/kubernetes/pull/345#issuecomment-48177702
> > .
"
845	365	brendandburns	2014-07-08 20:16:42	CONTRIBUTOR	"Comments addressed.  Please re-check.

Thanks!
--brendan
"
846	187	brendandburns	2014-07-08 20:29:33	CONTRIBUTOR	"Cool, thanks!

--brendan
"
847	349	thockin	2014-07-08 21:48:51	MEMBER	"@smarterclayton cleaned up after today's chat.  Still two FIXMEs about loosening requirements - I think I know which side you fall on, but I want to confirm and clarify.
"
848	358	thockin	2014-07-08 22:25:00	MEMBER	"I found a commit to the go compiler to elide the copy in the one-return case, but all discussions around eliding it entirely devolve into ""just iterate by index"" or someone proposing a language change which goes nowhere. :(  This seems like an opportunity for the language to be highly efficient, and it's not.

Rebased
"
849	349	thockin	2014-07-08 22:51:26	MEMBER	"re: name vs ID and autogen.  ACK.  Do you think the auto-generated name has to be deterministic?  Or is random sufficient?  I think random is good enough.  If you didn't care enough to specify the name to apiserver, you get something random.  Very much like docker itself.  This does not provide idempotency, but it's hard to say what idempotency means here.  If the user specified neither a name nor an ID, how do we divine their intentions?

Now, Kubelet gets value out of making it deterministic because we can assume that the same pod spec from the same file means the same thing, with or without a name or ID.

new commit added - please take a look.  If happy I can squash before merge.
"
850	349	smarterclayton	2014-07-08 22:53:25	CONTRIBUTOR	"LGTM
"
851	349	thockin	2014-07-08 23:39:31	MEMBER	"Will wait for 2nd LGTM from Brendan or someone else.

On Tue, Jul 8, 2014 at 3:53 PM, Clayton Coleman notifications@github.com
wrote:

> LGTM
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/349#issuecomment-48409706
> .
"
852	377	monnand	2014-07-08 23:44:29	CONTRIBUTOR	"LGTM. Anyone wants to review and probably merge it? cc/ @thockin @brendanburns 
"
853	377	vmarmol	2014-07-08 23:45:53	CONTRIBUTOR	"LGTM
"
854	341	bgrant0607	2014-07-09 00:21:36	MEMBER	"@timothysc Could you please provide a motivating example for comparisons? We could accept integer ranges, including open ranges. However, the restrictive selection semantics are careful chosen to facilitate set overlap detection, efficient indexing and reverse indexing, and human understandability.
"
855	341	timothysc	2014-07-09 00:45:29	MEMBER	"It is my understanding that Labels 'currently' only apply to pods.  If labeling were extended to Kublets then users could define things such as: 

rank = memory 
(implies sort, meaning < operator required) 

requirements = memory > 4GB 
(Only land my pod on machines with > 4GB) 
"
856	365	bgrant0607	2014-07-09 00:48:46	MEMBER	"Good enough for a first cut. Thanks.
"
857	147	bgrant0607	2014-07-09 00:54:53	MEMBER	"@timothysc 

At the node level, QoS would be specified together with resource requests to the execution layer, such as an exec driver over lmctfy (which might eventually be layered on libcontainer).

At the scheduling level, a variety of policies are possible. We could start with the naive approach of not considering QoS in scheduling. Eventually we could support some kind of overcommitment for lower QoS tiers.
"
858	341	bgrant0607	2014-07-09 01:27:44	MEMBER	"Ok, thanks. As I commented in #367 , labels / constraints shouldn't be used for resource-based placement or binpacking. The scheduler should place pods based on their minimum resource requirements and resources available.
"
859	341	timothysc	2014-07-09 01:36:40	MEMBER	"You could also view it as 

requirements = distance(podX) < 3  
"
860	341	bgrant0607	2014-07-09 02:03:39	MEMBER	"Labels are explicit and literal.

Locality requirements need to be expressed at a higher level. Expressing them as individual hard constraints on pods is likely to lead to a greedy scheduler to paint itself into a corner.

Scheduling is a nuanced service, with workload-specific and provider-specific objectives and constraints, topological and architectural considerations, security concerns, dependencies, QoS and fairness considerations, workload interference, etc. Let's not try to shoehorn too much into the label mechanism.
"
861	341	thockin	2014-07-09 02:07:52	MEMBER	"I really think scheduling resources and scheduling constraints are two
different things, both useful, but not the same.

For your other example, distance to another pod, I would also not suggest
labels.  Labels are relatively static and low in cardinality.  Asking for
distance to another pod (for some definition of distance) may be a legit
constrainy but doesn't sound like labels to me.

I also started with the expectation that label selectors would be an
arbitrary expression, including regex matching or prefix/suffix matching.
But I did not come up with a solid use case for more than simple sets.

Tim
On Jul 8, 2014 5:45 PM, ""Timothy St. Clair"" notifications@github.com
wrote:

> It is my understanding that Labels 'currently' only apply to pods. If
> labeling were extended to Kublets then users could define things such as:
> 
> rank = memory
> (implies sort, meaning < operator required)
> 
> requirements = memory > 4GB
> (Only land my pod on machines with > 4GB)
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/341#issuecomment-48417431
> .
"
862	341	timothysc	2014-07-09 02:13:45	MEMBER	"@thockin ok I'll buy the idea of not trying to overload labels.  
I'll try to hash it out on #367 then. 
"
863	341	bgrant0607	2014-07-09 02:14:59	MEMBER	"Regular expressions and prefix/suffix matching are hostile to the goals I mentioned earlier. We should not support them. The escape hatch is attaching a new label using whatever arbitrary computation you wish to decide where to attach it, or to dump the data into a database and use SQL or whatever.
"
864	341	timothysc	2014-07-09 02:21:53	MEMBER	"I proposed the SQL method on NVP sets earlier today.
"
865	341	thockin	2014-07-09 02:45:37	MEMBER	"I am behind on email, so I promise to digest the other threads tonight :)
On Jul 8, 2014 7:22 PM, ""Timothy St. Clair"" notifications@github.com
wrote:

> I proposed the SQL method on NVP sets earlier today.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/341#issuecomment-48422790
> .
"
866	367	timothysc	2014-07-09 02:45:44	MEMBER	"I'm ok with doing the selection from a set of offers/resources from the scheduler.  

Provided the offers have enough NVP information to enable discrimination.
"
867	367	thockin	2014-07-09 02:51:28	MEMBER	"I don't know about NVP - where can I read more on it?
On Jul 8, 2014 7:45 PM, ""Timothy St. Clair"" notifications@github.com
wrote:

> I'm ok with doing the selection from a set of offers/resources from the
> scheduler.
> 
> Provided the offers have enough NVP information to enable discrimination.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/367#issuecomment-48424027
> .
"
868	367	bgrant0607	2014-07-09 03:16:40	MEMBER	"Searching for ""NVP SQL"" or ""name value pair SQL"" or ""key value pair SQL"" comes up with lots of hits. Common arguments against are performance and loss of control over DB schema. But I'm getting the feeling that we're barking up the wrong forest.

@timothysc What are you trying to do? Right now, k8s has essentially no intelligent scheduling. However, that's not a desirable end state. If what you want is a scheduler, we should figure out how to support scheduling plugins and/or layers on top of k8s.
"
869	160	monnand	2014-07-09 03:27:51	CONTRIBUTOR	"@timothysc cAdvisor now could dump all stats into [influxdb](http://influxdb.com). Kubelet will retrieve a summary of such stats from cAdvisor and in turn pulled by master to do scheduling decisions. I think its similar to what you described?
"
870	367	thockin	2014-07-09 03:32:31	MEMBER	"Name Value Pairs?   Now I feel dumb :)

On Tue, Jul 8, 2014 at 7:51 PM, Tim Hockin thockin@google.com wrote:

> I don't know about NVP - where can I read more on it?
> On Jul 8, 2014 7:45 PM, ""Timothy St. Clair"" notifications@github.com
> wrote:
> 
> > I'm ok with doing the selection from a set of offers/resources from the
> > scheduler.
> > 
> > Provided the offers have enough NVP information to enable discrimination.
> > 
> > ## 
> > 
> > Reply to this email directly or view it on GitHub
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/367#issuecomment-48424027
> > .
"
871	367	bgrant0607	2014-07-09 04:08:57	MEMBER	"Something somewhat different than label selectors is per-attribute limits for spreading. Aurora is one system that supports this model:
https://aurora.incubator.apache.org/documentation/latest/configuration-reference/#specifying-scheduling-constraints

This is more relevant to physical rather than virtual deployments. I'd consider it a distinct mechanism from constraints. @timothysc If you'd like this, we should file a separate issue. However, I'd prefer a a new failure tolerance scheduling policy object that specifies a label selector to identify the set of instances to be spread. We could debate about how to describe what kind and/or how much spreading to apply, but I'd initially just leave it entirely up to the infrastructure. 
"
872	378	brendandburns	2014-07-09 04:45:53	CONTRIBUTOR	"I'm open to using this, although I don't necessarily see the value, but I'd like to understand exactly what the workflow does.

Can you add some documentation on how it is used (and how you would update dependencies) to the developer section of the README.md?

Thanks!
--brendan
"
873	375	brendandburns	2014-07-09 04:48:43	CONTRIBUTOR	"LGTM.
"
874	365	brendandburns	2014-07-09 04:51:47	CONTRIBUTOR	"Comments addressed.  ptal.

Thanks!
--brendan
"
875	35	brendandburns	2014-07-09 05:01:46	CONTRIBUTOR	"This is closed by #372 
"
876	365	bgrant0607	2014-07-09 05:16:20	MEMBER	"Thanks. I'm having some caching troubles with Github. Just try to ensure
that the API example field names match types.go (e.g. livenessProbe vs.
liveProbe). It would be nice to update the schema and regenerate the html,
also, but that could be a separate PR if you prefer.
"
877	365	brendandburns	2014-07-09 05:22:01	CONTRIBUTOR	"ok, fixed (again ;)
"
878	302	brendandburns	2014-07-09 05:23:48	CONTRIBUTOR	"The problem I saw was with watches hanging and not getting new data.  I suppose we could just timeout the watch, and then issue a new watch with the old last modified, and we'd likely get the same behavior.
"
879	349	thockin	2014-07-09 14:51:40	MEMBER	"rebased and squashed
"
880	371	thockin	2014-07-09 14:57:59	MEMBER	"rebased
"
881	367	timothysc	2014-07-09 14:58:12	MEMBER	"I completely agree its more relevant to physical rather then virtual deployments.

I was somewhat testing the possibility of enabling the capabilities for more general purpose scheduling, on par with a mini-Condor approach but it's not a requirement.  

Aurora or Marathon -esk capabilities will fill the gap. 
https://github.com/mesosphere/marathon/wiki/Constraints
"
882	379	thockin	2014-07-09 15:37:17	MEMBER	"This is sort of annoying, but can you fix the typo in the comment?   s/up/IP/ ?  LGTM otherwise.
"
883	66	bgrant0607	2014-07-09 15:58:44	MEMBER	"FWIW, here's a description of Marathon's liveness checks:
https://github.com/mesosphere/marathon/wiki/Health-Checks

HTTP responses between 200-399 are considered live. The max # of consecutive failures is configurable (as with GCE's LB readiness checks). 

Aurora's are similar:
http://aurora.incubator.apache.org/documentation/latest/configuration-tutorial/
"
884	381	thockin	2014-07-09 16:00:00	MEMBER	"Thanks for all the recent PRs and issues!  Have you signed Google's CLA, as
described in https://github.com/thockin/kubernetes/blob/master/CONTRIB.md ?
 I don't see your name on the list, but maybe I am missing it...

On Tue, Jul 8, 2014 at 11:05 PM, Yuki Yugui Sonoda <notifications@github.com

> wrote:
> 
> ---
> 
> You can merge this Pull Request by running
> 
>   git pull https://github.com/yugui/kubernetes feature/api-scope
> 
> Or view, comment on, or merge it at:
> 
>   https://github.com/GoogleCloudPlatform/kubernetes/pull/381
> Commit Summary
> - Allows adding custom api scopes to service accounts available in
> 
> File Changes
> - _M_ cluster/config-default.sh
>   https://github.com/GoogleCloudPlatform/kubernetes/pull/381/files#diff-0
>   (1)
> - _M_ cluster/config-test.sh
>   https://github.com/GoogleCloudPlatform/kubernetes/pull/381/files#diff-1
>   (2)
> - _M_ cluster/kube-up.sh
>   https://github.com/GoogleCloudPlatform/kubernetes/pull/381/files#diff-2
>   (2)
> 
> Patch Links:
> - https://github.com/GoogleCloudPlatform/kubernetes/pull/381.patch
> - https://github.com/GoogleCloudPlatform/kubernetes/pull/381.diff
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/381.
"
885	368	vmarmol	2014-07-09 16:37:02	CONTRIBUTOR	"Just to update the Grub config and reboot.
"
886	382	brendandburns	2014-07-09 16:57:44	CONTRIBUTOR	"Raphael,
Thanks for the PR!  Have you signed our CLA, its described in [CONTRIB.md](https://github.com/GoogleCloudPlatform/kubernetes/blob/master/CONTRIB.md), once that's signed, I can merge this in.

Thanks again.
--brendan
"
887	368	brendandburns	2014-07-09 17:12:37	CONTRIBUTOR	"Ok, we need to do this in the image build stage, so that it is built into the container VM image.

--brendan
"
888	368	brendandburns	2014-07-09 17:14:36	CONTRIBUTOR	"@dchen1107 , are you still looking to build an image?
"
889	368	jkaplowitz	2014-07-09 17:16:01	CONTRIBUTOR	"It's already there for the container VM image, either in the currently live
one or at least the build tree for future builds. @vmarmol clarified to me
that his request was about our vanilla GCE images, or at least some build
where customers would be following standard Kubernetes install instructions
rather than having Kubelet preinstalled.

Ok, we need to do this in the image build stage, so that it is built into
the container VM image.

--brendan

—
Reply to this email directly or view it on GitHub
https://github.com/GoogleCloudPlatform/kubernetes/issues/368#issuecomment-48504604
.
"
890	378	roberthbailey	2014-07-09 17:17:41	MEMBER	"Out of curiosity (not having fully grocked the update-deps shell script or your PR), have you considered using https://github.com/kr/goven instead of godep?
"
891	368	vmarmol	2014-07-09 17:21:00	CONTRIBUTOR	"I think this boils down to changing the default image from `backports-debian-7-wheezy` to the containers-vm image or something else we can have the cgroup enabled. I can try that and see how it goes (unless someone thinks its a terrible idea).
"
892	368	brendandburns	2014-07-09 17:21:29	CONTRIBUTOR	"SGTM, we were headed there anyway as part of the desire to run all of the Kubernetes binaries inside of containers.
"
893	368	jkaplowitz	2014-07-09 17:24:45	CONTRIBUTOR	"So this would be a default for our container-optimized docs/examples with
Kubelet itself moved into a container? If so, SGTM from me as well. If you
mean a GCE-wide default beyond container-centric contexts, that would need
more discussion outside of this GitHub issue.
On Jul 9, 2014 10:21 AM, ""brendandburns"" notifications@github.com wrote:

> SGTM, we were headed there anyway as part of the desire to run all of the
> Kubernetes binaries inside of containers.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/368#issuecomment-48505766
> .
"
894	368	vmarmol	2014-07-09 17:26:09	CONTRIBUTOR	"Just default for the Kubernetes scripts for now :) the examples are next and I think we stop there.
"
895	368	jkaplowitz	2014-07-09 17:26:48	CONTRIBUTOR	"SGTM.
On Jul 9, 2014 10:26 AM, ""Victor Marmol"" notifications@github.com wrote:

> Just default for the Kubernetes scripts for now :) the examples are next
> and I think we stop there.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/368#issuecomment-48506399
> .
"
896	378	monnand	2014-07-09 18:02:15	CONTRIBUTOR	"@brendandburns Sure. I can add a section in the README.md.
@roberthbailey I did considered goven, however, it will change import paths, which is not desirable.
"
897	379	brendandburns	2014-07-09 18:17:58	CONTRIBUTOR	"done.  ptal

thanks
--brendan
"
898	378	roberthbailey	2014-07-09 18:18:34	MEMBER	"@monnand thanks, just wanted to make sure it'd been considered, since it also seems designed to solve the dependency problem.
"
899	381	brendandburns	2014-07-09 18:19:09	CONTRIBUTOR	"Yuki is a Googler (in Tokyo?) (in the future, you can tell since she's in the Google Cloud github org)
"
900	378	monnand	2014-07-09 18:25:48	CONTRIBUTOR	"@roberthbailey Thank you!  I did a brief survey on dependency management in Go before starting this PR. It's a topic still under hot debate. [This list](https://code.google.com/p/go-wiki/wiki/PackageManagementTools) is a good starting point. I choose godep because it does exactly what we did using shell scripts and requires least change. 
"
901	383	brendandburns	2014-07-09 18:47:37	CONTRIBUTOR	"LGTM, this is good for a first fix.  @thockin ?
"
902	378	monnand	2014-07-09 18:52:43	CONTRIBUTOR	"@brendandburns Added htpasswd back to third_party and changed README.md. PTAL.
"
903	365	brendandburns	2014-07-09 19:19:29	CONTRIBUTOR	"ok, re-added the test (which somehow disappeared), validated things, ptal.

Thanks!
--brendan
"
904	383	thockin	2014-07-09 19:32:58	MEMBER	"Thanks!
"
905	381	thockin	2014-07-09 19:35:32	MEMBER	"OH!  My mistake.  Thanks!

On Wed, Jul 9, 2014 at 11:19 AM, brendandburns notifications@github.com
wrote:

> Merged #381 https://github.com/GoogleCloudPlatform/kubernetes/pull/381.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/381#event-139843797
> .
"
906	369	jdef	2014-07-09 20:30:12	CONTRIBUTOR	"Looks like README.md in the master branch is already cleaned up. Given that, there's probably no need for this PR
"
907	368	verdverm	2014-07-09 20:33:48	NONE	"Tried the  container-vm like this:

in config-default.sh

```
IMAGE=https://www.googleapis.com/compute/v1/projects/google-containers/global/images/container-vm-v20140624
```

VMs booted, so `docker ps`

```
tony@kube-minion-1:~$ sudo docker ps
2014/07/09 20:24:20 Error response from daemon: client and server don't have same version (client : 1.13, server: 1.12)
```

Unable to bring up a replicationController
"
908	368	jkaplowitz	2014-07-09 20:43:47	CONTRIBUTOR	"Hm. We released that with Docker 1.0.1. I hope there's not already an
incompatibility with 1.1.0 less than a month after 1.0.1 was released.

On Wed, Jul 9, 2014 at 1:33 PM, Tony Worm notifications@github.com wrote:

> Tried the container-vm like this:
> 
> in config-default.sh
> 
> IMAGE=https://www.googleapis.com/compute/v1/projects/google-containers/global/images/container-vm-v20140624
> 
> VMs booted, so docker ps
> 
> tony@kube-minion-1:~$ sudo docker ps
> 2014/07/09 20:24:20 Error response from daemon: client and server don't have same version (client : 1.13, server: 1.12)
> 
> Unable to bring up a replicationController
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/368#issuecomment-48530336
> .
"
909	368	vmarmol	2014-07-09 20:45:35	CONTRIBUTOR	"In general, Docker expects the CLI and daemon to be the same version which is the error here. Somehow those are different in the Kubernetes setup.
"
910	368	jkaplowitz	2014-07-09 20:47:58	CONTRIBUTOR	"There shouldn't be any Docker version skew within the image as shipped. If
the Kubernetes setup scripts in our GitHub tree involve reinstalling Docker
in possibly a different way, I could see potential issues to fix there.

On Wed, Jul 9, 2014 at 1:45 PM, Victor Marmol notifications@github.com
wrote:

> In general, Docker expects the CLI and daemon to be the same version which
> is the error here. Somehow those are different in the Kubernetes setup.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/368#issuecomment-48531616
> .
"
911	368	brendandburns	2014-07-09 20:48:34	CONTRIBUTOR	"Hrm, that seems highly unlikely.  The Kubernetes set up uses
http://get.docker.io so unless something's borked there I don't see how it
could happen.

(unless cAdvisor ships with a docker client?)

--brendan

On Wed, Jul 9, 2014 at 1:45 PM, Victor Marmol notifications@github.com
wrote:

> In general, Docker expects the CLI and daemon to be the same version which
> is the error here. Somehow those are different in the Kubernetes setup.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/368#issuecomment-48531616
> .
"
912	368	vmarmol	2014-07-09 20:50:00	CONTRIBUTOR	"I think the image brings up the Docker daemon (v 1.0.1) before Kubernetes installs the new Docker image (v 1.1.0). So the CLI invocations are of the new kind with the old client. Kubernetes should not need to install Docker if it is already installed.
"
913	368	brendandburns	2014-07-09 20:50:15	CONTRIBUTOR	"Ah, I missed that you had tried by switching to container vm.

Can you disable the docker install on the minion?

Go into cluster/saltbase/salt/top.sls and comment out 'docker' for the
'kubernetes-pool' section.  That will disable the docker install, since it
is already present in the container VM.

--brendan

On Wed, Jul 9, 2014 at 1:48 PM, jkaplowitz notifications@github.com wrote:

> There shouldn't be any Docker version skew within the image as shipped. If
> the Kubernetes setup scripts in our GitHub tree involve reinstalling
> Docker
> in possibly a different way, I could see potential issues to fix there.
> 
> On Wed, Jul 9, 2014 at 1:45 PM, Victor Marmol notifications@github.com
> wrote:
> 
> > In general, Docker expects the CLI and daemon to be the same version
> > which
> > is the error here. Somehow those are different in the Kubernetes setup.
> > 
> > —
> > Reply to this email directly or view it on GitHub
> > <
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/368#issuecomment-48531616>
> > 
> > .
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/368#issuecomment-48531881
> .
"
914	368	brendandburns	2014-07-09 20:50:35	CONTRIBUTOR	"You will need to ./cluster/kube-down.sh and re-create your cluster.

--brendan

On Wed, Jul 9, 2014 at 1:49 PM, Brendan Burns bburns@google.com wrote:

> Ah, I missed that you had tried by switching to container vm.
> 
> Can you disable the docker install on the minion?
> 
> Go into cluster/saltbase/salt/top.sls and comment out 'docker' for the
> 'kubernetes-pool' section.  That will disable the docker install, since it
> is already present in the container VM.
> 
> --brendan
> 
> On Wed, Jul 9, 2014 at 1:48 PM, jkaplowitz notifications@github.com
> wrote:
> 
> > There shouldn't be any Docker version skew within the image as shipped.
> > If
> > the Kubernetes setup scripts in our GitHub tree involve reinstalling
> > Docker
> > in possibly a different way, I could see potential issues to fix there.
> > 
> > On Wed, Jul 9, 2014 at 1:45 PM, Victor Marmol notifications@github.com
> > wrote:
> > 
> > > In general, Docker expects the CLI and daemon to be the same version
> > > which
> > > is the error here. Somehow those are different in the Kubernetes setup.
> > > 
> > > —
> > > Reply to this email directly or view it on GitHub
> > > <
> > > https://github.com/GoogleCloudPlatform/kubernetes/issues/368#issuecomment-48531616>
> > > 
> > > .
> > 
> > —
> > Reply to this email directly or view it on GitHub
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/368#issuecomment-48531881
> > .
"
915	368	jkaplowitz	2014-07-09 20:51:14	CONTRIBUTOR	"Yup, that seems to be the issue there.

On Wed, Jul 9, 2014 at 1:50 PM, brendandburns notifications@github.com
wrote:

> Ah, I missed that you had tried by switching to container vm.
> 
> Can you disable the docker install on the minion?
> 
> Go into cluster/saltbase/salt/top.sls and comment out 'docker' for the
> 'kubernetes-pool' section. That will disable the docker install, since it
> is already present in the container VM.
> 
> --brendan
> 
> On Wed, Jul 9, 2014 at 1:48 PM, jkaplowitz notifications@github.com
> wrote:
> 
> > There shouldn't be any Docker version skew within the image as shipped.
> > If
> > the Kubernetes setup scripts in our GitHub tree involve reinstalling
> > Docker
> > in possibly a different way, I could see potential issues to fix there.
> > 
> > On Wed, Jul 9, 2014 at 1:45 PM, Victor Marmol notifications@github.com
> > 
> > wrote:
> > 
> > > In general, Docker expects the CLI and daemon to be the same version
> > > which
> > > is the error here. Somehow those are different in the Kubernetes
> > > setup.
> > > 
> > > —
> > > Reply to this email directly or view it on GitHub
> > > <
> > 
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/368#issuecomment-48531616>
> > 
> > > .
> > 
> > —
> > Reply to this email directly or view it on GitHub
> > <
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/368#issuecomment-48531881>
> > 
> > .
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/368#issuecomment-48532156
> .
"
916	368	verdverm	2014-07-09 21:10:40	NONE	"ok, `docker ps -a` works, but there's nothing there, on any of the minions

I started a controller, which gets registered with k8s, 1 of 4 pods started in the output.
but still nothing there

```
Services
=====================
Name                Labels              Selector            Port
----------          ----------          ----------          ----------

Controllers
=====================
Name                Image(s)                     Selector            Replicas
----------          ----------                   ----------          ----------
algebra             23.251.148.42:5000/algebra   name=algebra        4

Pods
=====================
Name                Image(s)                     Host                                  Labels
----------          ----------                   ----------                            ----------
9acb0442            23.251.148.42:5000/algebra   kube-minion-3.c.cloud-pge.internal/   name=algebra,replicationController=algebra
```

I also noticed this at the end of the dev-build-and-up.sh

```
Security note: The server above uses a self signed certificate.  This is
    subject to ""Man in the middle"" type attacks.
fatal: Not a git repository (or any parent up to mount point /home)
Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).
```
"
917	368	dchen1107	2014-07-09 21:40:33	MEMBER	"I will build a new container vm image with docker 1.1. Should that mitigate the issue here? 
"
918	368	vmarmol	2014-07-09 21:42:28	CONTRIBUTOR	"@dchen1107 I think at this point it is just working through the issues of being on a different image. They should be similar enough to get it to work.

@verdverm not sure about the last error, but is there anything in the Kubelet log to say why the other 3 haven't come up? They make take some time depending on the size of the Docker image.
"
919	368	jkaplowitz	2014-07-09 21:43:01	CONTRIBUTOR	"Only until the next build comes out. Really the fix is to have kubernetes's
setup not install docker if it's already installed, and possibly to replace
any existing kubelet install that the container vm ships with as part of
the build you're installing.

On Wed, Jul 9, 2014 at 2:40 PM, Dawn Chen notifications@github.com wrote:

> I will build a new container vm image with docker 1.1. Should that
> mitigate the issue here?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/368#issuecomment-48538704
> .
"
920	365	bgrant0607	2014-07-09 21:59:01	MEMBER	"LGTM
"
921	365	smarterclayton	2014-07-09 22:03:01	CONTRIBUTOR	"LGTM
"
922	368	brendandburns	2014-07-09 22:08:04	CONTRIBUTOR	"The fact that they aren't in the pod list means that the master never got
the request to schedule them.

My guess is that your cluster only has a single machine, and your pod is
exposing an external port, so the master is refusing to schedule two pods
onto the same machine since the ports will conflict (we need to add a
'pending' state to handle these sorts of situations in the UX...)

--brendan

On Wed, Jul 9, 2014 at 2:10 PM, Tony Worm notifications@github.com wrote:

> ok, docker ps -a works, but there's nothing there, on any of the minions
> 
> I started a controller, which gets registered with k8s, 1 of 4 pods
> started in the output.
> but still nothing there
> 
> # Services
> 
> Name                Labels              Selector            Port
> 
> ---
> 
> # Controllers
> 
> Name                Image(s)                     Selector            Replicas
> 
> ---
> 
> algebra             23.251.148.42:5000/algebra   name=algebra        4
> 
> # Pods
> 
> Name                Image(s)                     Host                                  Labels
> 
> ---
> 
> 9acb0442            23.251.148.42:5000/algebra   kube-minion-3.c.cloud-pge.internal/   name=algebra,replicationController=algebra
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/368#issuecomment-48534621
> .
"
923	368	verdverm	2014-07-09 22:11:15	NONE	"I'm spinning up 4 minions, the same startup script works with the debian backport
After stopping k8s from installing docker the client-server error went away.
Now, after starting a k8s cluster, I ssh in and `docker ps -a` reports nothing.

I then run my usual startup script, which can talk to the k8s master,
but the minions don't appear to be set up correctly to handle their duties
"
924	260	bgrant0607	2014-07-09 22:11:19	MEMBER	"As part of this, I'd remove service objects from the core apiserver and facilitate the use of other load balancers, such as HAProxy and nginx.
"
925	170	bgrant0607	2014-07-09 22:13:05	MEMBER	"If we were to remove replicationController from the core apiserver into a separate service, I'd leave the pod template in the core.
"
926	368	vmarmol	2014-07-09 22:14:57	CONTRIBUTOR	"@verdverm is the Kubelet complaining of something? It may be that the environment is not setup correctly for them (we did want to move that to Docker containers eventually)
"
927	146	bgrant0607	2014-07-09 22:18:53	MEMBER	"We should also make it possible to plug in other naming/discovery mechanisms, such as etcd, Eureka, and Consul. One prerequisite is that it needs to be possible to get pod IP addresses #385.
"
928	260	smarterclayton	2014-07-09 22:32:28	CONTRIBUTOR	"It would be nice if the logical definition of a service (the query and/or global name) was able to be used/specialized in multiple ways - as a simple load balancer installed via the infrastructure, as a more feature complete load balancer like nginx or haproxy also offered by the infrastructure, as a queryable endpoint an integrator could poll/wait on (GET /services/foo -> { endpoints: [{host, port}, ...] }), or as information available to hosts to expose local load balancers.  Obviously these could be multiple different use cases and as such split into their own resources, but having some flexibility to specify intent (unify under a lb) distinct from mechanism makes it easier to satisfy a wide range of reqts.
"
929	368	verdverm	2014-07-09 22:35:02	NONE	"how would I find out if the Kubelet is complaining?

on master or minion?
"
930	368	brendandburns	2014-07-09 22:38:02	CONTRIBUTOR	"Can you try ""kubecfg.sh list minions""?

Brendan
 On Jul 9, 2014 3:35 PM, ""Tony Worm"" notifications@github.com wrote:

> how would I find out if the Kubelet is complaining?
> 
> on master or minion?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/368#issuecomment-48544079
> .
"
931	386	smarterclayton	2014-07-09 22:42:25	CONTRIBUTOR	"Would it be better to make this explicit in the manifest (VAR_A=$K8_POD_ID) rather than implicit?  I ask because in Openshift we found this led to lazy coupling, and in scenarios where you have code that wants a unique id, you want to be able to alias names instead.  At least that couples k8s to an image via a specific resource provided by k8s, vs making the image dependent on k8s.
"
932	260	bgrant0607	2014-07-09 22:54:57	MEMBER	"@smarterclayton I agree with separating policy and mechanism. 

Primitives we need:
1. The ability to poll/watch a set identified by a label selector. Not sure if there is an issue filed yet.
2. The ability to query pod IP addresses (#385).

This would be enough to compose with other naming/discovery mechanisms and/or load balancers. We could then build a higher-level layer on top of the core that bundles common patterns with a simple API.
"
933	365	thockin	2014-07-09 23:14:27	MEMBER	"Merging.  We can fix anything else in followups.
"
934	368	verdverm	2014-07-09 23:25:34	NONE	"no complaints, still no dockers running on any of the minions

```
Minion identifier
----------
kube-minion-1.c.cloud-pge.internal
kube-minion-2.c.cloud-pge.internal
kube-minion-3.c.cloud-pge.internal
kube-minion-4.c.cloud-pge.internal
```
"
935	386	bgrant0607	2014-07-09 23:42:42	MEMBER	"@smarterclayton Hmm. I see the case for doing the substitution in k8s rather than in the container, though I do have some concerns about introducing yet another substitution mechanism.

I was thinking that, at least for now, any additional information the container wanted about itself could be retrieved by querying the apiserver, which would obviously also couple the querier to k8s. 

I think we are going to need to standardize on a ""downward-facing API"", including this type of contextual information (and also IP address, DNS name, auth info, etc.), but also a variety of notifications and/or hooks (#140). If we had hooks, we could make the context info available to the hooks, which could then normalize the info for the application. I see hooks as being necessarily specific to the hosting framework and other glue services.
"
936	368	vmarmol	2014-07-09 23:46:53	CONTRIBUTOR	"what does the minion logs say?: /var/log/kubelet.log I think

On Wed, Jul 9, 2014 at 4:25 PM, Tony Worm notifications@github.com wrote:

> no complaints, still no dockers running on any of the minions
> 
> ## Minion identifier
> 
> kube-minion-1.c.cloud-pge.internal
> kube-minion-2.c.cloud-pge.internal
> kube-minion-3.c.cloud-pge.internal
> kube-minion-4.c.cloud-pge.internal
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/368#issuecomment-48548117
> .
"
937	368	brendandburns	2014-07-09 23:51:08	CONTRIBUTOR	"Also, look at /var/log/controller-manager.log on the kubernetes-master

Thanks!
--brendan

On Wed, Jul 9, 2014 at 4:47 PM, Victor Marmol notifications@github.com
wrote:

> what does the minion logs say?: /var/log/kubelet.log I think
> 
> On Wed, Jul 9, 2014 at 4:25 PM, Tony Worm notifications@github.com
> wrote:
> 
> > no complaints, still no dockers running on any of the minions
> > 
> > ## Minion identifier
> > 
> > kube-minion-1.c.cloud-pge.internal
> > kube-minion-2.c.cloud-pge.internal
> > kube-minion-3.c.cloud-pge.internal
> > kube-minion-4.c.cloud-pge.internal
> > 
> > —
> > Reply to this email directly or view it on GitHub
> > <
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/368#issuecomment-48548117>
> > 
> > .
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/368#issuecomment-48549687
> .
"
938	368	verdverm	2014-07-09 23:55:37	NONE	"MASTER:

```
I0709 23:22:24.679437 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:22:24.679458 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:22:24.679839 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:22:24.679866 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:22:34.679349 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:22:34.679439 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:22:34.679455 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:22:34.679853 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:22:34.679925 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:22:44.679285 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:22:44.679374 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:22:44.679389 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:22:44.679769 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:22:44.679844 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:22:54.679373 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:22:54.679455 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:22:54.679498 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:22:54.679920 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:22:54.679991 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:23:04.679349 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:23:04.679441 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:23:04.679456 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:23:04.679861 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:23:04.679933 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:23:14.679286 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:23:14.679366 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:23:14.679390 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:23:14.679800 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:23:14.679895 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:23:17.779099 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true]
I0709 23:23:17.779251 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:23:17.779283 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&wait=true&waitIndex=4]
I0709 23:23:17.779294 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true&waitIndex=4  | method  GET]
I0709 23:23:17.779334 15148 replication_controller.go:128] Got watch: &etcd.Response{Action:""set"", Node:(*etcd.Node)(0xc210089840), PrevNode:(*etcd.Node)(nil), EtcdIndex:0x2, RaftIndex:0x270, RaftTerm:0x0}
I0709 23:23:17.779842 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true&waitIndex=4]
I0709 23:23:17.930444 15148 replication_controller.go:187] []api.Pod(nil)
I0709 23:23:17.930471 15148 replication_controller.go:190] Too few replicas, creating 4
I0709 23:28:17.779260 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true&waitIndex=4]
E0709 23:28:17.779316 15148 replication_controller.go:113] etcd.Watch stopped unexpectedly: unexpected end of JSON input (&json.SyntaxError{msg:""unexpected end of JSON input"", Offset:0})
```

MINION-3

```
Couldn't unmarshal configuration: YAML error: resolveTable item not yet handled: < (with <!DOCTYPE html> <html lang=en> <meta charset=utf-8> <meta name=viewport content=""initial-scale=1, minimum-scale=1, width=device-width""> <title>Error 404 (Not Found)!!1</title> <style> *{margin:0;padding:0}html,code{font:15px/22px arial,sans-serif}html{background:#fff;color:#222;padding:15px}body{margin:7% auto 0;max-width:390px;min-height:180px;padding:30px 0 15px}* > body{background:url(//www.google.com/images/errors/robot.png) 100% 5px no-repeat;padding-right:205px}p{margin:11px 0 22px;overflow:hidden}ins{color:#777;text-decoration:none}a img{border:0}@media screen and (max-width:772px){body{background:none;margin-top:0;max-width:none;padding-right:0}}#logo{background:url(//www.google.com/images/errors/logo_sm_2.png) no-repeat}@media only screen and (min-resolution:192dpi){#logo{background:url(//www.google.com/images/errors/logo_sm_2_hr.png) no-repeat 0% 0%/100% 100%;-moz-border-image:url(//www.google.com/images/errors/logo_sm_2_hr.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/errors/logo_sm_2_hr.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:55px;width:150px} </style> <a href=//www.google.com/><span id=logo aria-label=Google></span></a> <p><b>404.</b> <ins>That’s an error.</ins> <p>The requested URL <code>/computeMetadata/v1beta1/instance/attributes/google-container-manifest</code> was not found on this server.  <ins>That’s all we know.</ins>)
```
"
939	368	brendanburns	2014-07-09 23:58:44	CONTRIBUTOR	"Is there more on the master after ""Too few replicas, creating 4""?

Thanks
--brendan

On Wed, Jul 9, 2014 at 4:55 PM, Tony Worm notifications@github.com wrote:

> MASTER:
> 
> I0709 23:22:24.679437 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:22:24.679458 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:22:24.679839 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:22:24.679866 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:22:34.679349 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:22:34.679439 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:22:34.679455 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:22:34.679853 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:22:34.679925 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:22:44.679285 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:22:44.679374 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:22:44.679389 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:22:44.679769 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:22:44.679844 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:22:54.679373 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:22:54.679455 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:22:54.679498 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:22:54.679920 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:22:54.679991 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:23:04.679349 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:23:04.679441 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:23:04.679456 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:23:04.679861 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:23:04.679933 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:23:14.679286 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:23:14.679366 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:23:14.679390 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:23:14.679800 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:23:14.679895 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:23:17.779099 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true]
> I0709 23:23:17.779251 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:23:17.779283 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&wait=true&waitIndex=4]
> I0709 23:23:17.779294 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true&waitIndex=4  | method  GET]
> I0709 23:23:17.779334 15148 replication_controller.go:128] Got watch: &etcd.Response{Action:""set"", Node:(_etcd.Node)(0xc210089840), PrevNode:(_etcd.Node)(nil), EtcdIndex:0x2, RaftIndex:0x270, RaftTerm:0x0}
> I0709 23:23:17.779842 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true&waitIndex=4]
> I0709 23:23:17.930444 15148 replication_controller.go:187] []api.Pod(nil)
> I0709 23:23:17.930471 15148 replication_controller.go:190] Too few replicas, creating 4
> I0709 23:28:17.779260 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true&waitIndex=4]
> E0709 23:28:17.779316 15148 replication_controller.go:113] etcd.Watch stopped unexpectedly: unexpected end of JSON input (&json.SyntaxError{msg:""unexpected end of JSON input"", Offset:0})
> 
> MINION-3
> 
> Couldn't unmarshal configuration: YAML error: resolveTable item not yet handled: < (with <!DOCTYPE html> <html lang=en> <meta charset=utf-8> <meta name=viewport content=""initial-scale=1, minimum-scale=1, width=device-width""> <title>Error 404 (Not Found)!!1</title> <style> _{margin:0;padding:0}html,code{font:15px/22px arial,sans-serif}html{background:#fff;color:#222;padding:15px}body{margin:7% auto 0;max-width:390px;min-height:180px;padding:30px 0 15px}_ > body{background:url(//www.google.com/images/errors/robot.png) 100% 5px no-repeat;padding-right:205px}p{margin:11px 0 22px;overflow:hidden}ins{color:#777;text-decoration:none}a img{border:0}@media screen and (max-width:772px){body{background:none;margin-top:0;max-width:none;padding-right:0}}#logo{background:url(//www.google.com/images/errors/logo_sm_2.png) no-repeat}@media only screen and (min-resolution:192dpi){#logo{background:url(//www.google.com/images/errors/logo_sm_2_hr.png) no-repeat 0% 0%/100% 100%;-moz-border-image:url(//www.google.com/images/errors/logo_sm_2_hr.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/errors/logo_sm_2_hr.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:55px;width:150px} </style> <a href=//www.google.com/><span id=logo aria-label=Google></span></a> <p><b>404.</b> <ins>That’s an error.</ins> <p>The requested URL <code>/computeMetadata/v1beta1/instance/attributes/google-container-manifest</code> was not found on this server.  <ins>That’s all we know.</ins>)
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/368#issuecomment-48550279
> .
"
940	368	verdverm	2014-07-09 23:59:57	NONE	"FULL MASTER LOG

```

I0709 23:13:44.679194 15148 logs.go:38] etcd DEBUG: watch [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:13:44.679269 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:13:44.679308 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&wait=true]
I0709 23:13:44.679320 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true  | method  GET]
I0709 23:13:44.679475 15148 logs.go:38] etcd DEBUG: [network error: Get http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true: dial tcp 10.240.16.104:4001: connection refused]
E0709 23:13:44.679489 15148 replication_controller.go:113] etcd.Watch stopped unexpectedly: 501: All the given peers are not reachable (Tried to connect to each peer twice and failed) [0] (&etcd.EtcdError{ErrorCode:501, Message:""All the given peers are not reachable"", Cause:""Tried to connect to each peer twice and failed"", Index:0x0})
I0709 23:13:54.679612 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:13:54.679676 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:13:54.679689 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:13:54.679725 15148 logs.go:38] etcd DEBUG: watch [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:13:54.679740 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:13:54.679758 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&wait=true]
I0709 23:13:54.679770 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true  | method  GET]
I0709 23:13:54.679946 15148 logs.go:38] etcd DEBUG: [network error: Get http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true: dial tcp 10.240.16.104:4001: connection refused]
E0709 23:13:54.679959 15148 replication_controller.go:113] etcd.Watch stopped unexpectedly: 501: All the given peers are not reachable (Tried to connect to each peer twice and failed) [0] (&etcd.EtcdError{ErrorCode:501, Message:""All the given peers are not reachable"", Cause:""Tried to connect to each peer twice and failed"", Index:0x0})
I0709 23:13:54.680010 15148 logs.go:38] etcd DEBUG: [network error: Get http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false: dial tcp 10.240.16.104:4001: connection refused]
E0709 23:13:54.680021 15148 replication_controller.go:208] Synchronization error: 501: All the given peers are not reachable (Tried to connect to each peer twice and failed) [0] (&etcd.EtcdError{ErrorCode:501, Message:""All the given peers are not reachable"", Cause:""Tried to connect to each peer twice and failed"", Index:0x0})
I0709 23:14:04.680814 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:14:04.680915 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:14:04.680938 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:14:04.680975 15148 logs.go:38] etcd DEBUG: watch [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:14:04.680996 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:14:04.681032 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&wait=true]
I0709 23:14:04.681077 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true  | method  GET]
I0709 23:14:04.681287 15148 logs.go:38] etcd DEBUG: [network error: Get http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true: dial tcp 10.240.16.104:4001: connection refused]
E0709 23:14:04.681306 15148 replication_controller.go:113] etcd.Watch stopped unexpectedly: 501: All the given peers are not reachable (Tried to connect to each peer twice and failed) [0] (&etcd.EtcdError{ErrorCode:501, Message:""All the given peers are not reachable"", Cause:""Tried to connect to each peer twice and failed"", Index:0x0})
I0709 23:14:04.681400 15148 logs.go:38] etcd DEBUG: [network error: Get http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false: dial tcp 10.240.16.104:4001: connection refused]
E0709 23:14:04.681426 15148 replication_controller.go:208] Synchronization error: 501: All the given peers are not reachable (Tried to connect to each peer twice and failed) [0] (&etcd.EtcdError{ErrorCode:501, Message:""All the given peers are not reachable"", Cause:""Tried to connect to each peer twice and failed"", Index:0x0})
I0709 23:14:14.681983 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:14:14.682051 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:14:14.682067 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:14:14.682095 15148 logs.go:38] etcd DEBUG: watch [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:14:14.682106 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:14:14.682205 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&wait=true]
I0709 23:14:14.682217 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true  | method  GET]
I0709 23:14:14.683200 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true]
I0709 23:14:14.683241 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:14:14.683264 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:14:24.679537 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:14:24.679602 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:14:24.679617 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:14:24.679983 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:14:24.680033 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:14:34.679408 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:14:34.679570 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:14:34.679593 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:14:34.680092 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:14:34.680127 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:14:44.679287 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:14:44.679337 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:14:44.679351 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:14:44.679833 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:14:44.679873 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:14:54.679366 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:14:54.679433 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:14:54.679447 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:14:54.679837 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:14:54.679876 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:15:04.679335 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:15:04.679424 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:15:04.679438 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:15:04.679834 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:15:04.679864 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:15:14.679289 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:15:14.679384 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:15:14.679400 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:15:14.679804 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:15:14.679845 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:15:24.679342 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:15:24.679428 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:15:24.679444 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:15:24.679820 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:15:24.679918 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:15:34.679339 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:15:34.679398 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:15:34.679412 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:15:34.679771 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:15:34.679857 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:15:44.679303 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:15:44.679389 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:15:44.679411 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:15:44.679789 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:15:44.679873 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:15:54.679329 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:15:54.679405 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:15:54.679421 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:15:54.679796 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:15:54.679825 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:16:04.679349 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:16:04.679443 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:16:04.679475 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:16:04.679859 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:16:04.679928 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:16:14.679314 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:16:14.679391 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:16:14.679426 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:16:14.679841 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:16:14.679941 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:16:24.679372 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:16:24.679839 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:16:24.679860 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:16:24.680266 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:16:24.680343 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:16:34.679352 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:16:34.679415 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:16:34.679430 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:16:34.679812 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:16:34.679882 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:16:44.679285 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:16:44.679372 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:16:44.679387 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:16:44.679786 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:16:44.679820 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:16:54.679333 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:16:54.679401 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:16:54.679426 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:16:54.679813 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:16:54.679888 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:17:04.679343 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:17:04.679433 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:17:04.679448 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:17:04.679824 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:17:04.679895 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:17:14.679299 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:17:14.679358 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:17:14.679374 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:17:14.679732 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:17:14.679813 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:17:24.679322 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:17:24.679402 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:17:24.679417 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:17:24.679786 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:17:24.679856 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:17:34.679340 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:17:34.679411 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:17:34.679426 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:17:34.679816 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:17:34.679894 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:17:44.679315 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:17:44.679383 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:17:44.679398 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:17:44.679820 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:17:44.679905 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:17:54.679348 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:17:54.679422 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:17:54.679436 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:17:54.679848 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:17:54.679942 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:18:04.679506 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:18:04.679613 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:18:04.679631 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:18:04.679987 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:18:04.680085 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:18:14.679315 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:18:14.679383 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:18:14.679397 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:18:14.679817 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:18:14.679910 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:18:24.679333 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:18:24.679416 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:18:24.679430 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:18:24.679853 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:18:24.679940 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:18:34.679329 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:18:34.679398 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:18:34.679413 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:18:34.679810 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:18:34.679889 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:18:44.679290 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:18:44.679371 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:18:44.679385 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:18:44.679812 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:18:44.679890 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:18:54.679346 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:18:54.679455 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:18:54.679470 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:18:54.679866 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:18:54.679944 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:19:04.679347 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:19:04.679438 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:19:04.679453 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:19:04.679869 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:19:04.679955 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:19:14.679326 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:19:14.679402 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:19:14.679417 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:19:14.679785 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:19:14.679856 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
E0709 23:19:14.683273 15148 replication_controller.go:113] etcd.Watch stopped unexpectedly: 501: All the given peers are not reachable (Tried to connect to each peer twice and failed) [0] (&etcd.EtcdError{ErrorCode:501, Message:""All the given peers are not reachable"", Cause:""Tried to connect to each peer twice and failed"", Index:0x0})
I0709 23:19:24.683482 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:19:24.683559 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:19:24.683574 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:19:24.683614 15148 logs.go:38] etcd DEBUG: watch [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:19:24.683626 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:19:24.683651 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&wait=true]
I0709 23:19:24.683662 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true  | method  GET]
I0709 23:19:24.684269 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true]
I0709 23:19:24.684533 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:19:24.684622 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:19:34.679347 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:19:34.679422 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:19:34.679436 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:19:34.679845 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:19:34.679930 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:19:44.679277 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:19:44.679346 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:19:44.679361 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:19:44.679744 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:19:44.679814 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:19:54.679339 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:19:54.679427 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:19:54.679441 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:19:54.679833 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:19:54.679904 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:20:04.679346 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:20:04.679437 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:20:04.679452 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:20:04.679862 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:20:04.679940 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:20:14.679321 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:20:14.679399 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:20:14.679418 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:20:14.679823 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:20:14.679847 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:20:24.679335 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:20:24.679427 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:20:24.679443 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:20:24.679853 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:20:24.679928 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:20:34.679396 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:20:34.679483 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:20:34.679498 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:20:34.679864 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:20:34.679937 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:20:44.679271 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:20:44.679352 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:20:44.679367 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:20:44.679747 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:20:44.679818 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:20:54.679349 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:20:54.679420 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:20:54.679435 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:20:54.679804 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:20:54.679902 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:21:04.679346 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:21:04.679424 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:21:04.679439 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:21:04.679835 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:21:04.679904 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:21:14.679286 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:21:14.679351 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:21:14.679365 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:21:14.679767 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:21:14.679866 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:21:24.679354 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:21:24.679431 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:21:24.679446 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:21:24.679847 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:21:24.679927 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:21:34.679332 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:21:34.679427 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:21:34.679444 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:21:34.679887 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:21:34.679961 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:21:44.679294 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:21:44.679382 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:21:44.679399 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:21:44.679816 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:21:44.679906 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:21:54.679344 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:21:54.679426 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:21:54.679441 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:21:54.679846 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:21:54.679933 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:22:04.679351 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:22:04.679470 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:22:04.679486 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:22:04.679923 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:22:04.679998 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:22:14.679313 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:22:14.679397 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:22:14.679411 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:22:14.679841 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:22:14.679911 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:22:24.679364 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:22:24.679437 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:22:24.679458 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:22:24.679839 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:22:24.679866 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:22:34.679349 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:22:34.679439 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:22:34.679455 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:22:34.679853 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:22:34.679925 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:22:44.679285 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:22:44.679374 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:22:44.679389 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:22:44.679769 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:22:44.679844 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:22:54.679373 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:22:54.679455 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:22:54.679498 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:22:54.679920 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:22:54.679991 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:23:04.679349 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:23:04.679441 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:23:04.679456 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:23:04.679861 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:23:04.679933 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:23:14.679286 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:23:14.679366 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:23:14.679390 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:23:14.679800 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:23:14.679895 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:23:17.779099 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true]
I0709 23:23:17.779251 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:23:17.779283 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&wait=true&waitIndex=4]
I0709 23:23:17.779294 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true&waitIndex=4  | method  GET]
I0709 23:23:17.779334 15148 replication_controller.go:128] Got watch: &etcd.Response{Action:""set"", Node:(*etcd.Node)(0xc210089840), PrevNode:(*etcd.Node)(nil), EtcdIndex:0x2, RaftIndex:0x270, RaftTerm:0x0}
I0709 23:23:17.779842 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true&waitIndex=4]
I0709 23:23:17.930444 15148 replication_controller.go:187] []api.Pod(nil)
I0709 23:23:17.930471 15148 replication_controller.go:190] Too few replicas, creating 4
I0709 23:28:17.779260 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true&waitIndex=4]
E0709 23:28:17.779316 15148 replication_controller.go:113] etcd.Watch stopped unexpectedly: unexpected end of JSON input (&json.SyntaxError{msg:""unexpected end of JSON input"", Offset:0})
```
"
941	368	brendandburns	2014-07-10 00:02:06	CONTRIBUTOR	"Is the controller manager still running:

either
/etc/init.d/controller-manager status

or

ps -ef | grep controller-manager

?

I wonder if the request to create the pod is hanging?

On Wed, Jul 9, 2014 at 5:00 PM, Tony Worm notifications@github.com wrote:

> FULL MASTER LOG
> 
> I0709 23:13:44.679194 15148 logs.go:38] etcd DEBUG: watch [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:13:44.679269 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:13:44.679308 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&wait=true]
> I0709 23:13:44.679320 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true  | method  GET]
> I0709 23:13:44.679475 15148 logs.go:38] etcd DEBUG: [network error: Get http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true: dial tcp 10.240.16.104:4001: connection refused]
> E0709 23:13:44.679489 15148 replication_controller.go:113] etcd.Watch stopped unexpectedly: 501: All the given peers are not reachable (Tried to connect to each peer twice and failed) [0](&etcd.EtcdError{ErrorCode:501, Message:""All the given peers are not reachable"", Cause:""Tried to connect to each peer twice and failed"", Index:0x0})
> I0709 23:13:54.679612 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:13:54.679676 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:13:54.679689 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:13:54.679725 15148 logs.go:38] etcd DEBUG: watch [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:13:54.679740 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:13:54.679758 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&wait=true]
> I0709 23:13:54.679770 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true  | method  GET]
> I0709 23:13:54.679946 15148 logs.go:38] etcd DEBUG: [network error: Get http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true: dial tcp 10.240.16.104:4001: connection refused]
> E0709 23:13:54.679959 15148 replication_controller.go:113] etcd.Watch stopped unexpectedly: 501: All the given peers are not reachable (Tried to connect to each peer twice and failed) [0](&etcd.EtcdError{ErrorCode:501, Message:""All the given peers are not reachable"", Cause:""Tried to connect to each peer twice and failed"", Index:0x0})
> I0709 23:13:54.680010 15148 logs.go:38] etcd DEBUG: [network error: Get http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false: dial tcp 10.240.16.104:4001: connection refused]
> E0709 23:13:54.680021 15148 replication_controller.go:208] Synchronization error: 501: All the given peers are not reachable (Tried to connect to each peer twice and failed) [0](&etcd.EtcdError{ErrorCode:501, Message:""All the given peers are not reachable"", Cause:""Tried to connect to each peer twice and failed"", Index:0x0})
> I0709 23:14:04.680814 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:14:04.680915 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:14:04.680938 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:14:04.680975 15148 logs.go:38] etcd DEBUG: watch [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:14:04.680996 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:14:04.681032 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&wait=true]
> I0709 23:14:04.681077 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true  | method  GET]
> I0709 23:14:04.681287 15148 logs.go:38] etcd DEBUG: [network error: Get http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true: dial tcp 10.240.16.104:4001: connection refused]
> E0709 23:14:04.681306 15148 replication_controller.go:113] etcd.Watch stopped unexpectedly: 501: All the given peers are not reachable (Tried to connect to each peer twice and failed) [0](&etcd.EtcdError{ErrorCode:501, Message:""All the given peers are not reachable"", Cause:""Tried to connect to each peer twice and failed"", Index:0x0})
> I0709 23:14:04.681400 15148 logs.go:38] etcd DEBUG: [network error: Get http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false: dial tcp 10.240.16.104:4001: connection refused]
> E0709 23:14:04.681426 15148 replication_controller.go:208] Synchronization error: 501: All the given peers are not reachable (Tried to connect to each peer twice and failed) [0](&etcd.EtcdError{ErrorCode:501, Message:""All the given peers are not reachable"", Cause:""Tried to connect to each peer twice and failed"", Index:0x0})
> I0709 23:14:14.681983 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:14:14.682051 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:14:14.682067 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:14:14.682095 15148 logs.go:38] etcd DEBUG: watch [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:14:14.682106 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:14:14.682205 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&wait=true]
> I0709 23:14:14.682217 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true  | method  GET]
> I0709 23:14:14.683200 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true]
> I0709 23:14:14.683241 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:14:14.683264 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:14:24.679537 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:14:24.679602 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:14:24.679617 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:14:24.679983 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:14:24.680033 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:14:34.679408 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:14:34.679570 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:14:34.679593 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:14:34.680092 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:14:34.680127 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:14:44.679287 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:14:44.679337 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:14:44.679351 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:14:44.679833 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:14:44.679873 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:14:54.679366 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:14:54.679433 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:14:54.679447 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:14:54.679837 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:14:54.679876 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:15:04.679335 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:15:04.679424 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:15:04.679438 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:15:04.679834 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:15:04.679864 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:15:14.679289 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:15:14.679384 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:15:14.679400 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:15:14.679804 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:15:14.679845 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:15:24.679342 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:15:24.679428 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:15:24.679444 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:15:24.679820 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:15:24.679918 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:15:34.679339 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:15:34.679398 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:15:34.679412 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:15:34.679771 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:15:34.679857 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:15:44.679303 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:15:44.679389 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:15:44.679411 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:15:44.679789 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:15:44.679873 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:15:54.679329 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:15:54.679405 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:15:54.679421 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:15:54.679796 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:15:54.679825 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:16:04.679349 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:16:04.679443 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:16:04.679475 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:16:04.679859 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:16:04.679928 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:16:14.679314 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:16:14.679391 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:16:14.679426 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:16:14.679841 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:16:14.679941 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:16:24.679372 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:16:24.679839 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:16:24.679860 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:16:24.680266 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:16:24.680343 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:16:34.679352 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:16:34.679415 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:16:34.679430 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:16:34.679812 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:16:34.679882 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:16:44.679285 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:16:44.679372 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:16:44.679387 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:16:44.679786 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:16:44.679820 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:16:54.679333 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:16:54.679401 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:16:54.679426 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:16:54.679813 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:16:54.679888 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:17:04.679343 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:17:04.679433 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:17:04.679448 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:17:04.679824 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:17:04.679895 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:17:14.679299 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:17:14.679358 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:17:14.679374 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:17:14.679732 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:17:14.679813 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:17:24.679322 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:17:24.679402 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:17:24.679417 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:17:24.679786 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:17:24.679856 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:17:34.679340 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:17:34.679411 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:17:34.679426 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:17:34.679816 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:17:34.679894 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:17:44.679315 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:17:44.679383 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:17:44.679398 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:17:44.679820 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:17:44.679905 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:17:54.679348 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:17:54.679422 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:17:54.679436 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:17:54.679848 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:17:54.679942 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:18:04.679506 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:18:04.679613 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:18:04.679631 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:18:04.679987 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:18:04.680085 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:18:14.679315 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:18:14.679383 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:18:14.679397 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:18:14.679817 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:18:14.679910 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:18:24.679333 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:18:24.679416 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:18:24.679430 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:18:24.679853 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:18:24.679940 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:18:34.679329 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:18:34.679398 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:18:34.679413 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:18:34.679810 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:18:34.679889 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:18:44.679290 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:18:44.679371 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:18:44.679385 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:18:44.679812 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:18:44.679890 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:18:54.679346 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:18:54.679455 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:18:54.679470 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:18:54.679866 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:18:54.679944 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:19:04.679347 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:19:04.679438 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:19:04.679453 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:19:04.679869 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:19:04.679955 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:19:14.679326 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:19:14.679402 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:19:14.679417 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:19:14.679785 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:19:14.679856 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> E0709 23:19:14.683273 15148 replication_controller.go:113] etcd.Watch stopped unexpectedly: 501: All the given peers are not reachable (Tried to connect to each peer twice and failed) [0](&etcd.EtcdError{ErrorCode:501, Message:""All the given peers are not reachable"", Cause:""Tried to connect to each peer twice and failed"", Index:0x0})
> I0709 23:19:24.683482 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:19:24.683559 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:19:24.683574 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:19:24.683614 15148 logs.go:38] etcd DEBUG: watch [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:19:24.683626 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:19:24.683651 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&wait=true]
> I0709 23:19:24.683662 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true  | method  GET]
> I0709 23:19:24.684269 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true]
> I0709 23:19:24.684533 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:19:24.684622 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:19:34.679347 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:19:34.679422 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:19:34.679436 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:19:34.679845 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:19:34.679930 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:19:44.679277 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:19:44.679346 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:19:44.679361 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:19:44.679744 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:19:44.679814 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:19:54.679339 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:19:54.679427 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:19:54.679441 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:19:54.679833 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:19:54.679904 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:20:04.679346 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:20:04.679437 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:20:04.679452 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:20:04.679862 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:20:04.679940 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:20:14.679321 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:20:14.679399 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:20:14.679418 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:20:14.679823 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:20:14.679847 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:20:24.679335 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:20:24.679427 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:20:24.679443 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:20:24.679853 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:20:24.679928 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:20:34.679396 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:20:34.679483 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:20:34.679498 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:20:34.679864 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:20:34.679937 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:20:44.679271 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:20:44.679352 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:20:44.679367 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:20:44.679747 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:20:44.679818 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:20:54.679349 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:20:54.679420 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:20:54.679435 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:20:54.679804 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:20:54.679902 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:21:04.679346 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:21:04.679424 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:21:04.679439 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:21:04.679835 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:21:04.679904 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:21:14.679286 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:21:14.679351 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:21:14.679365 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:21:14.679767 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:21:14.679866 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:21:24.679354 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:21:24.679431 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:21:24.679446 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:21:24.679847 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:21:24.679927 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:21:34.679332 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:21:34.679427 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:21:34.679444 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:21:34.679887 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:21:34.679961 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:21:44.679294 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:21:44.679382 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:21:44.679399 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:21:44.679816 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:21:44.679906 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:21:54.679344 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:21:54.679426 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:21:54.679441 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:21:54.679846 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:21:54.679933 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:22:04.679351 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:22:04.679470 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:22:04.679486 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:22:04.679923 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:22:04.679998 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:22:14.679313 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:22:14.679397 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:22:14.679411 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:22:14.679841 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:22:14.679911 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:22:24.679364 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:22:24.679437 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:22:24.679458 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:22:24.679839 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:22:24.679866 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:22:34.679349 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:22:34.679439 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:22:34.679455 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:22:34.679853 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:22:34.679925 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:22:44.679285 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:22:44.679374 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:22:44.679389 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:22:44.679769 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:22:44.679844 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:22:54.679373 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:22:54.679455 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:22:54.679498 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:22:54.679920 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:22:54.679991 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:23:04.679349 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:23:04.679441 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:23:04.679456 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:23:04.679861 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:23:04.679933 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:23:14.679286 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:23:14.679366 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:23:14.679390 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:23:14.679800 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:23:14.679895 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:23:17.779099 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true]
> I0709 23:23:17.779251 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:23:17.779283 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&wait=true&waitIndex=4]
> I0709 23:23:17.779294 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true&waitIndex=4  | method  GET]
> I0709 23:23:17.779334 15148 replication_controller.go:128] Got watch: &etcd.Response{Action:""set"", Node:(_etcd.Node)(0xc210089840), PrevNode:(_etcd.Node)(nil), EtcdIndex:0x2, RaftIndex:0x270, RaftTerm:0x0}
> I0709 23:23:17.779842 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true&waitIndex=4]
> I0709 23:23:17.930444 15148 replication_controller.go:187] []api.Pod(nil)
> I0709 23:23:17.930471 15148 replication_controller.go:190] Too few replicas, creating 4
> I0709 23:28:17.779260 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true&waitIndex=4]
> E0709 23:28:17.779316 15148 replication_controller.go:113] etcd.Watch stopped unexpectedly: unexpected end of JSON input (&json.SyntaxError{msg:""unexpected end of JSON input"", Offset:0})
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/368#issuecomment-48550561
> .
"
942	368	verdverm	2014-07-10 00:04:24	NONE	"```
tony@kube-master:/var/log$ ps -ef | grep controller-manager
998      15148     1  0 Jul09 ?        00:00:00 /usr/local/bin/controller-manager --master=127.0.0.1:8080 -etcd_servers=http://10.240.16.104:4001
tony     17663 17234  0 00:03 pts/1    00:00:00 grep controller-manager
tony@kube-master:/var/log$ /etc/init.d/controller-manager status
[ ok ] controller-manager is running.
```
"
943	386	smarterclayton	2014-07-10 00:07:25	CONTRIBUTOR	"That's true, although one strong goal in docker has been to try and offer via libchan/libswarm a ""Docker"" abstraction where on multiple platforms images can invoke consistent actions that affect the platform.  That applies to more generic operations, whereas k8s specific actions will always require some downwards API.  The abstraction is a long term play that requires active participation by a number of stakeholders and also implemented, demonstrated use cases, so it's not an argument for avoiding trying.

I don't think the downward API is bad, but some of the use cases (unique id for an instance, what ip do I have) might benefit from looser coupling or by trying to find the most lowest common denominator expression.  In the end image authors can always ignore those settings, but you do want image users to have the tools to bend a recaltricant image to their will.
"
944	368	brendandburns	2014-07-10 00:22:32	CONTRIBUTOR	"OK, my guess is the api request to create a pod is hanging (clearly we
should add some timeouts ...)

What's in /var/log/apiserver.log on the master?

Brendan
On Jul 9, 2014 5:04 PM, ""Tony Worm"" notifications@github.com wrote:

> tony@kube-master:/var/log$ ps -ef | grep controller-manager
> 998      15148     1  0 Jul09 ?        00:00:00 /usr/local/bin/controller-manager --master=127.0.0.1:8080 -etcd_servers=http://10.240.16.104:4001
> tony     17663 17234  0 00:03 pts/1    00:00:00 grep controller-manager
> tony@kube-master:/var/log$ /etc/init.d/controller-manager status
> [ ok ] controller-manager is running.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/368#issuecomment-48550875
> .
"
945	368	verdverm	2014-07-10 00:41:28	NONE	"/var/log/apiserver.log is full of errors

```
E0710 00:37:35.989748 10488 pod_cache.go:86] Error synchronizing container: &url.Error{Op:""Get"", URL:""http://kube-minion-3.c.cloud-pge.internal:10250/podInfo?podID=9acb0442"", Err:(*net.OpError)(0xc210209cc0)}
I0710 00:37:38.139230 10488 logger.go:111] GET /api/v1beta1/operations/2: (78.984us) 202
E0710 00:37:42.597234 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
E0710 00:37:52.892041 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
I0710 00:37:58.140174 10488 logger.go:111] GET /api/v1beta1/operations/2: (53.212us) 202
E0710 00:38:03.130514 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
E0710 00:38:06.132851 10488 pod_cache.go:86] Error synchronizing container: &url.Error{Op:""Get"", URL:""http://kube-minion-3.c.cloud-pge.internal:10250/podInfo?podID=9acb0442"", Err:(*net.OpError)(0xc210293940)}
E0710 00:38:13.406755 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
I0710 00:38:18.141094 10488 logger.go:111] GET /api/v1beta1/operations/2: (58.027us) 202
E0710 00:38:23.661463 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
E0710 00:38:34.000058 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
E0710 00:38:36.281340 10488 pod_cache.go:86] Error synchronizing container: &url.Error{Op:""Get"", URL:""http://kube-minion-3.c.cloud-pge.internal:10250/podInfo?podID=9acb0442"", Err:(*net.OpError)(0xc2103145c0)}
I0710 00:38:38.141953 10488 logger.go:111] GET /api/v1beta1/operations/2: (39.632us) 202
E0710 00:38:44.234893 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
E0710 00:38:54.510522 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
I0710 00:38:58.142792 10488 logger.go:111] GET /api/v1beta1/operations/2: (47.495us) 202
E0710 00:39:04.818036 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
E0710 00:39:06.406022 10488 pod_cache.go:86] Error synchronizing container: &url.Error{Op:""Get"", URL:""http://kube-minion-3.c.cloud-pge.internal:10250/podInfo?podID=9acb0442"", Err:(*net.OpError)(0xc21014ebc0)}
E0710 00:39:15.067191 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
I0710 00:39:18.143782 10488 logger.go:111] GET /api/v1beta1/operations/2: (55.755us) 202
E0710 00:39:25.331916 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
E0710 00:39:35.688293 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
E0710 00:39:36.561170 10488 pod_cache.go:86] Error synchronizing container: &url.Error{Op:""Get"", URL:""http://kube-minion-3.c.cloud-pge.internal:10250/podInfo?podID=9acb0442"", Err:(*net.OpError)(0xc210395200)}
I0710 00:39:38.144647 10488 logger.go:111] GET /api/v1beta1/operations/2: (61.725us) 202
E0710 00:39:45.973080 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
E0710 00:39:56.267239 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
I0710 00:39:58.145535 10488 logger.go:111] GET /api/v1beta1/operations/2: (43.775us) 202
E0710 00:40:06.510327 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
E0710 00:40:06.732296 10488 pod_cache.go:86] Error synchronizing container: &url.Error{Op:""Get"", URL:""http://kube-minion-3.c.cloud-pge.internal:10250/podInfo?podID=9acb0442"", Err:(*net.OpError)(0xc2102b4100)}
```
"
946	368	verdverm	2014-07-10 00:42:04	NONE	"I noticed cAdvisor is not running in any of the minions
"
947	368	verdverm	2014-07-10 00:43:05	NONE	"here's the head of the apiserver.log

```
E0709 23:13:06.129932 10488 pod_cache.go:80] Error synchronizing container list: &etcd.EtcdError{ErrorCode:501, Message:""All the given peers are not reachable"", Cause:""Tried to connect to each peer twice and failed"", Index:0x0}
E0709 23:13:36.386628 10488 pod_cache.go:80] Error synchronizing container list: &etcd.EtcdError{ErrorCode:501, Message:""All the given peers are not reachable"", Cause:""Tried to connect to each peer twice and failed"", Index:0x0}
E0709 23:14:06.624785 10488 pod_cache.go:80] Error synchronizing container list: &etcd.EtcdError{ErrorCode:501, Message:""All the given peers are not reachable"", Cause:""Tried to connect to each peer twice and failed"", Index:0x0}
I0709 23:14:32.376591 10488 logger.go:111] GET /api/v1beta1/pods: (250.086536ms) 200
I0709 23:22:18.029275 10488 logger.go:111] GET /api/v1beta1/minions?labels=: (361.476716ms) 200
I0709 23:22:39.509469 10488 logger.go:111] GET /api/v1beta1/services?labels=: (531.306us) 200
I0709 23:22:41.115735 10488 logger.go:111] GET /api/v1beta1/replicationControllers?labels=: (577.55us) 200
I0709 23:22:42.279955 10488 logger.go:111] GET /api/v1beta1/pods?labels=: (132.530447ms) 200
I0709 23:23:17.778127 10488 logger.go:111] POST /api/v1beta1/replicationControllers?labels=: (428.798us) 202
I0709 23:23:17.930070 10488 logger.go:111] GET /api/v1beta1/pods?labels=name%3Dalgebra: (150.152397ms) 200
I0709 23:23:17.931302 10488 logger.go:111] POST /api/v1beta1/pods: (351.445us) 202
E0709 23:23:18.546175 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
E0709 23:23:28.796310 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
I0709 23:23:37.822511 10488 logger.go:111] GET /api/v1beta1/operations/1: (37.04us) 202
I0709 23:23:37.932141 10488 logger.go:111] GET /api/v1beta1/operations/2: (37.74us) 202
E0709 23:23:39.149032 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
E0709 23:23:40.772891 10488 pod_cache.go:86] Error synchronizing container: &url.Error{Op:""Get"", URL:""http://kube-minion-3.c.cloud-pge.internal:10250/podInfo?podID=9acb0442"", Err:(*net.OpError)(0xc2102b5e40)}
E0709 23:23:49.391719 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
I0709 23:23:57.864421 10488 logger.go:111] GET /api/v1beta1/operations/1: (36.899us) 202
I0709 23:23:57.933013 10488 logger.go:111] GET /api/v1beta1/operations/2: (34.499us) 202
E0709 23:23:59.678929 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
E0709 23:24:10.015214 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
E0709 23:24:10.924896 10488 pod_cache.go:86] Error synchronizing container: &url.Error{Op:""Get"", URL:""http://kube-minion-3.c.cloud-pge.internal:10250/podInfo?podID=9acb0442"", Err:(*net.OpError)(0xc2103132c0)}
I0709 23:24:17.907743 10488 logger.go:111] GET /api/v1beta1/operations/1: (54.777us) 202
I0709 23:24:17.933756 10488 logger.go:111] GET /api/v1beta1/operations/2: (33.736us) 202
I0709 23:24:18.007363 10488 logger.go:111] GET /api/v1beta1/services?labels=: (639.489us) 200
I0709 23:24:19.562447 10488 logger.go:111] GET /api/v1beta1/replicationControllers?labels=: (776.742us) 200
E0709 23:24:20.304152 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
E0709 23:24:20.775735 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
I0709 23:24:20.775842 10488 logger.go:111] GET /api/v1beta1/pods?labels=: (168.672866ms) 200
E0709 23:24:30.629913 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
I0709 23:24:37.934663 10488 logger.go:111] GET /api/v1beta1/operations/2: (48.649us) 202
I0709 23:24:37.950103 10488 logger.go:111] GET /api/v1beta1/operations/1: (38.2us) 202
E0709 23:24:40.928183 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
E0709 23:24:41.130443 10488 pod_cache.go:86] Error synchronizing container: &url.Error{Op:""Get"", URL:""http://kube-minion-3.c.cloud-pge.internal:10250/podInfo?podID=9acb0442"", Err:(*net.OpError)(0xc210293fc0)}
E0709 23:24:51.192263 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
I0709 23:24:57.935518 10488 logger.go:111] GET /api/v1beta1/operations/2: (36.987us) 202
I0709 23:24:57.995266 10488 logger.go:111] GET /api/v1beta1/operations/1: (43.676us) 202
E0709 23:25:01.440263 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
E0709 23:25:11.272386 10488 pod_cache.go:86] Error synchronizing container: &url.Error{Op:""Get"", URL:""http://kube-minion-3.c.cloud-pge.internal:10250/podInfo?podID=9acb0442"", Err:(*net.OpError)(0xc210339180)}
E0709 23:25:11.723786 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
I0709 23:25:17.936402 10488 logger.go:111] GET /api/v1beta1/operations/2: (36.953us) 202
I0709 23:25:18.042979 10488 logger.go:111] GET /api/v1beta1/operations/1: (46.98us) 202
E0709 23:25:22.031073 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
E0709 23:25:32.301266 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
I0709 23:25:37.937317 10488 logger.go:111] GET /api/v1beta1/operations/2: (37.115us) 202
I0709 23:25:38.085363 10488 logger.go:111] GET /api/v1beta1/operations/1: (36.737us) 202
E0709 23:25:41.431432 10488 pod_cache.go:86] Error synchronizing container: &url.Error{Op:""Get"", URL:""http://kube-minion-3.c.cloud-pge.internal:10250/podInfo?podID=9acb0442"", Err:(*net.OpError)(0xc210313f80)}
E0709 23:25:42.562650 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
E0709 23:25:52.855564 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
I0709 23:25:57.938119 10488 logger.go:111] GET /api/v1beta1/operations/2: (36.193us) 202
I0709 23:25:58.128160 10488 logger.go:111] GET /api/v1beta1/operations/1: (38.593us) 202
E0709 23:26:03.352062 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
E0709 23:26:11.603278 10488 pod_cache.go:86] Error synchronizing container: &url.Error{Op:""Get"", URL:""http://kube-minion-3.c.cloud-pge.internal:10250/podInfo?podID=9acb0442"", Err:(*net.OpError)(0xc21025e780)}
E0709 23:26:13.630040 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
I0709 23:26:17.939022 10488 logger.go:111] GET /api/v1beta1/operations/2: (37.38us) 202
I0709 23:26:18.173360 10488 logger.go:111] GET /api/v1beta1/operations/1: (37.163us) 202
E0709 23:26:23.885397 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
I0709 23:26:26.786686 10488 logger.go:111] POST /api/v1beta1/services?labels=: (249.584us) 202
E0709 23:26:34.201922 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
I0709 23:26:37.939971 10488 logger.go:111] GET /api/v1beta1/operations/2: (60.075us) 202
E0709 23:26:41.751316 10488 pod_cache.go:86] Error synchronizing container: &url.Error{Op:""Get"", URL:""http://kube-minion-3.c.cloud-pge.internal:10250/podInfo?podID=9acb0442"", Err:(*net.OpError)(0xc2102e3780)}
E0709 23:26:44.474538 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
E0709 23:26:54.723643 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
I0709 23:26:57.941113 10488 logger.go:111] GET /api/v1beta1/operations/2: (52.434us) 202
E0709 23:27:05.002365 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
E0709 23:27:11.938910 10488 pod_cache.go:86] Error synchronizing container: &url.Error{Op:""Get"", URL:""http://kube-minion-3.c.cloud-pge.internal:10250/podInfo?podID=9acb0442"", Err:(*net.OpError)(0xc2103ad100)}
E0709 23:27:15.281687 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
I0709 23:27:17.942073 10488 logger.go:111] GET /api/v1beta1/operations/2: (71.731us) 202
E0709 23:27:25.548852 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
E0709 23:27:35.879749 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
```
"
948	368	vmarmol	2014-07-10 01:08:30	CONTRIBUTOR	"cAdvisor is a Docker container so the Kubelet may be having issues starting
those.

On Wed, Jul 9, 2014 at 5:43 PM, Tony Worm notifications@github.com wrote:

> here's the head of the apiserver.log
> 
> E0709 23:13:06.129932 10488 pod_cache.go:80] Error synchronizing container list: &etcd.EtcdError{ErrorCode:501, Message:""All the given peers are not reachable"", Cause:""Tried to connect to each peer twice and failed"", Index:0x0}
> E0709 23:13:36.386628 10488 pod_cache.go:80] Error synchronizing container list: &etcd.EtcdError{ErrorCode:501, Message:""All the given peers are not reachable"", Cause:""Tried to connect to each peer twice and failed"", Index:0x0}
> E0709 23:14:06.624785 10488 pod_cache.go:80] Error synchronizing container list: &etcd.EtcdError{ErrorCode:501, Message:""All the given peers are not reachable"", Cause:""Tried to connect to each peer twice and failed"", Index:0x0}
> I0709 23:14:32.376591 10488 logger.go:111] GET /api/v1beta1/pods: (250.086536ms) 200
> I0709 23:22:18.029275 10488 logger.go:111] GET /api/v1beta1/minions?labels=: (361.476716ms) 200
> I0709 23:22:39.509469 10488 logger.go:111] GET /api/v1beta1/services?labels=: (531.306us) 200
> I0709 23:22:41.115735 10488 logger.go:111] GET /api/v1beta1/replicationControllers?labels=: (577.55us) 200
> I0709 23:22:42.279955 10488 logger.go:111] GET /api/v1beta1/pods?labels=: (132.530447ms) 200
> I0709 23:23:17.778127 10488 logger.go:111] POST /api/v1beta1/replicationControllers?labels=: (428.798us) 202
> I0709 23:23:17.930070 10488 logger.go:111] GET /api/v1beta1/pods?labels=name%3Dalgebra: (150.152397ms) 200
> I0709 23:23:17.931302 10488 logger.go:111] POST /api/v1beta1/pods: (351.445us) 202
> E0709 23:23:18.546175 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
> E0709 23:23:28.796310 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
> I0709 23:23:37.822511 10488 logger.go:111] GET /api/v1beta1/operations/1: (37.04us) 202
> I0709 23:23:37.932141 10488 logger.go:111] GET /api/v1beta1/operations/2: (37.74us) 202
> E0709 23:23:39.149032 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
> E0709 23:23:40.772891 10488 pod_cache.go:86] Error synchronizing container: &url.Error{Op:""Get"", URL:""http://kube-minion-3.c.cloud-pge.internal:10250/podInfo?podID=9acb0442"", Err:(_net.OpError)(0xc2102b5e40)}
> E0709 23:23:49.391719 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
> I0709 23:23:57.864421 10488 logger.go:111] GET /api/v1beta1/operations/1: (36.899us) 202
> I0709 23:23:57.933013 10488 logger.go:111] GET /api/v1beta1/operations/2: (34.499us) 202
> E0709 23:23:59.678929 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
> E0709 23:24:10.015214 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
> E0709 23:24:10.924896 10488 pod_cache.go:86] Error synchronizing container: &url.Error{Op:""Get"", URL:""http://kube-minion-3.c.cloud-pge.internal:10250/podInfo?podID=9acb0442"", Err:(_net.OpError)(0xc2103132c0)}
> I0709 23:24:17.907743 10488 logger.go:111] GET /api/v1beta1/operations/1: (54.777us) 202
> I0709 23:24:17.933756 10488 logger.go:111] GET /api/v1beta1/operations/2: (33.736us) 202
> I0709 23:24:18.007363 10488 logger.go:111] GET /api/v1beta1/services?labels=: (639.489us) 200
> I0709 23:24:19.562447 10488 logger.go:111] GET /api/v1beta1/replicationControllers?labels=: (776.742us) 200
> E0709 23:24:20.304152 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
> E0709 23:24:20.775735 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
> I0709 23:24:20.775842 10488 logger.go:111] GET /api/v1beta1/pods?labels=: (168.672866ms) 200
> E0709 23:24:30.629913 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
> I0709 23:24:37.934663 10488 logger.go:111] GET /api/v1beta1/operations/2: (48.649us) 202
> I0709 23:24:37.950103 10488 logger.go:111] GET /api/v1beta1/operations/1: (38.2us) 202
> E0709 23:24:40.928183 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
> E0709 23:24:41.130443 10488 pod_cache.go:86] Error synchronizing container: &url.Error{Op:""Get"", URL:""http://kube-minion-3.c.cloud-pge.internal:10250/podInfo?podID=9acb0442"", Err:(_net.OpError)(0xc210293fc0)}
> E0709 23:24:51.192263 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
> I0709 23:24:57.935518 10488 logger.go:111] GET /api/v1beta1/operations/2: (36.987us) 202
> I0709 23:24:57.995266 10488 logger.go:111] GET /api/v1beta1/operations/1: (43.676us) 202
> E0709 23:25:01.440263 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
> E0709 23:25:11.272386 10488 pod_cache.go:86] Error synchronizing container: &url.Error{Op:""Get"", URL:""http://kube-minion-3.c.cloud-pge.internal:10250/podInfo?podID=9acb0442"", Err:(_net.OpError)(0xc210339180)}
> E0709 23:25:11.723786 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
> I0709 23:25:17.936402 10488 logger.go:111] GET /api/v1beta1/operations/2: (36.953us) 202
> I0709 23:25:18.042979 10488 logger.go:111] GET /api/v1beta1/operations/1: (46.98us) 202
> E0709 23:25:22.031073 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
> E0709 23:25:32.301266 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
> I0709 23:25:37.937317 10488 logger.go:111] GET /api/v1beta1/operations/2: (37.115us) 202
> I0709 23:25:38.085363 10488 logger.go:111] GET /api/v1beta1/operations/1: (36.737us) 202
> E0709 23:25:41.431432 10488 pod_cache.go:86] Error synchronizing container: &url.Error{Op:""Get"", URL:""http://kube-minion-3.c.cloud-pge.internal:10250/podInfo?podID=9acb0442"", Err:(_net.OpError)(0xc210313f80)}
> E0709 23:25:42.562650 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
> E0709 23:25:52.855564 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
> I0709 23:25:57.938119 10488 logger.go:111] GET /api/v1beta1/operations/2: (36.193us) 202
> I0709 23:25:58.128160 10488 logger.go:111] GET /api/v1beta1/operations/1: (38.593us) 202
> E0709 23:26:03.352062 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
> E0709 23:26:11.603278 10488 pod_cache.go:86] Error synchronizing container: &url.Error{Op:""Get"", URL:""http://kube-minion-3.c.cloud-pge.internal:10250/podInfo?podID=9acb0442"", Err:(_net.OpError)(0xc21025e780)}
> E0709 23:26:13.630040 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
> I0709 23:26:17.939022 10488 logger.go:111] GET /api/v1beta1/operations/2: (37.38us) 202
> I0709 23:26:18.173360 10488 logger.go:111] GET /api/v1beta1/operations/1: (37.163us) 202
> E0709 23:26:23.885397 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
> I0709 23:26:26.786686 10488 logger.go:111] POST /api/v1beta1/services?labels=: (249.584us) 202
> E0709 23:26:34.201922 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
> I0709 23:26:37.939971 10488 logger.go:111] GET /api/v1beta1/operations/2: (60.075us) 202
> E0709 23:26:41.751316 10488 pod_cache.go:86] Error synchronizing container: &url.Error{Op:""Get"", URL:""http://kube-minion-3.c.cloud-pge.internal:10250/podInfo?podID=9acb0442"", Err:(_net.OpError)(0xc2102e3780)}
> E0709 23:26:44.474538 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
> E0709 23:26:54.723643 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
> I0709 23:26:57.941113 10488 logger.go:111] GET /api/v1beta1/operations/2: (52.434us) 202
> E0709 23:27:05.002365 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
> E0709 23:27:11.938910 10488 pod_cache.go:86] Error synchronizing container: &url.Error{Op:""Get"", URL:""http://kube-minion-3.c.cloud-pge.internal:10250/podInfo?podID=9acb0442"", Err:(_net.OpError)(0xc2103ad100)}
> E0709 23:27:15.281687 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/368#issuecomment-48553228
> .
"
949	389	thockin	2014-07-10 05:18:14	MEMBER	"LGTM
"
950	389	thockin	2014-07-10 05:21:31	MEMBER	"Travis failed.  panic: runtime error: invalid memory address or nil pointer dereference
"
951	389	brendandburns	2014-07-10 05:24:57	CONTRIBUTOR	"looking...

--brendan

On Wed, Jul 9, 2014 at 10:21 PM, Tim Hockin notifications@github.com
wrote:

> Travis failed. panic: runtime error: invalid memory address or nil pointer
> dereference
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/389#issuecomment-48566379
> .
"
952	389	bgrant0607	2014-07-10 05:41:44	MEMBER	"Thanks. Would be useful to reference the issue:
https://help.github.com/articles/closing-issues-via-commit-messages

For example:
Fixes #385 
"
953	382	raphael	2014-07-10 06:12:12	CONTRIBUTOR	"Just signed it (as an individual contributor).
"
954	382	brendandburns	2014-07-10 06:14:15	CONTRIBUTOR	"Many thanks!  Merging.
"
955	374	brendandburns	2014-07-10 06:15:03	CONTRIBUTOR	"Makes sense to me, thanks for the PR!

--brendan
"
956	140	smarterclayton	2014-07-10 13:15:24	CONTRIBUTOR	"> We could override the container entrypoint with the pre-start hook command and then use ""runin"" to execute the real entrypoint, but the container's status, wait, restart, etc. would be broken. Creating a new container image that included the pre-start command, actual entrypoint, and post-termination hook might work, but we'd need to carefully propagate arguments, signals, exit status, etc.

We tried make this work, but it felt wrong on all sorts of levels and ultimately we felt there were two cases we wanted to handle:
- hooks that either need the container context (and as such executing outside the process namespace would be pointless), or if interrupted by container shutdown would not be internally inconsistent.  Pre-termination is a good example
- hooks that should be outside of a container, because they need to continue to run even if a container fails.  Deploy across multiple containers is a good example, or post-termination.

I don't think you need to subclass the entrypoint in the image - for the former, you could use ""docker exec in"" (add process to namespace) and for the latter, you can use restart-once containers (outside or inside a pod).

There's some crossover here between ""event hooks"" and ""intent/application hooks"" - reacting to changes to the infrastructure, vs reacting to user intent across multiple pods.  The ""intent hooks"" tend to be things that I think of as wanting the auth/orchestration parts of the downward-facing API (I want to wait for this label query to reach X instances, and then update another replication controller to go to Y), as well as being more like regular jobs in their own right.
"
957	345	smarterclayton	2014-07-10 15:08:37	CONTRIBUTOR	"For the purposes of this pull, I think the change here fits within the agreed direction and is merely an incremental step towards it (accordingly I marked ContainerManifest.ID as deprecated).  Disagreement?  If no could use a final rereview and merge.
"
958	390	thockin	2014-07-10 15:12:16	MEMBER	"First, the apiserver is supposed to ensure that HostPort conflicts do not
happen among pods.  We recognize that this is sort of awful, and want to
find a better answer :)

Second, I agree that the current Ports arrangement is not ideal.  I have an
idea that I want to flesh out to make it ""better"" (in my opinion).

Keep in mind that you really only need to specify a Ports entry if you want
a HostPort.  You only want a HostPort in GCE if you want an external IP to
be able to access it.  What we have done with networking is clever, but
maybe too much so.

My feeling is that this distinction is not clear, and instead everyone will
list all their ports when they do not need to.  It should be possible to
list your ports and NOT get a HostPort at all, which I think is what you
really want.

I am not against doing a random HostPort, but i want to think about it
carefully.

On Thu, Jul 10, 2014 at 12:16 AM, Yuki Yugui Sonoda <
notifications@github.com> wrote:

> IIUC, host port numbers in each pods are not important for replicated
> services in Kubernetes.
> From the perspective of orchestration, the important thing is port of
> service, but not port of pod.
> 
> Also manual assignment of host port can be troublesome because spawn of
> container just fails if the container manifest specified a port which is
> already taken by other containers or Kubernetes daemons.
> 
> So I propose the following enhancement.
> - Keep tracking taken ports in etcd
> - Allow omitting host port in container manifest
>   - If omitted, Kubelet automatically choose an available host port
>     for each exposed container port.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/390.
"
959	188	lexlapax	2014-07-10 15:17:36	NONE	"Some feedback here on the ip per pod/container model.
-IPV6 is further along in larger service providers and enterprise datacenters than we think.
-larger enterprises already use datacenter wide private ip spaces with datacenter (or row/rack/floor) wide NAT
I think rather than changing/removing the model of network addressable containers / pods, there should be other options added to allow operating in openstack or other similar clouds..
Agree with clayton, putting pods inside VMs is probably not the way to think about this.
"
960	391	thockin	2014-07-10 15:18:25	MEMBER	"This is a dangerous opening.  Can you detail why people should be able to
use this?  I'd rather pursue teaching Docker how to enable various
privileges in a more granular fashion.

If anything, we should be convincing people to run containers with LESS
privs (i.e. not root to start).

On Thu, Jul 10, 2014 at 12:28 AM, Yuki Yugui Sonoda <
notifications@github.com> wrote:

> Sometimes containers need to run with privileged mode.
> 
> https://docs.docker.com/reference/run/#runtime-privilege-and-lxc-configuration
> 
> Container manifest schema should support privileged flag so that users can
> deploy privileged container.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/391.
"
961	378	proppy	2014-07-10 16:00:56	CONTRIBUTOR	"> although I don't necessarily see the value

@brendanburns, I had a brief chat during Google I/O w/ @crawshaw from the Go team.

He gave a talk about managing dependencies for golang project.

Maybe he can expand on the rational of using godep, versus vendoring + rewriting imports versus hand crafted script.

What I remember from our discussion was that:
- using godep was useful if you wanted to distribute a stable binary of an executable (i.e: kubelet)
- using vendoring + rewriting imports was useful when `go get`ability is important: i.e, for libraries that other project might depends on.
"
962	393	brendandburns	2014-07-10 16:19:36	CONTRIBUTOR	"Thanks!
"
963	378	crawshaw	2014-07-10 16:39:51	NONE	"I haven't looked too closely at this pull request, but godep can certainly do what you need it to do. My personal preference is to write my own little program for dependency management, because I prefer code to configuration. But there are many reasonable options here, and godep is one of them.
"
964	389	brendandburns	2014-07-10 17:55:44	CONTRIBUTOR	"Comment addressed, please take another look.
"
965	389	smarterclayton	2014-07-10 18:09:05	CONTRIBUTOR	"LGTM
"
966	378	monnand	2014-07-10 18:12:07	CONTRIBUTOR	"Rebased.

Thank you, @crawshaw. I checked most available tools to do dependency management and it seems that godep requires least change to our current scripts.

The main reason I want to switch to godep is the ability to add/update dependencies automatically.
godep could tell us the dependencies by analyzing the source code, remove all unnecessary meta data (like .git). Our current approach requires us to manually add a new line to a file for each new dependency. It may not be a big deal but break the workflow.
"
967	395	brendandburns	2014-07-10 18:44:58	CONTRIBUTOR	"LGTM.  I'll wait for others, and then if there are no comments merge this afternoon.
"
968	362	brendandburns	2014-07-10 19:19:44	CONTRIBUTOR	"Cloud SDK isn't sure what could cause this, it looks like a hiccup in the console.  Closing this for now.  Please re-open if it re-occurs.

Thanks!
--brendan
"
969	396	brendandburns	2014-07-10 19:27:12	CONTRIBUTOR	"Fixes #298 
"
970	397	brendandburns	2014-07-10 19:45:34	CONTRIBUTOR	"Fixes #248 
"
971	345	thockin	2014-07-10 19:55:49	MEMBER	"LGTM - resolve conflicts or whatever is causing ""We can’t automatically merge this pull request""
"
972	378	monnand	2014-07-10 20:19:08	CONTRIBUTOR	"@brendandburns Passed CI. PTAL.
"
973	399	brendandburns	2014-07-10 21:25:57	CONTRIBUTOR	"Thanks for the PR!  Have you folks signed our CLA?  Details in [CONTRIB.md](https://github.com/GoogleCloudPlatform/kubernetes/blob/master/CONTRIB.md).

Thanks!
--brendan
"
974	399	jeffmendoza	2014-07-10 21:40:06	MEMBER	"Yes, I did about a week ago.
"
975	386	thockin	2014-07-10 23:28:01	MEMBER	"As much as I want to keep this simple, Clayton's argument of using a non
k8s native image in k8s by application of clever env vars is compelling.

But what scope?  Env vars only?  Env vars and command lines?
On Jul 9, 2014 5:08 PM, ""Clayton Coleman"" notifications@github.com wrote:

> That's true, although one strong goal in docker has been to try and offer
> via libchan/libswarm a ""Docker"" abstraction where on multiple platforms
> images can invoke consistent actions that affect the platform. That applies
> to more generic operations, whereas k8s specific actions will always
> require some downwards API. The abstraction is a long term play that
> requires active participation by a number of stakeholders and also
> implemented, demonstrated use cases, so it's not an argument for avoiding
> trying.
> 
> I don't think the downward API is bad, but some of the use cases (unique
> id for an instance, what ip do I have) might benefit from looser coupling
> or by trying to find the most lowest common denominator expression. In the
> end image authors can always ignore those settings, but you do want image
> users to have the tools to bend a recaltricant image to their will.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/386#issuecomment-48551080
> .
"
976	386	bgrant0607	2014-07-10 23:35:59	MEMBER	"I find it compelling, too. Internally, we have thousands of uses of the downward API, and I agree it introduces unnecessary coupling. OTOH, I'd like to think more about the configuration and parameterization story before introducing substitution.

As opposed to the k8s-specific mechanism I proposed, perhaps we could start by thinking about what a typical application would need to know about itself and how that information could be provided in the least entangled manner.
"
977	403	thockin	2014-07-10 23:58:55	MEMBER	"I find ""Interface"" to be a awkward given the meaning of the word in Go.  Would ""client.Client"" be better?  If someone were using this in a different project they might rename the package to ""k8s"" or something, yielding k8s.Client - which seems better than k8s.Interface
"
978	403	dsymonds	2014-07-11 00:00:26	CONTRIBUTOR	"Renaming both the package and interface sounds even better. `kubernetes.Client` reads well.
"
979	403	proppy	2014-07-11 00:01:12	CONTRIBUTOR	"There is already a `type Client` struct
https://github.com/nf/kubernetes/blob/cleanup/pkg/client/client.go#L70

So maybe `s/Client/KubeClient` for the concreate type, and `s/ClientInterface/Client` for the interface.
"
980	403	nf	2014-07-11 00:10:16	CONTRIBUTOR	"There is already a client.Client; that's the client implementation. That is fine.

`foo.Interface` is an established idiom. See `sort.Interface`, `hash.Interface` from the standard library.

If we want to rename the package we can do it in a separate PR. I am happy to share my rationale for each change, but I would like to avoid lengthy debate on each pull request as I'm likely to make hundreds of them. The faster I can move, the less likely I'll get bored.
"
981	403	thockin	2014-07-11 00:10:29	MEMBER	"Is there a standard pattern for this?  In Google C++ we would call it
Client and ClientImpl, but I don't know if all the code that uses the
existing Client uses it exclusively by the interface or not.

I.e. in C++ I would say

class Client {
  virtual int Foo() = 0;
}

class ClientImpl : public Client {
  int Foo() override;
}

Client New() {
  return new ClientImpl();
}

But I see that this code returns a Client, not ClientInterface.  If it can
return ClientInterface, it maps better to Foo and FooImpl.

On Thu, Jul 10, 2014 at 5:01 PM, Johan Euphrosine notifications@github.com
wrote:

> There is already a type Client struct
> https://github.com/nf/kubernetes/blob/cleanup/pkg/client/client.go#L70
> 
> So maybe s/Client/KubeClient for the concreate type, and
> s/ClientInterface/Client for the interface.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/403#issuecomment-48680041
> .
"
982	403	nf	2014-07-11 00:11:33	CONTRIBUTOR	"This PR brings the code in line with the standard Go idiom. `client.Client` for the concrete type, `client.Interface` for the interface.
"
983	403	thockin	2014-07-11 00:11:51	MEMBER	"if pkg.Interface is a common pattern, so be it.
"
984	405	dsymonds	2014-07-11 00:39:43	CONTRIBUTOR	"Definitely not.

Please read the README at https://github.com/golang/lint carefully to understand the golint philosophy. In short, golint has false positives, where gofmt does not. The same applies to govet.

There are a number of editor plugins that individuals can use as part of their own workflow, but golint and govet both require human judgment and are not suitable for automation.
"
985	402	monnand	2014-07-11 01:10:24	CONTRIBUTOR	"@nf Actually, we discussed about this with @proppy, @brendanburns and @jbeda before this project get open source. I strongly support to make it go get able. But @jbeda a and @brendandburns have some concerns. If I remember it correctly, here are some requirement they have:
- vendoring all dependencies in a subdirectory.
- Import paths should not be changed. (this roles out tools like goven)

(I cannot find the discussion now, so there may be other requirements.)

Based on the discussion, I proposed a PR #378 to use [godep](http://github.com/tools/godep) so that we could have least changes to existing scripts.

I would like to hear your solutions.
"
986	404	nf	2014-07-11 01:27:19	CONTRIBUTOR	"LGTM
"
987	391	bgrant0607	2014-07-11 01:32:10	MEMBER	"Docker is adding capability whitelisting/blacklisting as we speak.

On Thu, Jul 10, 2014 at 8:18 AM, Tim Hockin notifications@github.com
wrote:

> This is a dangerous opening. Can you detail why people should be able to
> use this? I'd rather pursue teaching Docker how to enable various
> privileges in a more granular fashion.
> 
> If anything, we should be convincing people to run containers with LESS
> privs (i.e. not root to start).
> 
> On Thu, Jul 10, 2014 at 12:28 AM, Yuki Yugui Sonoda <
> notifications@github.com> wrote:
> 
> > Sometimes containers need to run with privileged mode.
> > 
> > https://docs.docker.com/reference/run/#runtime-privilege-and-lxc-configuration
> > 
> > Container manifest schema should support privileged flag so that users
> > can
> > deploy privileged container.
> > 
> > ## 
> > 
> > Reply to this email directly or view it on GitHub
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/391.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/391#issuecomment-48618877
> .
"
988	390	bgrant0607	2014-07-11 01:46:53	MEMBER	"I don't understand the motivation for automatically assigned host ports. The motivation for requesting one is so that it can be opened in firewall rules and connected to via external clients, through frontend load balancing, etc. This is not possible with automatically assigned ports.
"
989	398	bgrant0607	2014-07-11 02:03:46	MEMBER	"For info on Docker security generally:
https://docs.docker.com/articles/security/

There are efforts underway to make it easier to use Linux security mechanisms with Docker, such as restricted capabilities, AppArmor, SELinux, non-root users, etc. 

Are you asking how to access these features through Kubernetes?

That said, our position is that Docker doesn't yet provide a sufficient security boundary:
https://developers.google.com/compute/docs/security-bulletins?_ga=1.91563352.392654364.1401901573
"
990	392	bgrant0607	2014-07-11 02:14:58	MEMBER	"You could use labels to version your containers. If you have a new version to deploy, you can create a new replicationController with a new version label, but with labels that match your service's label selector, and the new containers will start to receive traffic automatically. Once the new version has been confirmed to operate correctly, you could grow the number of replicas to the full service size and shrink the previous replicationController's count down to zero, then delete it. I agree that a bit of tooling could make this easier, but it's possible to do now. We may have demoed something similar at Dockercon.
"
991	391	yugui	2014-07-11 03:29:13	CONTRIBUTOR	"I close this issue because I discussed with thockin, and I agreed that my original motivation of adding privileged container can be covered by another approach.  On the other hand, container should be run with less priv.
So I agree that allowing privileged container is a wrong direction.
"
992	391	thockin	2014-07-11 03:32:00	MEMBER	"If we can't find a better way to do this, we can revisit this idea.

On Thu, Jul 10, 2014 at 8:29 PM, Yuki Yugui Sonoda <notifications@github.com

> wrote:
> 
> I close this issue because I discussed with thockin, and I agreed that my
> original motivation of adding privileged container can be covered by
> another approach. On the other hand, container should be run with less priv.
> So I agree that allowing privileged container is a wrong direction.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/391#issuecomment-48690921
> .
"
993	402	brendandburns	2014-07-11 03:43:52	CONTRIBUTOR	"Yeah, I think that the original discussion of this issue got eaten in the transition from our pre-release staging git repo to the released public repo.  I'll try to re-cap:

For reasons of clarity about OSS licensing (and general organization) we prefer to keep dependent libraries in a separate directory (third_party/...).  That way its very clear which LICENSE files apply to which directories, and it is also very clear what parts of the codebase our our code, and which parts are library code.

Additionally, modifying all of the OSS libraries we use causes a maintenance headache as we need to re-adjust paths every single time that we move versions of libraries to a new version.  Finally, modifying the library code to have new import paths can have non-trivial OSS license implications that we'd rather not mess around with.

I checked camlistore and it appears to not work with a vanilla go get:

``` sh
bburns$ go get github.com/bradfitz/camlistore
package github.com/bradfitz/camlistore
    imports github.com/bradfitz/camlistore
    imports github.com/bradfitz/camlistore: no buildable Go source files in /Users/bburns/git/tmp/src/github.com/bradfitz/camlistore
```

Our basic methodology for building is actually quite similar to camlistore's as documented [here](https://github.com/bradfitz/camlistore/blob/master/BUILDING)

``` sh
git clone https://github.com/GoogleCloudPlatform/kubernetes.git
./hack/build-go.sh
```

If its preferable to turn build-go.sh into a go program similar to Brad's make.go, you're welcome to send the PR for consideration. 

If there is a way to achieve go get-ability and maintain the goals that we set out above, obviously it would be great to hear about it.

Thanks!
--brendan
"
994	401	brendandburns	2014-07-11 03:55:10	CONTRIBUTOR	"Duplicate of #402 
"
995	398	brendandburns	2014-07-11 03:57:36	CONTRIBUTOR	"Closing this since I think @bgrant0607's response addresses the issue, and anything remaining is likely a broader issue for Docker/libcontainer/... rather than k8s.
"
996	392	brendandburns	2014-07-11 03:58:57	CONTRIBUTOR	"We also have the facility to do rolling updates, see the example here:

https://github.com/GoogleCloudPlatform/kubernetes/tree/master/examples/update-demo

If you combine a rolling update of a replicated service with a health checking load balancer, then you have a pretty good solution to this problem.
"
997	402	nf	2014-07-11 04:06:12	CONTRIBUTOR	"@brendanburns Thanks for the reply.

Fortunately there's precedent for all of this. Camlistore is a Google project and we already manage all this stuff easily and complying with OSS licenses.

The Camlistore approach to external dependencies:
- put them under `third_party` (with a suitable [README](https://camlistore.googlesource.com/camlistore/+/master/third_party/README)),
- rewrite import paths with a [trivial script](https://camlistore.googlesource.com/camlistore/+/master/third_party/rewrite-imports.sh),
- update them by copying over the latest version of the repo and running the above script.

In my extensive experience with OSS releasing at Google there are no non-trivial licensing issues surrounding these kinds of mechanical edits. If you believe there are, I am happy to follow this up with the Open Source team at Google on your behalf. 

Camlistore _is_ go-gettable. The error you see is because you're not specifying a go package path, just the repo base path. Try something like this:

```
$ go get camlistore.org/server/camlistored
```

(Note that `github.com/bradfitz/camlistore` is the old deprecated repo path.)

I don't object at all to special build scripts that updates the version hash and so on. This issue doesn't ask that ""go get"" be the default build mechanism, merely that the codebase should work with ""go get"" by default.

Note that installation with ""go get"" isn't the only benefit of this cleanup; having a well-organised repository permits static analysis tools like godoc and goimports to analyse your codebase without setting a special GOPATH.

I hope this puts your concerns at ease. What do you think?
"
998	401	nf	2014-07-11 04:06:36	CONTRIBUTOR	"It's not a dupe. The build error in pkg/kubelet is real, is it not?
"
999	401	brendandburns	2014-07-11 04:15:56	CONTRIBUTOR	"Pretty sure not, Travis is clean @ head:

https://travis-ci.org/GoogleCloudPlatform/kubernetes

And I just tried clean branch @ upstream/master and it builds.
"
1000	401	monnand	2014-07-11 04:20:39	CONTRIBUTOR	"I think that's because the cAdvisor and libcontainer code in third_party directory is old. cAdvisor changed its client API by adding one more parameter to its GetContainerInfo() function; libcontainer renamed `Cgroups` to `Config` if I remember it correctly. Travis works because our build script use our own code in third_party directory.
"
1001	401	nf	2014-07-11 04:22:33	CONTRIBUTOR	"Ah, I see. This is indeed a dupe of #402, then.
"
1002	401	brendandburns	2014-07-11 04:23:08	CONTRIBUTOR	"Ah, yes, you must have used go get to install the cAdvisor code, rather
than the version we ship in third_party/

This is why we check everything into third_party/ so we can build against
reliable versions.

--brendan

On Thu, Jul 10, 2014 at 9:20 PM, monnand notifications@github.com wrote:

> I think that's because the cAdvisor and libcontainer code in third_party
> directory is old. cAdvisor changed its client API by adding one more
> parameter to its GetContainerInfo() function; libcontainer renamed Cgroups
> to Config if I remember it correctly. Travis works because our build
> script use our own code in third_party directory.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/401#issuecomment-48692994
> .
"
1003	401	nf	2014-07-11 04:25:27	CONTRIBUTOR	"Yep. This is also why your import paths should be rewritten to use the packages in third_party. :-)
"
1004	402	brendandburns	2014-07-11 04:34:17	CONTRIBUTOR	"To make sure I understand this correctly, let me clarify:

what you are proposing is that a user would:

`go get github.com/GoogleCloudPlatform/kubernetes/cmd/apiserver`

to install the source

and then run our custom build script (in go/bash/whatever) to do the build?

I think this is a little tricky because there's lots of stuff in the git repo that is required to actually turn up a kubernetes cluster (e.g. the salt stack scripts, the cluster turn-up scripts that create GCE instances, ...) that a user needs to successfully use Kubernetes.

So simply downloading the go source code and building it is a pretty minor part of the process.  (and indeed if the user is using a Mac or Windows, building it natively won't do them any good, since they need to be compiled for Linux)

I don't really object to making `go get` functional in the workflow above, its just that I'm not really sure it will do anything useful for the end user.
"
1005	402	monnand	2014-07-11 04:46:08	CONTRIBUTOR	"@brendanburns To me, for a real end user, i.e. someone who does not want to change any code in k8s, the best way of using k8s would be downloading binaries and deploying the system only using those binaries. In this case, an end user would never need to install any go-related tool (go compiler, linker, etc.)  K8s itself is written in pure Go, which makes every executable statically linked with their dependencies. This gives us the opportunity to distribute binaries directly to end users. 

(I personally think it may be better to replace most, if not all, shell scripts with go programs. In that case, we only require users to have ssh and docker on VMs and our go programs could upload and start kubelet, master, cAdvisor according to some config file using ssh --- there's an ssh package available in Go. But this would be a big change on our existing code base and I don't think we could accomplish this in near future.)

With that being said, k8s' users would be:
- end users, who could use binaries directly.
- (potential) developers/contributors: They should familiar with Go's environment already and a code base following Go's convention would make their life easier.
"
1006	402	nf	2014-07-11 04:49:58	CONTRIBUTOR	"@brendanburns Not exactly. I want `go get`  to correctly download _and_ build the k8s commands and packages, including their dependencies. The `go` tool is only for building and working with Go source code, so of course it shouldn't be considered the sole means of turning up a k8s cluster. And it's fine that there be extra steps/scripts (that may modify the source and run `go get` (or `go install`) again) 

We have already seen the negative effects of having a broken `go get` with issue #401. Me, a potential contributor, did the reasonable thing (`go get`) and saw a broken build because of bad dependencies. Issues like this will repeatedly take valuable time from users, potential contributors, and the k8s maintainers.

And, as I mentioned in my earlier comment, having a nice clean package structure enables other tools to work well with your code base.
"
1007	402	nf	2014-07-11 04:51:52	CONTRIBUTOR	"Probably also worth mentioning: if you leave things organised as they are, you leave the door open for people to _successfully_ build your binaries with skewed dependencies (when the stars align and the skew doesn't break the compile step). That's a real problem for end users that will be very hard to track down.
"
1008	402	brendandburns	2014-07-11 05:18:49	CONTRIBUTOR	"@monnand agree 100% we are working towards having a completely binary release where the user never builds anything really soon.
"
1009	402	dsymonds	2014-07-11 05:30:14	CONTRIBUTOR	"As far as I can see, the licensing and import path issues are resolved, and there aren't any outstanding issues with following the Camlistore approach of vendoring + updating import paths. There's tools like `godep` or `rx` that can help with automating that.

The major issue that remains is, perhaps, a difference on what's being discussed. I see the kubernetes codebase has two main aspects:
- a collection of Go packages, and
- a collection of Go binaries

For the second, it does indeed sound like the `go` tool is going to not solve every step of building the entire system, and using a custom build script is probably the right approach. Under the hood, that build script could mostly be a bunch of `go build` commands to build the Go binaries, but then also do whatever it needs with the other non-Go stuff (GCE turnup scripts, salt stack scripts, etc.).

For the first (the Go packages), however, there's no reason why they couldn't be made `go get`-able. Furthermore, if they were then your build script would be simpler because it could use the `go` tool more directly. But perhaps most importantly, it would permit Go packages elsewhere to use the kubernetes Go packages properly. This would enable other people to build tooling and more advanced libraries on top of kubernetes, just as people build Go tools on top of the standard library or stuff in `code.google.com/p/go.tools`.
"
1010	378	brendandburns	2014-07-11 05:45:18	CONTRIBUTOR	"Added a couple of comments, this seems reasonable to me assuming the docs get clarified.

Once the docs get clarified, I'd like to try out the process myself to validate that it works.

Thanks!
"
1011	406	roberthbailey	2014-07-11 06:09:40	MEMBER	"Thanks for the cleanups. :)
"
1012	390	yugui	2014-07-11 07:56:58	CONTRIBUTOR	"On Fri Jul 11 2014 at 12:12:24 AM, Tim Hockin notifications@github.com wrote:

> First, the apiserver is supposed to ensure that HostPort conflicts do not
> happen among pods. We recognize that this is sort of awful, and want to
> find a better answer :)
> 
> Second, I agree that the current Ports arrangement is not ideal. I have an
> idea that I want to flesh out to make it ""better"" (in my opinion).
> 
> Keep in mind that you really only need to specify a Ports entry if you want
> a HostPort. You only want a HostPort in GCE if you want an external IP to
> be able to access it. What we have done with networking is clever, but
> maybe too much so.
> 
> My feeling is that this distinction is not clear, and instead everyone will
> list all their ports when they do not need to. It should be possible to
> list your ports and NOT get a HostPort at all, which I think is what you
> really want.

I'm not sure if I get your point correctly, but I would be better if I can expose services to external IP without having HostPort at all.

What I actually want is the followings.
- I have a system which consists of several services.  Some of them are backends, which don't need to be accessible from external IPs.  Others are exposed to external.
- IIUC, exposed services need to have HostPort so that external IP can access to them.
  - But I don't want to _carefully_ manage my list of HostPorts so that ports don't conflict to each other.  Why doesn't Kuberlet do for me?
  - I also want to assign an available HostPort without digging into Kubernetes internal because it might change and can depend on cloudprovider implementation.
    - e.g. HostPort 5000 is not available to pods because Kubelet uses it
    - e.g. HostPort 22 is not available to pods because sshd in GCE uses it.

On Fri Jul 11 2014 at 10:47:02 AM, bgrant0607 notifications@github.com wrote:

> I don't understand the motivation for automatically assigned host ports. The motivation for requesting one is so that it can be opened in firewall rules and connected to via external clients, through frontend load balancing, etc. This is not possible with automatically assigned ports.

Automatic assignment I imagine does not mean random assignment per kubelet.
I am thinking of automatic but consistent assignment among kubelets.
I imagine a mechanism like:
- Keep tracking ports used in kubelet nodes. Record the current state in etcd.
- When kubemaster sends requests to kubelets, it allocates an available port according to the record in etcd.  All pods for the replication consistently uses the port allocated by kubemaster.
"
1013	188	MalteJ	2014-07-11 10:45:50	NONE	"I would like to get an IPv6 per pod.
Yes, it is true: lots of IaaS do not support IPv6 (digitalocean does). But if you rent a dedicated Linux server most of the times you get an IPv6 subnet for free (e.g. /64). So it would be nice to get an IPv6 or even a subnet (/112 or something) per pod and one (/128) IPv6 per container.
The next feature would be a nice firewalling/security group solution to restrict unallowed internet access to the pods or even restrict inter-pod access - e.g. for different deployment stages (dev, test, prod etc.).
"
1014	408	nyaxt	2014-07-11 12:06:30	CONTRIBUTOR	"Sorry. I committed with wrong email address. I'll close this one.
"
1015	407	yugui	2014-07-11 13:41:29	CONTRIBUTOR	"Rebased onto #406 because it touches the same files as this PR.
Resolved conflict with fixes by @rakyll.
"
1016	108	kelseyhightower	2014-07-11 15:25:57	CONTRIBUTOR	"I'm going a bit against the crowd here, but I prefer each service as a separate binary mainly to help keep each one focused. If the decision was to move to a single binary then subcommands would be ideal:

```
kube apiserver
kube controller-manager
kube kubelet
kube proxy
```

But I think the UX of subcommands for completely different services to be pretty bad. It works well for users with experience, but will most likely confuse new users. Finally, again non-technical, but I think unnecessary coupling between components starts to happen once you have a ""single"" binary. 
"
1017	108	thockin	2014-07-11 15:58:10	MEMBER	"I'll agree with this last comment - I don't find 50 MB to be particularly damning - but maybe I have been in C++ land for too long.  At the same time, I also don't find the idea of jamming them all together into one binary to make a lot of sense.  It's sort of early to be apologizing for the size of Go binaries isn't it?
"
1018	410	brendandburns	2014-07-11 16:33:46	CONTRIBUTOR	"Thanks for the PR!  Can you sign our CLA as described in CONTRIB.md?

Thanks!
--brendan
"
1019	412	brendandburns	2014-07-11 16:35:00	CONTRIBUTOR	"Hello, I merged this accidentally without getting a CLA signed from you.  Can you go ahead and sign our CLA, the instructions are in [CONTRIB.md](https://github.com/GoogleCloudPlatform/kubernetes/blob/master/CONTRIB.md)

Many thanks!
--brendan
"
1020	394	brendandburns	2014-07-11 16:38:33	CONTRIBUTOR	"Friendly ping on this one @thockin @smarterclayton 
"
1021	347	derekwaynecarr	2014-07-11 16:38:49	MEMBER	"Is this the same thing as this PR: https://github.com/GoogleCloudPlatform/kubernetes/pull/379

Should this issue be closed?
"
1022	397	brendandburns	2014-07-11 16:38:50	CONTRIBUTOR	"Friendly friday ping @thockin @smarterclayton 
"
1023	347	brendandburns	2014-07-11 16:40:16	CONTRIBUTOR	"Ah, yes.  Thanks, that issue was closed by that PR.
"
1024	388	brendandburns	2014-07-11 17:16:54	CONTRIBUTOR	"Rebased.
"
1025	412	ggtools	2014-07-11 17:33:06	CONTRIBUTOR	"CLA Signed.
"
1026	378	monnand	2014-07-11 18:01:44	CONTRIBUTOR	"@brendandburns I made the following changes:
- Changed the version.go and version/template.go files, so that we could run `godep go build ./...` to build all commands.
- Changed README.md to specify the k8s' directory in GOPATH.

This PR would make k8s build without our build script, so it partly solved #402. I say partly because import paths are not changed, so `go get` will not work if our dependencies changed their API.
"
1027	397	smarterclayton	2014-07-11 18:04:01	CONTRIBUTOR	"LGTM
"
1028	394	smarterclayton	2014-07-11 18:07:53	CONTRIBUTOR	"LGTM
"
1029	402	monnand	2014-07-11 18:24:43	CONTRIBUTOR	"@dsymonds I think [godep](http://github.com/tools/godep) will not change import paths. It wraps the go command and add a directory to existing GOPATH. There's another tool named [godeps](https://github.com/dre1080/godeps), which changes import paths using [goven](https://github.com/kr/goven). (I know, this is confusing.)

I am working on #378 which uses [godep](https://github.com/tools/godep) to manage dependencies. However, it will not solve the `go get` problem, because import paths are not changed. It does make adding/updating dependencies easier.

Probably @nf @dsymonds would have some better suggestions?
"
1030	414	smarterclayton	2014-07-11 18:26:04	CONTRIBUTOR	"LGTM
"
1031	345	smarterclayton	2014-07-11 18:26:33	CONTRIBUTOR	"@thockin ptal
"
1032	414	proppy	2014-07-11 18:26:36	CONTRIBUTOR	"+1
"
1033	390	bgrant0607	2014-07-11 18:26:54	MEMBER	"I think we need a more concrete example.
1. Frontend service running on host port 8080, on hosts assigned externally visible IP addresses, opened in virtual network firewall, targeted by L3 load balancers, with DNS set up to hit the L3 LB virtual IP. We put this service into a container, which is the only container running in its pod. Kubernetes schedules just one of these pods per host. Yes, you need to avoid using ports that conflict with system services, like sshd. We could give Kubelet its own address in order to avoid polluting the host port space. We should at least clearly document which ports are in use. Clients of this frontend service, such as web browsers running on people's laptops, aren't going to be using etcd, so dynamically assigned ports are a non-starter. Eventually we'd like to not need to use the host's primary network in this scenario, but we don't have a way of doing that yet.
2. Multiple backend services. Again, one container per pod. They can all use port 5000 (or even 22) if they want. They shouldn't request host ports. They'll be scheduled to any minion in the cluster with no conflicts. Use etcd, Consul, Eureka, DNS, or whatever other naming/discovery mechanism you like. Or, if they are load-balanced services, use our service abstraction.

Issue #188 describes the problems associated with dynamic port assignment.
"
1034	414	thockin	2014-07-11 18:28:41	MEMBER	"Clayton, I believe you have commit access now, right?  Go for it if you
consider this to be simple enough for a single LGTM :)

On Fri, Jul 11, 2014 at 11:26 AM, Johan Euphrosine <notifications@github.com

> wrote:
> 
> +1
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/414#issuecomment-48765768
> .
"
1035	414	monnand	2014-07-11 18:50:49	CONTRIBUTOR	"LGTM :)
"
1036	390	thockin	2014-07-11 18:55:20	MEMBER	"Hi Yuki,

Some comments in-line

On Fri, Jul 11, 2014 at 12:57 AM, Yuki Yugui Sonoda
notifications@github.com wrote:

> I'm not sure if I get your point correctly, but I would be better if I can expose services to external IP without having HostPort at all.

In order for an external IP in GCE to find your service, you have to
have a HostPort mapping.  If you don't, then we don't bind you to the
main interface of the VM.

> What I actually want is the followings.
> 
> I have a system which consists of several services. Some of them are backends, which don't need to be accessible from external IPs. Others are exposed to external.
> IIUC, exposed services need to have HostPort so that external IP can access to them.
> 
> But I don't want to carefully manage my list of HostPorts so that ports don't conflict to each other. Why doesn't Kuberlet do for me?

We can assign you a port, but then nobody knows how to find it.  You
need to open the GCE firewall for that port.  You need to tell your
customers what port number it is.  We can put it into etcd and make
the firewall open up automatically, but that still doesn't fix the
fact that there is no port re-writing when routing between external
IPs and internal IPs.

> I also want to assign an available HostPort without digging into Kubernetes internal because it might change and can depend on cloudprovider implementation.
> 
> e.g. HostPort 5000 is not available to pods because Kubelet uses it
> e.g. HostPort 22 is not available to pods because sshd in GCE uses it.

Yes, this is a problem that I want to find a way to fix.  I know how I
want it to work, but GCE doesn't support it.
"
1037	416	monnand	2014-07-11 19:00:15	CONTRIBUTOR	"Does this mean that go 1.2's race detector will not report races? If so, we may want to add go 1.2 support back in .travis.yml.
"
1038	416	smarterclayton	2014-07-11 19:16:32	CONTRIBUTOR	"So if 1.2 had been in travis, it would have been failing.  With this change 1.2 should correctly identify races.
"
1039	416	brendandburns	2014-07-11 19:20:05	CONTRIBUTOR	"Yeah, we disabled 1.2 in travis because of this problem.  Want to update
.travis.yml in this PR?

Thanks!
--brendan

On Fri, Jul 11, 2014 at 12:16 PM, Clayton Coleman notifications@github.com
wrote:

> So if 1.2 had been in travis, it would have been failing. With this change
> 1.2 should correctly identify races.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/416#issuecomment-48771428
> .
"
1040	168	bgrant0607	2014-07-11 21:04:23	MEMBER	"We have cpu and memory in the container manifest:
    // Optional: Defaults to unlimited.
    Memory int `yaml:""memory,omitempty"" json:""memory,omitempty""`
    // Optional: Defaults to unlimited.
    CPU           int            `yaml:""cpu,omitempty"" json:""cpu,omitempty""`

However, AFAICT, we don't do anything with them. Besides, I think we want something more similar to lmctfy's API (request, limit, qos for each resource).

Another consideration: We could make it fairly easy to add new resources. Kubelet needs to understand each individual resource's characteristics, for isolation, QoS, overcommitment, etc. OTOH, the scheduler could deal with resources entirely abstractly. It could get resources and their capacities from the machines. Similarly, we'd need to make it possible to request abstract resources in the container/pod manifest.
"
1041	157	bgrant0607	2014-07-11 21:06:43	MEMBER	"FYI, Docker is also contemplating GC of unreachable images:
https://botbot.me/freenode/docker-dev/2014-07-11/?msg=17856832&page=3

That shouldn't stop us from doing something, but we should prioritize complementary functionality.
"
1042	168	thockin	2014-07-11 21:21:41	MEMBER	"What we described internally was that ""common"" resources like CPU, memory,
disk, etc were described as first-class things.  Other resources are
handled essentially as opaque counters.  E.g. a node says ""I have 5
resources with ID 12345"", a client says ""I need 2 resources with ID 12345"".
 The scheduler maps them.

On Fri, Jul 11, 2014 at 2:04 PM, bgrant0607 notifications@github.com
wrote:

> We have cpu and memory in the container manifest:
> // Optional: Defaults to unlimited.
> Memory int yaml:""memory,omitempty"" json:""memory,omitempty""
> // Optional: Defaults to unlimited.
> CPU int yaml:""cpu,omitempty"" json:""cpu,omitempty""
> 
> However, AFAICT, we don't do anything with them. Besides, I think we want
> something more similar to lmctfy's API (request, limit, qos for each
> resource).
> 
> Another consideration: We could make it fairly easy to add new resources.
> Kubelet needs to understand each individual resource's characteristics, for
> isolation, QoS, overcommitment, etc. OTOH, the scheduler could deal with
> resources entirely abstractly. It could get resources and their capacities
> from the machines. Similarly, we'd need to make it possible to request
> abstract resources in the container/pod manifest.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/168#issuecomment-48782790
> .
"
1043	397	brendandburns	2014-07-11 21:25:46	CONTRIBUTOR	"comment addressed, ptal
"
1044	378	brendandburns	2014-07-11 21:48:02	CONTRIBUTOR	"Thanks, I will patch this and give it a try this evening.

--brendan
"
1045	378	monnand	2014-07-11 22:00:50	CONTRIBUTOR	"@brendandburns Thank you. ping me if there's any problem.
"
1046	402	nf	2014-07-11 22:09:32	CONTRIBUTOR	"Actually godep does now have a rewrite flag. Check it out.

I intend to follow this up on my Monday (Australia time).

On Saturday, 12 July 2014, monnand notifications@github.com wrote:

> @dsymonds https://github.com/dsymonds I think godep
> https://github.com/tools/godep will not change import paths. It wraps
> the go command and add a directory to existing GOPATH. There's another tool
> named godeps https://github.com/dre1080/godeps, which changes import
> paths using goven https://github.com/kr/goven. (I know, this is
> confusing.)
> 
> I am working on #378
> https://github.com/GoogleCloudPlatform/kubernetes/pull/378 which uses
> godep https://github.com/tools/godep to manage dependencies. However,
> it will not solve the go get problem, because import paths are not
> changed. It does make adding/updating dependencies easier.
> 
> Probably @nf https://github.com/nf @dsymonds
> https://github.com/dsymonds would have some better suggestions?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/402#issuecomment-48765522
> .
"
1047	402	monnand	2014-07-11 22:21:05	CONTRIBUTOR	"@nf Then that would be great. We could go either way (changing import paths or not) if we use godep. I changed the build script in #378 with very little modification. But we may want to make more changes either in #378 or in another PR.
"
1048	15	bgrant0607	2014-07-11 22:39:12	MEMBER	"Docker issues that may be relevant to the iptables issue:
https://github.com/dotcloud/docker/issues/6002
https://github.com/dotcloud/docker/issues/6034
"
1049	298	bgrant0607	2014-07-11 23:06:00	MEMBER	"Or pod hostname, for IP per pod.
"
1050	298	thockin	2014-07-11 23:20:44	MEMBER	"This is an interesting one.  While we share an IP, we do not share a
hostname.  Those are two different namespaces (net and uts) and only net is
shared.

Besides that, Brendan resolved it this week.

On Fri, Jul 11, 2014 at 4:06 PM, bgrant0607 notifications@github.com
wrote:

> Or pod hostname, for IP per pod.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/298#issuecomment-48792931
> .
"
1051	188	bgrant0607	2014-07-11 23:39:29	MEMBER	"Thoughts on networking and naming, including more background, partly from Tim's aforementioned doc.

Kubernetes's current networking model is described here:
https://github.com/GoogleCloudPlatform/kubernetes/blob/master/DESIGN.md#network-model
And is described in some detail in issue #15 , and below:

We start Docker with:
DOCKER_OPTS=""--bridge cbr0 --iptables=false""

We set up this bridge on each node with SaltStack:
https://github.com/GoogleCloudPlatform/kubernetes/blob/master/cluster/saltbase/salt/_states/container_bridge.py

cbr0:
  container_bridge.ensure:
    - cidr: {{ grains['cbr-cidr'] }}
...
grains:
  roles:
    - kubernetes-pool
  cbr-cidr: $MINION_IP_RANGE

We make these addresses routable in GCE:
  gcutil addroute ${MINION_NAMES[$i]} ${MINION_IP_RANGES[$i]} \
  --norespect_terminal_width \
    --project ${PROJECT} \
    --network ${NETWORK} \
    --next_hop_instance ${ZONE}/instances/${MINION_NAMES[$i]} &

The minion IP ranges are /24s in the 10-dot space.

GCE itself does not know anything about these IPs, though.  

These are not externally routable, though, so containers that need to communicate with the outside world need to use host networking. To set up an external IP that forwards to the VM, it will only forward to the VM's primary IP (which is assigned to no pod).  So we use docker's -p flag to map published ports to the main interface.  This has the side effect of disallowing two pods from exposing the same port. (More discussion on this in #390.)

We create a container to use for the pod network namespace -- a single loopback device and a single veth device. All the user's containers get their network namespaces from this pod networking container.

Docker allocates IP addresses from a bridge we create on each node, using its “container” networking mode.
1. Create a normal (in the networking sense) container which uses a minimal image and runs a command that blocks forever.  This is not a user-defined container, and gets a special well-known name.
- creates a new network namespace (netns) and loopback device
- creates a new pair of veth devices and binds them to the netns
- auto-assigns an IP from docker’s IP range
  1. Create the user containers and specify their “net” argument as “container:<name of #1>. Docker finds the PID of the command running in the network container and attaches to the netns of that PID.

The net result is that all user containers within a pod behave as if they are on the same host with regard to networking.  They can all reach each other’s ports on localhost.  Ports which are published to the host interface are done so in the normal Docker way. All containers in all pods can talk to all other containers in all other pods by their 10-dot addresses.

In addition to avoiding the aforementioned problems with dynamic port allocation, this approach reduces friction for applications moving from the world of uncontainerized apps on physical or virtual hosts to containers within pods. People running application stacks together on the same host have already figured out how to make ports not conflict (e.g., by configuring them through environment variables) and have arranged for clients to find them. 

It reduces isolation between containers within a pod -- ports could conflict, and there couldn't be private ports across containers within a pod, but applications requiring their own port spaces could just run as separate pods and processes requiring private communication could run within the same container. Besides, the premise of pods is that containers within a pod share some resources (volumes, cpu, ram, etc.) and therefore expect and tolerate reduced isolation. Additionally, the user can control what containers belong to the same pod whereas, in general, they don't control what pods land together on a host.

When any container calls SIOCGIFADDR, it sees the IP that any peer container would see them coming from -- each pod has its own IP address that other pods can know. By making IP addresses and ports the same within and outside the containers and pods, we create a NAT-less, flat address space. ""ip addr show"" should work as expected. This would enable all existing naming/discovery mechanisms to work out of the box, including self-registration mechanisms and applications that distribute IP addresses. (We should test that with etcd and perhaps one other option, such as Eureka (used by Acme Air) or Consul.) We should be optimizing for inter-pod network communication. Within a pod, containers are more likely to use communication through volumes (e.g., tmpfs) or IPC.

This is different from the standard Docker model. In that mode, each container gets an IP in the 172-dot space and would only see that 172-dot address from SIOCGIFADDR.  If these containers connect to another container the peer would see the connect coming from a different IP than the container itself knows.  In short - you can never self-register anything from a container, because a container can not be reached on its private IP.

An alternative we considered was an additional layer of addressing: pod-centric IP per container. Each container would have its own local IP address, visible only within that pod. This would perhaps make it easier for containerized applications to move from physical/virtual hosts to pods, but would be more complex to implement (e.g., requiring a bridge per pod, split-horizon/VP DNS) and to reason about, due to the additional layer of addressing, and would break self-registration and IP distribution mechanisms.

We want to be able to assign IP addresses externally from Docker (https://github.com/dotcloud/docker/issues/6743) so that we don't need to statically allocate fixed-size IP ranges to each node, so that IP addresses can be made stable across network container restarts (https://github.com/dotcloud/docker/issues/2801), and to facilitate pod migration. Right now, if the network container dies, all the user containers must be stopped and restarted because the netns of the network container will change on restart, and any subsequent user container restart will join that new netns, thereby not being able to see its peers. Additionally, a change in IP address would encounter DNS caching/TTL problems. External IP assignment would also simplify DNS support (see below). And, we could potentially eliminate the bridge and use network interface aliases instead.

IPv6 would be a nice option, also, but we can't depend on it yet. Docker support is in progress: https://github.com/dotcloud/docker/pull/2974, https://github.com/dotcloud/docker/pull/6923, https://github.com/dotcloud/docker/issues/6975. Additionally, direct ipv6 assignment to instances doesn't appear to be supported by major cloud providers (e.g., AWS EC2, GCE) yet. We'd happily take pull requests from people running Kubernetes on bare metal, though. :-)

We'd also like to setup DNS automatically (#146). hostname, $HOSTNAME, etc. should return a name for the pod (#298), and gethostbyname should be able to resolve names of other pods. Probably we need to set up a DNS resolver to do the latter (https://github.com/dotcloud/docker/issues/2267), so that we don't need to keep /etc/hosts files up to date dynamically. 

If we want Docker links and/or docker inspect to work, we may have work to do there. Right now, docker inspect doesn't show the networking configuration of the containers, since they derive it from another container. That information should be exposed somehow. I haven't looked to see whether link variables would be set correctly, but I think there's a possibility they aren't.

We need to think more about what to do with the service proxy. Using a flat service namespace doesn't scale and environment variables don't permit dynamic updates. 

We'd also like to accommodate other load-balancing solutions (e.g., HAProxy), non-load-balanced services (#260), and other types of groups (worker pools, etc.). Providing the ability to Watch a label selector applied to pod addresses would enable efficient monitoring of group membership, which could be directly consumed or synced with a discovery mechanism. Event hooks (#140) for join/leave events would probably make this even easier.

We'd even like to make pods directly routable from the external internet, though we can't do that yet. One approach could be to create a new host interface for each pod, if we had a way to route an external IP to it.

We're also working on making it possible to specify a different bridge for each container. We may or may not still need this, but it could be useful for certain scenarios:
https://botbot.me/freenode/docker-dev/2014-06-05/?msg=15716610&page=4
https://github.com/dotcloud/docker/issues/6155
https://github.com/dotcloud/docker/pull/6704
"
1052	417	roberthbailey	2014-07-11 23:50:05	MEMBER	"lgtm
"
1053	410	nyaxt	2014-07-12 04:19:32	CONTRIBUTOR	"Thanks for your comments!

> CLA
> I'm a Googler (kouhei@) and I used the google.com address, so I think our corporate CLA should do the job?
"
1054	410	brendandburns	2014-07-12 04:40:16	CONTRIBUTOR	"Sorry, yes, (it's hard to track github handles to Google ldaps, and you're not in any of the Google github organizations).  No CLA is necessary.

Thanks for the PR, merging.
"
1055	411	brendandburns	2014-07-12 04:41:56	CONTRIBUTOR	"Thanks for the PR.
"
1056	188	bgrant0607	2014-07-12 04:58:45	MEMBER	"For completeness, other network-related issues:

host networking: #175 
multiple bridges: #222 
"
1057	423	brendandburns	2014-07-12 05:30:26	CONTRIBUTOR	"LGTM. 
"
1058	422	brendandburns	2014-07-12 05:32:02	CONTRIBUTOR	"LGTM
"
1059	421	brendandburns	2014-07-12 05:32:27	CONTRIBUTOR	"LGTM
"
1060	420	claire921	2014-07-12 05:37:18	CONTRIBUTOR	"closed on error. reopened now.
"
1061	430	smarterclayton	2014-07-12 17:50:55	CONTRIBUTOR	"LGTM
"
1062	428	smarterclayton	2014-07-12 17:52:30	CONTRIBUTOR	"LGTM
"
1063	427	smarterclayton	2014-07-12 17:54:07	CONTRIBUTOR	"LGTM
"
1064	420	smarterclayton	2014-07-12 17:58:36	CONTRIBUTOR	"LGTM
"
1065	434	smarterclayton	2014-07-12 18:03:25	CONTRIBUTOR	"Using httptest wherever possible is good.  I don't see anything the test is using fakeHTTPClient for that httptest could not do, so might make sense to do so while you are in here.
"
1066	434	claire921	2014-07-12 18:05:53	CONTRIBUTOR	"ok. i will change it to use httptest server. 
hold on this pr.
"
1067	436	smarterclayton	2014-07-12 18:19:57	CONTRIBUTOR	"LGTM
"
1068	435	smarterclayton	2014-07-12 18:35:31	CONTRIBUTOR	"LGTM
"
1069	425	smarterclayton	2014-07-12 18:41:17	CONTRIBUTOR	"If so, it would be useful to preserve the property that an image consumer can choose to override the port preferences of an image author.  Might entail making Ports a pointer and differentiating between null and empty array.

Also, implementor would need to pull the image first (since docker registry still does not expose remote metadata, which is an issue that needs to be pushed at upstream).
"
1070	424	smarterclayton	2014-07-12 18:45:19	CONTRIBUTOR	"Of course, links v2 would change this some.  I'd like to be able to define a ""link relationship"" that can be passed to the kubelet (to allow resolution of remote linkage to be delegated to minions at a minimum), but it will have to wait for docker to iterate on the link concept more (or risk being incompatible).

I'll track that separately.
"
1071	429	smarterclayton	2014-07-12 18:47:19	CONTRIBUTOR	"Change LGTM, no opinion on rename.  Might be good to squash changes.
"
1072	425	thockin	2014-07-12 18:49:13	MEMBER	"The way we currently use Ports is a bit broken, IMO.

I you expose a Port in our interface, it ALWAYS has a HostPort.  In fact,
given how we set up networking, the only reasons to list a Port are 1) to
get a HostPort (-p equiv) or 2) to give it a name for something like a
health check.

EXPOSE actually has no meaning that I can see in our current model
(especially in GCE) because we are trying to avoid any extra NAT - we
should actually measure the impact of that to see if it is REALLY worth
being different.

That said, I think it's totally legit for a container to have more exposed
ports than published ports - so how do I know which EXPOSE to map a Port
structure too.

Or maybe I am misunderstanding?

Ports in our API is _not_ redundant with EXPOSE in Dockerfiles - it's the
moral equivalent of the -p flag.

On Fri, Jul 11, 2014 at 10:38 PM, Johan Euphrosine <notifications@github.com

> wrote:
> 
> The containerPort attribute in the pod manifest could be inferred from
> the image metadata when not supplied by the developer.
> 
> That way developers won't have to duplicate the information they already
> put in EXPOSE in their Dockerfile.
> 
> This information looks available through the remote API on /image
> resources under PortSpecs:
> 
> https://docs.docker.com/reference/api/docker_remote_api_v1.13/#inspect-an-image
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/425.
"
1073	425	smarterclayton	2014-07-12 19:04:22	CONTRIBUTOR	"I think if EXPOSE as a suggestion to a consumer - there are cases where trusting expose is wrong (java debug port) or where just because a server is listening on port 494, doesn't mean that that port should be accessible outside the container (firewall type behavior).  Also, I don't think its unreasonable for different deployments of kube to have different reqts for networking, as long as the API is consistent to consumers.

Maybe we should spec two separate concepts:
- I want this port to be available for consumption by outsiders (firewall)
- I want to describe this port for use by higher level consumers (metadata)

The former might be linked to whether other pods can reach this port, the latter might be for service discovery (in the generic sense, not the service noun) by the implementor of a service proxy.
"
1074	425	thockin	2014-07-12 19:07:25	MEMBER	"I agree - as I said - what we have is broken.  I have a spec somewhere for
how I think we should change it, but I want to figure out some of the
networking issues before really getting into this.

So take my whole previous message and read it with ""AS THINGS ARE TODAY...""
in front :)

On Sat, Jul 12, 2014 at 12:04 PM, Clayton Coleman notifications@github.com
wrote:

> I think if EXPOSE as a suggestion to a consumer - there are cases where
> trusting expose is wrong (java debug port) or where just because a server
> is listening on port 494, doesn't mean that that port should be accessible
> outside the container (firewall type behavior). Also, I don't think its
> unreasonable for different deployments of kube to have different reqts for
> networking, as long as the API is consistent to consumers.
> 
> Maybe we should spec two separate concepts:
> - I want this port to be available for consumption by outsiders
>   (firewall)
> - I want to describe this port for use by higher level consumers
>   (metadata)
> 
> The former might be linked to whether other pods can reach this port, the
> latter might be for service discovery (in the generic sense, not the
> service noun) by the implementor of a service proxy.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/425#issuecomment-48821022
> .
"
1075	425	thockin	2014-07-12 19:08:15	MEMBER	"And if we could push port names down to Docker (which I think we have not
tried) we could simplify further.

On Sat, Jul 12, 2014 at 12:07 PM, Tim Hockin thockin@google.com wrote:

> I agree - as I said - what we have is broken.  I have a spec somewhere for
> how I think we should change it, but I want to figure out some of the
> networking issues before really getting into this.
> 
> So take my whole previous message and read it with ""AS THINGS ARE
> TODAY..."" in front :)
> 
> On Sat, Jul 12, 2014 at 12:04 PM, Clayton Coleman <
> notifications@github.com> wrote:
> 
> > I think if EXPOSE as a suggestion to a consumer - there are cases where
> > trusting expose is wrong (java debug port) or where just because a server
> > is listening on port 494, doesn't mean that that port should be accessible
> > outside the container (firewall type behavior). Also, I don't think its
> > unreasonable for different deployments of kube to have different reqts for
> > networking, as long as the API is consistent to consumers.
> > 
> > Maybe we should spec two separate concepts:
> > - I want this port to be available for consumption by outsiders
> >   (firewall)
> > - I want to describe this port for use by higher level consumers
> >   (metadata)
> > 
> > The former might be linked to whether other pods can reach this port, the
> > latter might be for service discovery (in the generic sense, not the
> > service noun) by the implementor of a service proxy.
> > 
> > ## 
> > 
> > Reply to this email directly or view it on GitHub
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/425#issuecomment-48821022
> > .
"
1076	425	smarterclayton	2014-07-12 19:10:19	CONTRIBUTOR	"Absolutely :)
"
1077	426	smarterclayton	2014-07-12 19:11:58	CONTRIBUTOR	"LGTM
"
1078	425	thockin	2014-07-12 19:12:04	MEMBER	"Let's leave this issue open then, when we're ready to have a design
discussion on EXPOSE vs PUBLISH and different network modes.

On Sat, Jul 12, 2014 at 12:10 PM, Clayton Coleman notifications@github.com
wrote:

> Absolutely :)
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/425#issuecomment-48821168
> .
"
1079	431	claire921	2014-07-12 21:22:57	CONTRIBUTOR	"@smarterclayton @thockin back to use var keyword.
"
1080	429	brendandburns	2014-07-13 04:12:00	CONTRIBUTOR	"LGTM

Renaming is fine with me.  RandomFitScheduler?
"
1081	437	brendandburns	2014-07-13 04:14:41	CONTRIBUTOR	"Thanks for the PR.  Can you sign our CLA as described in CONTRIB.md?

Thanks!
--brendan
"
1082	429	nyaxt	2014-07-13 05:41:01	CONTRIBUTOR	"Thanks for your comments!

Renamed to RandomFitScheduler.
"
1083	438	brendandburns	2014-07-13 05:45:38	CONTRIBUTOR	"Merging, since it passes travis, and its currently broken.
"
1084	439	brendandburns	2014-07-13 05:46:30	CONTRIBUTOR	"hold up on this.  It appears to not actually have created the os x build.  I'll keep fiddling.
"
1085	429	smarterclayton	2014-07-13 14:04:40	CONTRIBUTOR	"Rename LGTM.
"
1086	431	smarterclayton	2014-07-13 14:13:05	CONTRIBUTOR	"LGTM
"
1087	442	monnand	2014-07-13 20:50:58	CONTRIBUTOR	"If I understand it correctly, does sub-container (partly) solve this issue? This feature (sub-container) is implemented in [lmctfy](https://github.com/google/lmctfy), but not in docker. Not sure if docker will add this function in the future.

(I'm not saying that sub-container is the only solution. Just think it could be a way.)
"
1088	442	smarterclayton	2014-07-13 21:15:25	CONTRIBUTOR	"This is subdivision of total cluster resources across one or more tenants, vs subdivision of resources in a container.
"
1089	419	kelseyhightower	2014-07-13 21:59:28	CONTRIBUTOR	"/health
"
1090	441	brendandburns	2014-07-13 22:09:03	CONTRIBUTOR	"Thanks for the PR!  Can you sign our CLA as documented in CONTRIB.md?

--brendan
"
1091	443	brendandburns	2014-07-13 22:18:16	CONTRIBUTOR	"There are a number of small steps here to start:
1) Get the basic auth out of the nginx proxy, and into the apiserver (providing suitable abstractions to make auth pluggable)
2) Implement OAuth2 (using https://github.com/RangelReale/osin?)

Those are good places to start while we figure out the details of scope, acls and auditing.
"
1092	440	brendandburns	2014-07-13 22:19:58	CONTRIBUTOR	"Fixes #424 
"
1093	387	brendandburns	2014-07-13 22:24:30	CONTRIBUTOR	"I believe this is already complete, RESTStorage takes a label selector as a parameter, and each of the resource collections Pod/Service/ReplicationController support the filtering.

Please re-open if I'm missing something.
"
1094	385	brendandburns	2014-07-13 22:25:19	CONTRIBUTOR	"Done.  This was fixed in #389 
"
1095	260	brendandburns	2014-07-13 22:26:51	CONTRIBUTOR	"The two primitives described by @bgrant0607 is it worth keeping this issue open?  Or are there more specific issues we can file?
"
1096	270	brendandburns	2014-07-13 22:29:37	CONTRIBUTOR	"This is more or less done for now, and that which remains, is captured in other issues (e.g. #354 )
"
1097	248	brendandburns	2014-07-13 22:31:00	CONTRIBUTOR	"Fixed by #397 
"
1098	295	brendandburns	2014-07-13 22:31:44	CONTRIBUTOR	"Duplicate of #220 
"
1099	134	brendandburns	2014-07-13 22:34:02	CONTRIBUTOR	"Closing this issue as we now support lists of manifests everywhere.
"
1100	356	smarterclayton	2014-07-13 23:57:17	CONTRIBUTOR	"@thockin ptal at last commit - started the ContainerManifest.ID -> Pod.Name + Pod.FullName transition, and started decoupling Kubelet from config source information (no example yet, but Kubelet no longer tied to config sources, just the update stream).  Temporarily removed the incremental updates because it'd be a much more involved change, although I want to sync with you and brendan about whether we should queue updates to specific pods in their on goroutines so we can avoid having to block the update loop on individual kills (i.e. if a kill takes 10 minutes for some reason, the update loop would be stuck).
"
1101	356	smarterclayton	2014-07-13 23:59:21	CONTRIBUTOR	"Also, I apologize in advance for the wall-o'-commit - I'll break it up after I feel more comfortable about the changes.
"
1102	441	zhgwenming	2014-07-14 03:03:36	CONTRIBUTOR	"Thanks @brendandburns , signed the CLA, let me know if there's any other problem.
"
1103	441	brendandburns	2014-07-14 03:30:22	CONTRIBUTOR	"Thanks again.  Merging.
"
1104	402	nf	2014-07-14 04:11:48	CONTRIBUTOR	"I think we should avoid godep at first, in favour of an incremental change to the current process.

My suggested next step is to rewrite import statements that refer to third_party libraries to use the fully-qualified paths, like `""github.com/GoogleCloudPlatform/kubernetes/third_party/...""`.

Here's a change that does just that:  https://github.com/nf/kubernetes/commit/592939593860b0280b96d90d735328d4f9d9b6f2
Which includes the custom tool rewrite-import.go, that made these changes:  https://github.com/nf/kubernetes/blob/592939593860b0280b96d90d735328d4f9d9b6f2/third_party/rewrite-imports.go

I'm not ready to send this as a pull request yet. I need to test that everything is still working correctly and that update.sh does the right thing. But this is the direction I would like to take. Thoughts?
"
1105	402	brendandburns	2014-07-14 05:14:48	CONTRIBUTOR	"Since Joe Beda was involved in the original discussions wrt to this, and
the original author of these scripts, I would prefer to wait until he
returns from vacation (~2 weeks or so)

Thanks
--brendan

On Sun, Jul 13, 2014 at 9:11 PM, Andrew Gerrand notifications@github.com
wrote:

> I think we should avoid godep at first, in favour of an incremental change
> to the current process.
> 
> My suggested next step is to rewrite import statements that refer to
> third_party libraries to use the fully-qualified paths, like ""
> github.com/GoogleCloudPlatform/kubernetes/third_party/..."".
> 
> Here's a change that does just that: nf@5929395
> https://github.com/nf/kubernetes/commit/592939593860b0280b96d90d735328d4f9d9b6f2
> Which includes the custom tool rewrite-import.go, that made these changes:
> https://github.com/nf/kubernetes/blob/592939593860b0280b96d90d735328d4f9d9b6f2/third_party/rewrite-imports.go
> 
> I'm not ready to send this as a pull request yet. I need to test that
> everything is still working correctly and that update.sh does the right
> thing. But this is the direction I would like to take. Thoughts?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/402#issuecomment-48864016
> .
"
1106	402	nf	2014-07-14 05:18:43	CONTRIBUTOR	"SGTM.
"
1107	444	thockin	2014-07-14 05:54:01	MEMBER	"We're very open to teaching Docker to manage pods.  We have discussed with
Docker folks and they also seemed open to it, though the conversations
about it always seem to derail into other topics.  In the mean time, we
have been pushing on a few individual changes to add the features required
one-by-one.

On Sun, Jul 13, 2014 at 9:24 PM, Michael Neale notifications@github.com
wrote:

> Does it make any sense to extract the ""pod"" code into a place that could
> potentially be combined with docker itself (I say potentially, as the
> docker project may prefer to leave this out - as a 3rd party tool).
> 
> The need to stand up 3 containers for a single app is pretty common - for
> example - discourse has a setup that stands up a few containers to run
> their app on a single node: https://github.com/discourse/discourse_docker
> 
> The pod concept (at least some of it) may be able to be separated -
> avoiding a proliferation of formats/tricks to do essentially the same
> thing.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/444.
"
1108	167	kelseyhightower	2014-07-14 14:29:01	CONTRIBUTOR	"I've taken a slightly different approach for running Kubernetes on CoreOS. I'm building all the binaries in a separate container and using docker cp to extract them. Then I upload the binaries to Google Cloud Storage.

Next I create a Dockerfile, one per application, and build a container from the uploaded binaries. Details of this work and the related docker files can be found here:
- [Kubernetes Docker Containers](https://github.com/kelseyhightower/kubernetes-coreos/blob/master/docs/containers.md)
- [Dockerfiles](https://github.com/kelseyhightower/kubernetes-coreos/tree/master/docker)

I've chosen to build the individual containers using binaries to keep the images small. My goal is to build a generic set of containers for each Kubernetes component usable on any platform.
"
1109	167	brendandburns	2014-07-14 14:42:48	CONTRIBUTOR	"Kelsey,
Have a look at the contents of the './build' directory, that does exactly
what you suggest.

It would be great to centralize on on version of this stuff.

Brendan
On Jul 14, 2014 7:29 AM, ""Kelsey Hightower"" notifications@github.com
wrote:

> I've taken a slightly different approach for running Kubernetes on CoreOS.
> I'm building all the binaries in a separate container and using docker cp
> to extract them. Then I upload the binaries to Google Cloud Storage.
> 
> Next I create a Dockerfile, one per application, and build a container
> from the uploaded binaries. Details of this work and the related docker
> files can be found here:
> - Kubernetes Docker Containers
>   https://github.com/kelseyhightower/kubernetes-coreos/blob/master/docs/containers.md
> - Dockerfiles
>   https://github.com/kelseyhightower/kubernetes-coreos/tree/master/docker
> 
> I've chosen to build the individual containers using binaries to keep the
> images small. My goal is to build a generic set of containers for each
> Kubernetes component usable on any platform.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/167#issuecomment-48906331
> .
"
1110	167	kelseyhightower	2014-07-14 16:01:00	CONTRIBUTOR	"@brendanburns Thanks, did not see that. I wonder if we can now close out all of the Dockerfile related issues. Maybe once there are builds available from the Docker Hub.
"
1111	167	brendandburns	2014-07-14 16:07:18	CONTRIBUTOR	"Yeah, there are two (related) tasks, one is to build everything as a docker
container, the other is to default the running of k8s to using those
containers.

I think #1 is mostly done, but #2 is still in progress (though you're doing
a bunch to close out #2 as well ;)

Brendan
On Jul 14, 2014 9:01 AM, ""Kelsey Hightower"" notifications@github.com
wrote:

> @brendanburns https://github.com/brendanburns Thanks, did not see that.
> I wonder if we can now close out all of the Dockerfile related issues.
> Maybe once there are builds available from the Docker Hub.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/167#issuecomment-48918918
> .
"
1112	360	lavalamp	2014-07-14 16:33:23	MEMBER	"+1 to preferring something other than $ separated lists, that's really non-obvious. When I originally wrote the selector code, I started to write something that used go/tokens and go/ast to parse a string as if it were a go expression, and then build a structure similar to what Tim suggested. But that was getting complex, so I went with what we have now to get something in quickly.

Also, I think that there's no need for (2) x=*, as that's default behavior. I don't understand the need for (3) x=<<foo>>.
"
1113	444	smarterclayton	2014-07-14 16:33:25	CONTRIBUTOR	"Some discussion here: https://groups.google.com/forum/m/#!topic/docker-dev/1K0Ff7Em18U

I tried to capture the ideas from the unconf about ""groups"" of containers (which most people seemed to think might not be a terrible idea).

Might be good to list the items out somewhere.
"
1114	260	smarterclayton	2014-07-14 16:36:45	CONTRIBUTOR	"I don't think zookeeper is solved - since you need the unique identifier in each container.  I _think_ you could do this with 3 separate replication controllers (one per instance) or a mode on the replication controller.
"
1115	419	lavalamp	2014-07-14 16:56:02	MEMBER	"Can we provide this as a separate package? Bonus points if you can get this behavior in the default http server just by doing `import _ ""healthz""`.
"
1116	440	brendandburns	2014-07-14 17:22:06	CONTRIBUTOR	"Comments addressed, ptal.

Thanks!
--brendan
"
1117	434	brendandburns	2014-07-14 17:26:50	CONTRIBUTOR	"Yeah, originally this code didn't support a ""host"" and always looked @ localhost, that's why I stubbed it out with the fake.  As it stands now, I think that httptest server should work.

Thanks for updating!

--brendan
"
1118	433	brendandburns	2014-07-14 17:42:12	CONTRIBUTOR	"Comments addressed, please re-check.

Thanks
--brendan
"
1119	440	bgrant0607	2014-07-14 17:42:54	MEMBER	"Is the service environment variable format documented anywhere? I didn't see it in README.md or DESIGN.md or kubernetes.raml. It would be nice to document links compatibility somewhere. I could do that in a separate PR, if you like.
"
1120	440	brendandburns	2014-07-14 18:59:14	CONTRIBUTOR	"@bgrant0607 you already promised to do this ;)  See https://github.com/GoogleCloudPlatform/kubernetes/issues/363
"
1121	357	lavalamp	2014-07-14 19:29:32	MEMBER	"I heartily endorse this. Note that I already made the public API support compare-and-swap on updates. The ultimate goal of #354 was indeed to make the scheduler run on only the public API, at which point any number of schedulers could be run in parallel.
"
1122	353	lavalamp	2014-07-14 19:35:00	MEMBER	"Agree that we're ""done"" when the pod is accepted and stored in the system, and needn't wait until it's running.

Ultimately, one should be able to wait on events (i.e., we'd wrap etcd's watch); one such event would be the state change from assigned to running, and that's how clients should get this behavior if they need it.
"
1123	354	lavalamp	2014-07-14 19:36:47	MEMBER	"And this is originally by Brendan:

1) We should reverse the flow of operations for the scheduler:  The scheduler should use compareAndSwap to write the pod into the /registry/hosts/<machine>/kublet list first.  This ensures that we have scheduler data integrity at the kubelet level.  ie. we don't schedule something on a kubelet that actually can't fit.

2) Having the canonical list of pods include the host is actually really convenient when hosts come and go.  We get rescheduling on host failure for free if we make the machine path be the canonical list of pods, since when the machine disappears, its pods disappear too.

So I think that the primary goal, of getting some de-coupling between pod acceptance, and pod scheduling is a good one, but I think we should be careful not to paint ourselves into a nasty corner wrt to other parts of the scheduler because we want to make it happen.
"
1124	354	lavalamp	2014-07-14 19:38:26	MEMBER	"I kinda want the scheduler to run using only the public API without needing etcd at all. I think that's a key point we need to hash out, whether schedulers will talk to the public API, etcd, or both.
"
1125	353	smarterclayton	2014-07-14 19:46:18	CONTRIBUTOR	"Agreed
"
1126	362	brendandburns	2014-07-14 19:58:26	CONTRIBUTOR	"Follow up from the sdk folks:

There is a bug in determining terminal width.  They're working on fixing it.

In the mean time you can try adding `--norespect_terminal_width``
"
1127	287	brendandburns	2014-07-14 19:59:31	CONTRIBUTOR	"Ping on this.  I'd like to fix this (and close this PR, its been hanging for a while)

Best!
--brendan
"
1128	376	lavalamp	2014-07-14 20:17:14	MEMBER	"Does godep allow for using specific versions of our third party dependencies?
"
1129	380	lavalamp	2014-07-14 20:34:56	MEMBER	"Action items here:

1) We should wait for acceptance, not for up-and-running. Covered by another issue I can't find at the moment.
2) kublet should collect garbage so that this doesn't bring the node down.
3) We need a general mechanism to report errors like this upstream.
4) We should have rejected the request in the first place with a nice error message.
"
1130	365	lavalamp	2014-07-14 20:51:12	MEMBER	"Late thoughts: http health/liveness checker should go in its own package. It can be reused by master to check kublet's own health, for example.
"
1131	187	derekwaynecarr	2014-07-14 21:01:02	MEMBER	"I am looking at the following Vagrant setup as an initial solution.

Define VMs for the following:
1) master
2) minion

You can choose to run the master and minion boxes using either Fedora or Debian.  

The Vagrant provision scripts reuse the existing Salt configuration process for provisioning both the master and the minion machines.  

If you want to develop and run the master services locally because you cannot run docker on your laptop, then we should look to support just running the minion in a VM.

The master / minion boxes communicate over a private network.

To update and redeploy your local code to your vagrant setup, just re-run vagrant provision which will cause the salt configuration to get updated.

Once I get the SLS operating specific updates in place, will look to submit a PR for review/comment.
"
1132	445	yifan-gu	2014-07-14 21:23:41	MEMBER	"@lavalamp 
Thanks for catching. Fixed.
"
1133	406	rakyll	2014-07-14 21:39:24	CONTRIBUTOR	"I will return to this PR once I have time.
"
1134	446	dchen1107	2014-07-14 21:45:37	MEMBER	"Please hold the review. I messed up with my repository, will fix it first and resend PR.
"
1135	445	lavalamp	2014-07-14 22:08:48	MEMBER	"Thanks for the change!
"
1136	447	lavalamp	2014-07-14 22:10:15	MEMBER	"Thanks for the change-- can you squash these commits before I merge?
"
1137	448	monnand	2014-07-14 22:10:35	CONTRIBUTOR	"Just talked with @lavalamp offline, he suggested me to use cAdvisor's data structure directly. I think this would be good for me because there'll be less work.

Thoughts? ping @brendanburns @thockin @dchen1107 @vmarmol 
"
1138	447	juliaferraioli	2014-07-14 22:27:06	CONTRIBUTOR	"Absolutely -- how's that?
"
1139	447	lavalamp	2014-07-14 22:27:42	MEMBER	"LGTM
"
1140	376	yifan-gu	2014-07-14 22:32:39	MEMBER	"@lavalamp Yes, it does. 
"
1141	376	monnand	2014-07-14 22:40:19	CONTRIBUTOR	"@lavalamp As @yifan-gu said, yes. AFAIK, godep will do everything our scripts could do.

But in #402, @nf suggested to change import paths instead of using godep and we would like to postpone this and other related issues/PRs (basically, they are #378, #402 and this one) until @jbeda back.

I agree with @nf's proposal totally. But I remembered @jbeda mentioned some licensing issue if we change third parties' code. I'm definitely not an expert in legal field. So I would rather wait for others' suggestion.
"
1142	450	lavalamp	2014-07-14 23:06:55	MEMBER	"Can you confirm the ""Resolved the merge issues"" commit is correct? It seems to delete a bunch of stuff from third_party?
"
1143	448	monnand	2014-07-14 23:11:29	CONTRIBUTOR	"Talked with @thockin offline and he agrees with @lavalamp's suggestion. So I'm going to change the code to let kubelet directly use cadvisor's data structure. If anyone has other problem, please make a comment below.
"
1144	450	dchen1107	2014-07-14 23:21:02	MEMBER	"It caused by the change I made for deps.sh. I did ran build.sh, and
everything works fine there.

On Mon, Jul 14, 2014 at 4:07 PM, Daniel Smith notifications@github.com
wrote:

> Can you confirm the ""Resolved the merge issues"" commit is correct? It
> seems to delete a bunch of stuff from third_party?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/450#issuecomment-48971694
> .
"
1145	448	monnand	2014-07-14 23:25:31	CONTRIBUTOR	"This PR is ready to be reviewed now. ping @brendanburns @thockin @lavalamp 
"
1146	357	tristanz	2014-07-14 23:25:50	NONE	"Just to followup on offline discussion: I'm 100% for public API vs. direct etcd access as goal.  If that API could do something more subtle (Omega-like) than `CompareAndSwap` on the entire kubelet state that would be awesome too.
"
1147	440	brendandburns	2014-07-14 23:34:31	CONTRIBUTOR	"comments addressed, please take a look.

Thanks!
--brendan
"
1148	444	michaelneale	2014-07-15 00:00:43	NONE	"libswarm did pop into my mind too - good to see others are discussing and coming to same conclusion. 

My specific case was in distributing an open source ""app"" which is composed of a few containers working in concert. Having an obvious way to run via the docker command: ""docker pod run pod.json"" seemed like a desirable end goal.  
"
1149	419	brendandburns	2014-07-15 00:01:16	CONTRIBUTOR	"Addressed @lavalamp comments.  Sticking with /healthz ;)
"
1150	419	smarterclayton	2014-07-15 00:02:46	CONTRIBUTOR	"LGTM
"
1151	433	brendandburns	2014-07-15 00:09:14	CONTRIBUTOR	"Rebased.
"
1152	376	nf	2014-07-15 00:27:29	CONTRIBUTOR	"@monnand the licensing issue regarding rewriting import paths has been resolved. It is fine for us to proceed with that approach.

This issue is blocked on #402. #402 is blocked awaiting @jbeda's return.

#378 should not be merged until we have decided to go with godep. I would like to resolve #402 before moving to godep, if at all.
"
1153	378	monnand	2014-07-15 00:43:05	CONTRIBUTOR	"Please do not merge this PR until we got a consensus in #402 (and decide to use godep)
"
1154	376	monnand	2014-07-15 01:12:46	CONTRIBUTOR	"@nf Agree. If there's no legal issue, then changing import paths would be the best way to me. I will follow on #402 when @jbeda's back. 
"
1155	451	yifan-gu	2014-07-15 01:54:54	MEMBER	"Hi all, I will split this into several pr, and explain why I do this later this evening.
Now I am taking off...

Originally I was just going to replace those channel<-true with close(channel), but then I found #449 

So just a quick fix...

Thank you for any advice!
"
1156	449	smarterclayton	2014-07-15 02:20:57	CONTRIBUTOR	"This is fixed in #356
"
1157	451	smarterclayton	2014-07-15 02:21:52	CONTRIBUTOR	"I'm in the process of a larger change in #356 that fixes some of these.  Can you provide these as a change to that branch?
"
1158	433	brendandburns	2014-07-15 03:25:32	CONTRIBUTOR	"Comment addressed.  ptal

thanks!
--brendan
"
1159	419	brendandburns	2014-07-15 03:39:04	CONTRIBUTOR	"Comments addressed and rebased.  ptal.

--brendan
"
1160	453	brendandburns	2014-07-15 03:58:32	CONTRIBUTOR	"Addresses part of #168 
"
1161	433	lavalamp	2014-07-15 03:59:03	MEMBER	"LGTM
"
1162	433	smarterclayton	2014-07-15 04:22:22	CONTRIBUTOR	"LGTM
"
1163	356	lavalamp	2014-07-15 04:26:25	MEMBER	"This is a super ginormous change. I'm on board with all the directions it's going, but can we split this up, both for easier reviewing and to limit the number of other PRs we have to block?
"
1164	454	brendandburns	2014-07-15 04:30:11	CONTRIBUTOR	"comment addressed, ptal.
"
1165	454	lavalamp	2014-07-15 04:34:33	MEMBER	"One last comment, LGTM after that.
"
1166	454	brendandburns	2014-07-15 04:39:53	CONTRIBUTOR	"Comment addressed, ptal.

--brendan
"
1167	455	lavalamp	2014-07-15 04:45:58	MEMBER	"Calling this trivial, merging.
"
1168	451	lavalamp	2014-07-15 04:52:20	MEMBER	"Recommend splitting apart the two changes.
"
1169	453	vmarmol	2014-07-15 05:09:32	CONTRIBUTOR	"LGTM
"
1170	451	yifan-gu	2014-07-15 06:45:49	MEMBER	"@smarterclayton Thanks for pointing out! Good to see the refactor! But as @lavalamp said, it is a little too big to review.
I will try to read the `kubelet.go` file first.
"
1171	450	dchen1107	2014-07-15 07:12:31	MEMBER	"Abandon this one due to build failure caused by merging and update.sh. PR #457 was sent out to replace this.  
"
1172	451	yifan-gu	2014-07-15 07:34:25	MEMBER	"@lavalamp I put the changes in operation.go in #458. And will do other changes on @smarterclayton 's branch, so closing this one.
"
1173	356	yifan-gu	2014-07-15 07:42:35	MEMBER	"Hi @smarterclayton , btw are you going to abstract the LogEvent too to pull etcd out of the kubelet in this PR?
"
1174	390	yugui	2014-07-15 11:06:54	CONTRIBUTOR	"> > On Fri, Jul 11, 2014 at 12:57 AM, Yuki Yugui Sonoda notifications@github.com wrote:
> > 
> > I'm not sure if I get your point correctly, but I would be better if I can expose services to external IP without having HostPort at all.
> > In order for an external IP in GCE to find your service, you have to
> > have a HostPort mapping.  If you don't, then we don't bind you to the
> > main interface of the VM.
> > What I actually want is the followings.
> > 
> > I have a system which consists of several services. Some of them are backends, which don't need to be accessible from external IPs. Others are exposed to external.
> > IIUC, exposed services need to have HostPort so that external IP can access to them.
> > 
> > But I don't want to carefully manage my list of HostPorts so that ports don't conflict to each other. Why doesn't Kuberlet do for me?
> 
> We can assign you a port, but then nobody knows how to find it.  You
> need to open the GCE firewall for that port.  You need to tell your
> customers what port number it is.  We can put it into etcd and make
> the firewall open up automatically, but that still doesn't fix the
> fact that there is no port re-writing when routing between external
> IPs and internal IPs.

It doesn't matter for firewall or customers because kube-proxy proxies from service port to the automatically determined port.  So GCE firewall need to open the service port which kube-proxy serves at, and the customer also needs to know the service port.

I didn't think about automatic assignment of service ports.

> > I also want to assign an available HostPort without digging into Kubernetes internal because it might change and can depend on cloudprovider implementation.
> > 
> > e.g. HostPort 5000 is not available to pods because Kubelet uses it
> > e.g. HostPort 22 is not available to pods because sshd in GCE uses it.
> 
> Yes, this is a problem that I want to find a way to fix.  I know how I
> want it to work, but GCE doesn't support it.

If we manage the list of used ports in etcd, it can be also covered.
"
1175	356	smarterclayton	2014-07-15 12:47:08	CONTRIBUTOR	"For sure, the first two are ready to be split, pod fullname is trickier to separate.  I will split the container lookups and sync loop alterations into their own as well.
"
1176	461	smarterclayton	2014-07-15 13:35:55	CONTRIBUTOR	"LGTM
"
1177	458	smarterclayton	2014-07-15 13:36:39	CONTRIBUTOR	"LGTM
"
1178	453	smarterclayton	2014-07-15 13:37:31	CONTRIBUTOR	"LGTM
"
1179	380	smarterclayton	2014-07-15 13:39:33	CONTRIBUTOR	"1) above is #353 
"
1180	443	smarterclayton	2014-07-15 13:42:51	CONTRIBUTOR	"osin looks pretty good - it seems to be the most feature complete and active oauth2 server impl.

For pluggable auth, what makes the most sense?  Normally I'd make this a middleware function in the handler chain, and depending on a config value add that in so that a context object can be retrieved representing the authenticated identity.  However, the implementations must be statically compilable today, delegated to an external service (may not be that bad), or implemented via something like otto (embeddable js).  The first seems like the simplest option right now.
"
1181	407	yugui	2014-07-15 13:57:25	CONTRIBUTOR	"PTAL
"
1182	462	tnolet	2014-07-15 14:25:10	NONE	"This issue can be closed. Turns out disks are full after running the initial parts of the example. Please see issue: https://github.com/GoogleCloudPlatform/kubernetes/issues/463
"
1183	462	tnolet	2014-07-15 14:25:26	NONE	"closed
"
1184	464	thockin	2014-07-15 15:41:14	MEMBER	"LGTM except 1question and Travis failed..
"
1185	463	vmarmol	2014-07-15 15:53:49	CONTRIBUTOR	"Huh, I wonder why it downloads _all_ the cAdvisor images...I'll look into
fixing that since they're quite large at the moment.

Besides that, we will probably start running into cases where the Kubelet
will need to remove the images for you to make space.

On Tue, Jul 15, 2014 at 6:52 AM, Tim Nolet notifications@github.com wrote:

> Hi,
> 
> running the basic example / tutorial, you get stuck because the disk are
> full after you deploy the first real container (nginx or the redis master).
> After this, you have to log in to the nodes and start removing
> 
> tim_magnetic_io@kubernetes-minion-2:~$ sudo docker images
> REPOSITORY          TAG                   IMAGE ID            CREATED             VIRTUAL SIZE
> google/cadvisor     canary                ca3ee00db4f6        10 hours ago        317.6 MB
> google/cadvisor     beta                  b05fb52099db        10 hours ago        317.6 MB
> google/cadvisor     0.1.3                 3f695924c9b3        12 hours ago        317.6 MB
> google/cadvisor     0.1.2                 9fd09d2d9fc0        4 days ago          316.3 MB
> google/cadvisor     0.1.1                 13af514dd9c0        3 weeks ago         669.9 MB
> google/cadvisor     latest                13af514dd9c0        3 weeks ago         669.9 MB
> google/cadvisor     0.1.0                 54f91874541e        4 weeks ago         841.3 MB
> google/cadvisor     0.0.1                 8b068ee20e0f        4 weeks ago         359 MB
> busybox             buildroot-2013.08.1   d200959a3e91        5 weeks ago         2.489 MB
> busybox             ubuntu-14.04          37fca75d01ff        5 weeks ago         5.609 MB
> busybox             ubuntu-12.04          fd5373b3d938        5 weeks ago         5.455 MB
> busybox             buildroot-2014.02     a9eb17255234        5 weeks ago         2.433 MB
> busybox             latest                a9eb17255234        5 weeks ago         2.433 MB
> dockerfile/redis    latest                0a422c0cdf16        8 weeks ago         596.8 MB
> tim_magnetic_io@kubernetes-minion-2:~$ sudo df -h
> Filesystem                                              Size  Used Avail Use% Mounted on
> rootfs                                                  9.8G  9.3G     0 100% /
> udev                                                     10M     0   10M   0% /dev
> tmpfs                                                   171M  112K  171M   1% /run
> /dev/disk/by-uuid/3fb1905c-2e25-4854-8f22-a10196fc1e8f  9.8G  9.3G     0 100% /
> tmpfs                                                   5.0M     0  5.0M   0% /run/lock
> tmpfs                                                   341M  252K  341M   1% /run/shm
> cgroup                                                  853M     0  853M   0% /sys/fs/cgroup
> /dev/disk/by-uuid/3fb1905c-2e25-4854-8f22-a10196fc1e8f  9.8G  9.3G     0 100% /var/lib/docker/aufs
> none                                                    9.8G  9.3G     0 100% /var/lib/docker/aufs/mnt/ea68f82878445fb911de9e218859154fb55250d5b95d4620918a5fadb77e876a
> none                                                    9.8G  9.3G     0 100% /var/lib/docker/aufs/mnt/ddd921db4b257c09fc2877bc9abd4fcc7cd82957bdcf6f20299e4831e0c2a9c0
> none                                                    9.8G  9.3G     0 100% /var/lib/docker/aufs/mnt/6a65b87f2cd1e2d4f9c30b8ac75cecffb2bef8bfb89a9a76b5f556ded86ddf60
> none                                                    9.8G  9.3G     0 100% /var/lib/docker/aufs/mnt/25d49b91468b35863ffa76151de46736d96194efd8660fa8d621e02a74900eea
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/463.
"
1186	463	vmarmol	2014-07-15 15:54:48	CONTRIBUTOR	"Another thing I'd like to see is the output of:

sudo docker ps -a

if you don't mind. Some of the leftover containers take a bunch of space
too.

On Tue, Jul 15, 2014 at 8:53 AM, Victor Marmol vmarmol@google.com wrote:

> Huh, I wonder why it downloads _all_ the cAdvisor images...I'll look into
> fixing that since they're quite large at the moment.
> 
> Besides that, we will probably start running into cases where the Kubelet
> will need to remove the images for you to make space.
> 
> On Tue, Jul 15, 2014 at 6:52 AM, Tim Nolet notifications@github.com
> wrote:
> 
> > Hi,
> > 
> > running the basic example / tutorial, you get stuck because the disk are
> > full after you deploy the first real container (nginx or the redis master).
> > After this, you have to log in to the nodes and start removing
> > 
> > tim_magnetic_io@kubernetes-minion-2:~$ sudo docker images
> > REPOSITORY          TAG                   IMAGE ID            CREATED             VIRTUAL SIZE
> > google/cadvisor     canary                ca3ee00db4f6        10 hours ago        317.6 MB
> > google/cadvisor     beta                  b05fb52099db        10 hours ago        317.6 MB
> > google/cadvisor     0.1.3                 3f695924c9b3        12 hours ago        317.6 MB
> > google/cadvisor     0.1.2                 9fd09d2d9fc0        4 days ago          316.3 MB
> > google/cadvisor     0.1.1                 13af514dd9c0        3 weeks ago         669.9 MB
> > google/cadvisor     latest                13af514dd9c0        3 weeks ago         669.9 MB
> > google/cadvisor     0.1.0                 54f91874541e        4 weeks ago         841.3 MB
> > google/cadvisor     0.0.1                 8b068ee20e0f        4 weeks ago         359 MB
> > busybox             buildroot-2013.08.1   d200959a3e91        5 weeks ago         2.489 MB
> > busybox             ubuntu-14.04          37fca75d01ff        5 weeks ago         5.609 MB
> > busybox             ubuntu-12.04          fd5373b3d938        5 weeks ago         5.455 MB
> > busybox             buildroot-2014.02     a9eb17255234        5 weeks ago         2.433 MB
> > busybox             latest                a9eb17255234        5 weeks ago         2.433 MB
> > dockerfile/redis    latest                0a422c0cdf16        8 weeks ago         596.8 MB
> > tim_magnetic_io@kubernetes-minion-2:~$ sudo df -h
> > Filesystem                                              Size  Used Avail Use% Mounted on
> > rootfs                                                  9.8G  9.3G     0 100% /
> > udev                                                     10M     0   10M   0% /dev
> > tmpfs                                                   171M  112K  171M   1% /run
> > /dev/disk/by-uuid/3fb1905c-2e25-4854-8f22-a10196fc1e8f  9.8G  9.3G     0 100% /
> > tmpfs                                                   5.0M     0  5.0M   0% /run/lock
> > tmpfs                                                   341M  252K  341M   1% /run/shm
> > cgroup                                                  853M     0  853M   0% /sys/fs/cgroup
> > /dev/disk/by-uuid/3fb1905c-2e25-4854-8f22-a10196fc1e8f  9.8G  9.3G     0 100% /var/lib/docker/aufs
> > none                                                    9.8G  9.3G     0 100% /var/lib/docker/aufs/mnt/ea68f82878445fb911de9e218859154fb55250d5b95d4620918a5fadb77e876a
> > none                                                    9.8G  9.3G     0 100% /var/lib/docker/aufs/mnt/ddd921db4b257c09fc2877bc9abd4fcc7cd82957bdcf6f20299e4831e0c2a9c0
> > none                                                    9.8G  9.3G     0 100% /var/lib/docker/aufs/mnt/6a65b87f2cd1e2d4f9c30b8ac75cecffb2bef8bfb89a9a76b5f556ded86ddf60
> > none                                                    9.8G  9.3G     0 100% /var/lib/docker/aufs/mnt/25d49b91468b35863ffa76151de46736d96194efd8660fa8d621e02a74900eea
> > 
> > —
> > Reply to this email directly or view it on GitHub
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/463.
"
1187	459	thockin	2014-07-15 16:07:41	MEMBER	"Thanks for the fix!  Before we can merge this, we need you to please sign our CLA as described in CONTRIB.md (unless you have, in which case please give me more identifying information so I can find you on the list :)
"
1188	460	lavalamp	2014-07-15 16:31:56	MEMBER	"LGTM; needs rebase. Thanks for change!
"
1189	390	srobertson	2014-07-15 17:07:57	NONE	"Here's our usecase.

We have an app and we often want to run many different versions of  at the same time. For example production, staging and one off branches that a developer may want to spin up quickly for testing or demoing purposes. 

The app is light weight enough that it doesn't matter if it multiple versions are assigned to the same host.   Choosing  ports can be a pain and requires a high degree of coordination amongst those sharing the cluster.  

As a first pass I was planning/wanted to:

For each flavor of the app launch a pod with a coresponding  environment label set. We would set containerPort to 8000 but not set a host port.

Create an nginx  (or go app) pod/service listening on port 80 that maps virtual hosts to kupernetes api lookups and forwards traffic to the correct pod.

Setup DNS to point at  the services on port 80:

So these dns would all point to the same service on 80 which would find the right pods. 
prod.example.com
demo.example.com
foo.example.com 
"
1190	465	lavalamp	2014-07-15 17:16:11	MEMBER	"I'm still wrapping my head around this. I'll finish reading it later today.
"
1191	459	rrreeeyyy	2014-07-15 17:17:50	NONE	"@thockin Sorry, I just signed the individual contributor CLA electronically.  (github name: rrreeeyyy)
"
1192	463	tnolet	2014-07-15 17:21:00	NONE	"@vmarmol Sorry, I already deleted the instances. But I did do a docker ps -a and the only containers showing up were the ones you'd expect i.e. the one that were actually running or had ran.
"
1193	463	vmarmol	2014-07-15 17:24:55	CONTRIBUTOR	"I wonder if we should be deleting the containers after they exit. They do
take disk space. Although we may want to leave them around for some time
for people to examine their state.

Also sent out a PR:

https://github.com/GoogleCloudPlatform/kubernetes/pull/467

which should reduce disk usage and startup time.

On Tue, Jul 15, 2014 at 10:21 AM, Tim Nolet notifications@github.com
wrote:

> @vmarmol https://github.com/vmarmol Sorry, I already deleted the
> instances. But I did do a docker ps -a and the only containers showing up
> were the ones you'd expect i.e. the one that were actually running or had
> ran.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/463#issuecomment-49064504
> .
"
1194	463	tnolet	2014-07-15 17:28:20	NONE	"@vmarmol I guess leaving the containers around could be useful, but then again you wouldn't really want state in them. So you should externalise logging maybe (i.e. rsyslog, logstash, whatever) and all other forms of state you wish to inspect after the container stops. Easier said than done though.  
"
1195	458	lavalamp	2014-07-15 17:36:07	MEMBER	"Hi @yifan-gu, I'm terribly sorry but I've just realized we don't have a CLA from you. Can you sign one for us, please? (See CONTRIB.md) I've had to revert this in the meantime.
"
1196	466	lavalamp	2014-07-15 17:38:23	MEMBER	"Happy to see this stuff get separated!
"
1197	467	lavalamp	2014-07-15 17:39:05	MEMBER	"Thanks!
"
1198	168	erictune	2014-07-15 17:40:14	MEMBER	"Consider that the resource types and units used for pod/container requests could also be used for describing how to subdivide cluster resources (see https://github.com/GoogleCloudPlatform/kubernetes/issues/442 ).    For example, if team A is limited to using 10GB RAM at the cluster level, then team A can run 10 pods x 1GB RAM; or 2 pods x 5GB per pod; or some combination, etc. 
"
1199	469	lavalamp	2014-07-15 17:40:39	MEMBER	"LGTM, needs rebase now that I merged #469.
"
1200	390	bgrant0607	2014-07-15 17:51:34	MEMBER	"@srobertson Yes, all versions should be able to use the same containerPort. I'd definitely like to set up DNS, for both pods (#146) and services.

@yugui I could see the argument for automatic port allocation for services (rather than for containers), since the port is passed to clients using environment variables, anyway. However, I'd like to move towards an approach where we allocate an IP address for each service and then create DNS mappings for the services.
"
1201	471	bgrant0607	2014-07-15 18:29:16	MEMBER	"What does ""least load"" mean? Based on current resource usage? Or resource reservations? I'm assuming the former, since you mention cAdvisor. Will it take pod/container resource limits (#168) into account? We need to be careful about how we use cAdvisor's data.

There are potentially several issues to consider:
- resource limits
- min. required resources
- QoS (#147)
- degree of overcommitment
- node-level admission control policy (aka feasibility)

Also, which resources? Just cpu and memory for now?
"
1202	465	smarterclayton	2014-07-15 18:29:26	CONTRIBUTOR	"comments addressed
"
1203	356	smarterclayton	2014-07-15 18:30:17	CONTRIBUTOR	"@yifan-gu probably not in this pull given the scope - will do that as a separate pull.
"
1204	390	thockin	2014-07-15 18:38:35	MEMBER	"On Tue, Jul 15, 2014 at 10:08 AM, Scott Robertson
notifications@github.com wrote:

> Here's our usecase.
> 
> We have an app and we often want to run many different versions of at the same time. For example production, staging and one off branches that a developer may want to spin up quickly for testing or demoing purposes.
> 
> The app is light weight enough that it doesn't matter if it multiple versions are assigned to the same host. Choosing ports can be a pain and requires a high degree of coordination amongst those sharing the cluster.
> 
> As a first pass I was planning/wanted to:
> 
> For each flavor of the app launch a pod with a coresponding environment label set. We would set containerPort to 8000 but not set a host port.
> 
> Create an nginx (or go app) pod/service listening on port 80 that maps virtual hosts to kupernetes api lookups and forwards traffic to the correct pod.
> 
> Setup DNS to point at the services on port 80:
> 
> So these dns would all point to the same service on 80 which would find the right pods.
> prod.example.com
> demo.example.com
> foo.example.com

My main sentiment is that you should not have to do this - it should
just happen for you.
"
1205	459	thockin	2014-07-15 18:43:14	MEMBER	"Got it, thanks!
"
1206	456	brendandburns	2014-07-15 18:45:25	CONTRIBUTOR	"comments addressed.  ptal.

Thanks!
--brendan
"
1207	464	brendandburns	2014-07-15 18:46:17	CONTRIBUTOR	"the import _ isn't part of this PR, its something I added.  So let's not gate this PR on that.
"
1208	448	monnand	2014-07-15 18:53:19	CONTRIBUTOR	"Rebased.
"
1209	456	brendandburns	2014-07-15 18:56:36	CONTRIBUTOR	"oops, done.  ptal.

Thanks
--brendan
"
1210	472	monnand	2014-07-15 18:56:49	CONTRIBUTOR	"Will apiserver communicate with kubelet? I think currently it does.
"
1211	471	monnand	2014-07-15 19:04:38	CONTRIBUTOR	"@bgrant0607 Currently, cAdvisor could show us the most recent stats and other information like ""What's the 90th percentile of CPU usage of a certain container/machine?"" I think my first PR will solely based on current resource usage without considering the resource limits. But I will add resource limits later. (Just to make reviewer's work easier.)

Yes, just CPU and memory for now. I think we will track more resources in cAdvisor.
"
1212	472	lavalamp	2014-07-15 19:09:08	MEMBER	"Thanks, that reminded me to file #473.
"
1213	472	lavalamp	2014-07-15 19:12:56	MEMBER	"Corrected diagram to show apiserver->kublet communication path.
"
1214	457	dchen1107	2014-07-15 20:00:12	MEMBER	"The PR is ready for review now. Ping @lavalamp @thockin @brendandburns Thanks!
"
1215	472	dchen1107	2014-07-15 20:23:17	MEMBER	"s/kublet/kubelet in your diagram.
"
1216	458	yifan-gu	2014-07-15 20:25:31	MEMBER	"@lavalamp NP! Done!
"
1217	472	dchen1107	2014-07-15 20:27:43	MEMBER	"Also on each minion, there is a simple version proxy which is not in your diagram.
"
1218	472	lavalamp	2014-07-15 20:58:28	MEMBER	"Fixed spelling, added proxy.
"
1219	448	monnand	2014-07-15 21:09:10	CONTRIBUTOR	"Just updated cAdvisor with most recent changes.

The PR is ready for review now. Ping @lavalamp @thockin @brendandburns Thanks!
"
1220	274	johnwilkes	2014-07-15 21:43:49	CONTRIBUTOR	"related #168 

(FYI, labels are designed for user stuff, not system-provided information, so resource quantities aren't a good match for them.)
"
1221	458	thockin	2014-07-15 21:47:17	MEMBER	"Confirmed.  I have never done a rollback on github - do you need to re-offer the PR?
"
1222	472	brendandburns	2014-07-15 21:59:58	CONTRIBUTOR	"Can we export as SVG (better yet, can we use Inkscape instead of dia?)

That way we can render it on the web.
"
1223	458	yifan-gu	2014-07-15 22:01:27	MEMBER	"@thockin Sorry for this inconvenience. Do you mean to create a new PR for the this?
"
1224	472	thockin	2014-07-15 22:06:49	MEMBER	"Dia can save as SVG.  We should probably check in both PNG and SVG.
"
1225	472	lavalamp	2014-07-15 22:08:31	MEMBER	"Happy to put in an SVG. I found Inkscape totally unusable for diagram making.
"
1226	474	smarterclayton	2014-07-15 22:12:06	CONTRIBUTOR	"LGTM
"
1227	456	brendandburns	2014-07-15 22:17:37	CONTRIBUTOR	"Comments addressed.  ptal.

Thanks
--brendan
"
1228	448	monnand	2014-07-15 22:44:59	CONTRIBUTOR	"Add another comment to retrieve machine spec from cAdvisor. We may need to consider to rename some functions.
"
1229	274	monnand	2014-07-15 22:52:57	CONTRIBUTOR	"I'm currently working in #471. Once we land #448, k8s master would be able to use full information provided by cAdvisor (container stats, machine specs). With such information, I will build a simple least load scheduler using information provided by cAdvisor.

@vmarmol I think cAdvisor needs lmctfy to read root container's information now (  google/cadvisor#20 ). Since our docker image contains lmctfy, it may not be a big problem.
"
1230	472	lavalamp	2014-07-15 22:53:31	MEMBER	"Added SVG.
"
1231	475	brendandburns	2014-07-15 23:00:28	CONTRIBUTOR	"LGTM
"
1232	475	brendandburns	2014-07-15 23:00:37	CONTRIBUTOR	"(except the build/test fail ;)
"
1233	476	brendandburns	2014-07-15 23:08:50	CONTRIBUTOR	"Fixed merge problems.
"
1234	476	lavalamp	2014-07-15 23:10:20	MEMBER	"Initial thought: This duplicates the Encode/Decode functions, right? I want to think about that.
"
1235	472	brendandburns	2014-07-15 23:10:58	CONTRIBUTOR	"Why are the kubelet/cAdvisor/proxy inside the ""Docker"" box on the minion?
"
1236	472	brendandburns	2014-07-15 23:11:56	CONTRIBUTOR	"Also, please wrap etcd in a ""watchable storage API"" or some such, to indicate that we think it may be pluggable in the future.
"
1237	476	thockin	2014-07-15 23:12:54	MEMBER	"I will look at it this evening, promise.

On Tue, Jul 15, 2014 at 4:10 PM, Daniel Smith notifications@github.com
wrote:

> Initial thought: This duplicates the Encode/Decode functions, right? I
> want to think about that.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/476#issuecomment-49104965
> .
"
1238	472	robszumski	2014-07-15 23:13:42	CONTRIBUTOR	"Should the same be done for docker?
"
1239	472	lavalamp	2014-07-15 23:15:54	MEMBER	"> Why are the kubelet/cAdvisor/proxy inside the ""Docker"" box on the minion?

Because they all run (or should run) as docker containers.
"
1240	465	brendandburns	2014-07-15 23:20:56	CONTRIBUTOR	"Small stuff, basically LGTM, modulo using defer ... for all of the unlocks.
"
1241	477	monnand	2014-07-15 23:22:59	CONTRIBUTOR	"It would be so much easier for users if we could replace these scripts with Go programs. I was intended to do it but do not have time. Thank you!

Is it possible to make ssh and docker be the only requirements for k8s? In this case, we could use the [ssh package](https://godoc.org/code.google.com/p/go.crypto/ssh) and do all config/deployment work through ssh within a Go program. (Currently, we are using salt to do all installation.)
"
1242	478	monnand	2014-07-15 23:24:17	CONTRIBUTOR	"BTW, please do not merge this PR. It is not ready yet.
"
1243	476	brendandburns	2014-07-15 23:28:25	CONTRIBUTOR	"It sort of duplicates encode/decode, but it's actually different.  Eventually the conversion code will actually populate different objects (e.g. move/convert/create field values)

Encode/Decode = to/from wire format.

This is more like Versioned struct to Unversioned struct, so object to object transformations.
"
1244	476	brendandburns	2014-07-15 23:30:50	CONTRIBUTOR	"It looks huge, but its actually not that big.  Most of it is straight duplication from api -> internal, and then a bunch of path modifications.

(you can't do the equivalent of g4 cp, so you don't get clean diffs for the forked files.)

--brendan
"
1245	479	vmarmol	2014-07-15 23:52:33	CONTRIBUTOR	"LGTM
"
1246	479	lavalamp	2014-07-15 23:52:50	MEMBER	"LGTM
"
1247	188	smarterclayton	2014-07-16 00:06:04	CONTRIBUTOR	"@ironcladlou is also looking at OpenVSwitch and OpenDaylight integrations - making it easier to deploy these sorts of topologies on non-cloud infrastructure (or clouds with limited networking).
"
1248	479	thockin	2014-07-16 00:24:23	MEMBER	"looking now
"
1249	459	rrreeeyyy	2014-07-16 00:30:28	NONE	"@thockin thank you for merge!
"
1250	478	lavalamp	2014-07-16 00:35:16	MEMBER	"I will review by tomorrow EOD
"
1251	448	lavalamp	2014-07-16 00:37:26	MEMBER	"This is a really big and hard to review PR. Can we split it up some? 
"
1252	448	monnand	2014-07-16 01:01:31	CONTRIBUTOR	"Sure. I'm going to split it into three parts:
- Update kubelet package to use most recent cAdvisor's code
- Add code in client package to use kubelet's API to retrieve data from cAdvisor
- Add code to retrieve machine specs from cAdvisor.
"
1253	475	lavalamp	2014-07-16 01:08:34	MEMBER	"Should pass now. Added a unit test while I was at it.
"
1254	448	monnand	2014-07-16 01:08:40	CONTRIBUTOR	"@lavalamp Sent #480.
"
1255	478	monnand	2014-07-16 01:15:44	CONTRIBUTOR	"Right now, the basic structure is there. I will add some test tomorrow. But the current code itself is ready for review.

It is still a very simple scheduler without considering many corner cases.
"
1256	472	lavalamp	2014-07-16 01:19:18	MEMBER	"Added disclaimer to etcd.
"
1257	477	lavalamp	2014-07-16 01:23:32	MEMBER	"I dislike shell scripts, so I could probably get on board with this. 
"
1258	480	lavalamp	2014-07-16 01:25:36	MEMBER	"Thanks for splitting this up.
"
1259	406	yugui	2014-07-16 01:38:54	CONTRIBUTOR	"It was merged as a part of #407
"
1260	448	monnand	2014-07-16 03:41:36	CONTRIBUTOR	"@lavalamp Sent #482 
"
1261	472	thockin	2014-07-16 04:02:14	MEMBER	"What is ""info service""?

I was expecting to see the three-tiers we discussed yesterday: apiserver ->(storage)->scheduler->actuator->(storage)->kubelet
"
1262	477	brendandburns	2014-07-16 04:03:49	CONTRIBUTOR	"pretty please no.  If all you are doing is writing a go binary that ends up
shelling out to executables, what is the point?  Our time is _way_ better
spent on fixing actual problems.

--brendan

On Tue, Jul 15, 2014 at 6:23 PM, Daniel Smith notifications@github.com
wrote:

> I dislike shell scripts, so I could probably get on board with this.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/477#issuecomment-49113471
> .
"
1263	477	thockin	2014-07-16 04:09:05	MEMBER	"I was assuming the proposal was to use cloud-provider APis rather than
CLIs.  I agree that shelling out to CLIs is not more useful than a script.
 I am ambivalent about direct API use vs a script.

Scripts ain't so bad, in my book.

On Tue, Jul 15, 2014 at 9:03 PM, brendandburns notifications@github.com
wrote:

> pretty please no. If all you are doing is writing a go binary that ends up
> shelling out to executables, what is the point? Our time is _way_ better
> spent on fixing actual problems.
> 
> --brendan
> 
> On Tue, Jul 15, 2014 at 6:23 PM, Daniel Smith notifications@github.com
> wrote:
> 
> > I dislike shell scripts, so I could probably get on board with this.
> > 
> > ## 
> > 
> > Reply to this email directly or view it on GitHub
> > <
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/477#issuecomment-49113471>
> > 
> > .
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/477#issuecomment-49121433
> .
"
1264	482	lavalamp	2014-07-16 04:10:47	MEMBER	"There seems to be an update to cAdvisor mixed in with your changes? Does this need a rebase or something? Our dependency updating script should make separate commits when updating a dependency?
"
1265	475	thockin	2014-07-16 04:15:14	MEMBER	"We can’t automatically merge this pull request.
"
1266	482	monnand	2014-07-16 04:17:42	CONTRIBUTOR	"@lavalamp Ah. Yes, I squashed all commits. I thought it was the default way.

I think we don't need a rebase. I have already done it on my local machine.

I did not use the updating script. (Not sure when we added this script. I thought there's only deps.sh.)
"
1267	472	lavalamp	2014-07-16 04:21:00	MEMBER	"> What is ""info service""?

The hypothetical successor to the podcache that won't spam every kubelet from every apiserver.

> I was expecting to see the three-tiers we discussed yesterday: apiserver ->(storage)->scheduler->actuator->(storage)->kubelet

Hm, I understood something a little different; my understanding was that scheduler talks only to apiserver, not directly to storage, and I understood the kubelet to be the actuator. Let's chat next to a whiteboard tomorrow.
"
1268	481	brendandburns	2014-07-16 04:25:16	CONTRIBUTOR	"Couple of small comments but basically LGTM.
"
1269	475	lavalamp	2014-07-16 04:26:10	MEMBER	"Rebased.
"
1270	482	brendandburns	2014-07-16 04:32:20	CONTRIBUTOR	"LGTM, modulo the one small comment.
"
1271	482	lavalamp	2014-07-16 04:37:01	MEMBER	"Let's keep the dependency updating separate commits in the future-- makes it easier to see just your changes in the github interface. :)
"
1272	463	brendandburns	2014-07-16 04:37:12	CONTRIBUTOR	"There is a separate entry for GC-ing old containers, but I'm not sure that it would work in the timeframe where you saw a problem here.
"
1273	477	monnand	2014-07-16 05:00:05	CONTRIBUTOR	"This may be out of topic. But I do think a smoother work flow for end users would be an important thing for an open source project.

I agree that this issue may not be the most important feature we want. However, as a k8s user, I would like to see a smoother work flow to deploy k8s and start/stop pods on my k8s cluster. As a user, I personally do not want to put several intertwined shell scripts from a third party into any directory in my PATH.

To me, here is an ideal work flow for an end user:
- To install a k8s system on my cluster, I need to first download an executable on my machine and put it into my PATH:

```
# download an executable, say kubectrl, from some website.
$ wget https://some.server/kubectrl
# put kubectrl into my PATH
$ cp kubectrl ~/bin
```
- On my machine, `kubectrl` is the _only_ executable that I need to install. After that, I would like to start some VMs, either on GCE or other platforms, or even on my physical machines. I think it would be fine to assume that all VMs has an ssh server and a docker daemon. And those are only things we need for k8s.
- Then I would need to install kubelet, kubemaster, etcd, cAdvisor, etc. on my VMs:

```
# kubectrl will use Go's ssh library to execute commands to download and deploy all stuff.
# There may be other flags like ssh port, etc.
# It is also fine for me to put all such information into a yaml config file.
$ kubectrl deploy -master master-ip -minions ""minion-1,minion-2"" -ssh-key somekey.pem 
```
- Note that in this (my own) ideal universe, an end user do not need to download all things on to his local machine. `kubectrl` would download things directly onto VMs by sending commands through ssh.
- To start a pod, I could use the `kubectrl` command again:

```
$ kubectrl up pod.yaml
# Or stop a pod
$ kubectrl down podId
```

Again, it is true that functions mentioned above are already implemented in several scripts/programs now. However, for an end user with little knowledge about how k8s works, the work flow I mentioned above may be better for him to get started. Of course he needs to understand more about k8s internally, but he does not necessary to remember all different programs/scripts names inside k8s.

Besides, as developers of k8s, if we want to support more platforms (I believe we want), we may need to reduce platform-specific code and put those code into limited files. It may be hard for shell scripts when things getting large. IMHO, It would be way much easier if we could maintain a Go program instead of a bunch of shell scripts.

[Here](www.infoworld.com/t/paas/google-plunges-in-docker-management -244109) is an outside news describing k8s. I was exciting to see it but disappointed when I saw the following lines:

> While Kubernetes can theoretically run outside of Google Compute Engine, that isn't possible yet. 

Well. It's not in theory, I once set up a k8s cluster manually. The only reason we cannot support other platforms is the shell scripts which depends on gcutils.

Here is a summary: In my opinion, a separate Go program would be:
- Easier to maintain and extend, so that we could support other platforms easily.
- Friendly to end users: It's just a statically linked executable and could be copied around. As I said, I do not want to put several third party shell scripts into my PATH.
- Smoother work flow for users.

Thoughts?
"
1274	476	thockin	2014-07-16 05:04:11	MEMBER	"Done reviewing.  LGTM modulo comments.
"
1275	477	thockin	2014-07-16 05:10:22	MEMBER	"You're arguing for being a direct consumer of the APIs, which is fine.  I'm
ambivalent about it, and that is not what Brendan recoiled from, as I
understand.

On Tue, Jul 15, 2014 at 10:00 PM, monnand notifications@github.com wrote:

> This may be out of topic. But I do think a smoother work flow for end
> users would be an important thing for an open source project.
> 
> I agree that this issue may not be the most important feature we want.
> However, as a k8s user, I would like to see a smoother work flow to deploy
> k8s and start/stop pods on my k8s cluster. As a user, I personally do not
> want to put several intertwined shell scripts from a third party into any
> directory in my PATH.
> 
> To me, here is an ideal work flow for an end user:
> - To install a k8s system on my cluster, I need to first download an
>   executable on my machine and put it into my PATH:
> 
> # download an executable, say kubectrl, from some website.
> 
> $ wget https://some.server/kubectrl
> 
> # put kubectrl into my PATH
> 
> $ cp kubectrl ~/bin
> - On my machine, kubectrl is the _only_ executable that I need to
>   install. After that, I would like to start some VMs, either on GCE or other
>   platform. I think it would be fine to assume that all VMs has an ssh server
>   and a docker daemon. And those are only thing we need for k8s.
> - Then I would need to install kubelet, kubemaster, etcd, cAdvisor,
>   etc. on my VMs:
> 
> # There may be other flags like ssh port, etc.
> 
> # It is also fine for me to put all such information into a yaml config file.
> 
> # kubectrl will use Go's ssh library to execute commands to download and deploy all stuff.
> 
> $ kubectrl deploy -master master-ip -minions ""minion-1,minion-2"" -ssh-key somekey.pem
> - Note that in this (my own) ideal universe, an end user do not need
>   to download all things on to his local machine.
> - To start a pod, I could use the kubectrl command again:
> 
> $ kubectrl up pod.yaml
> 
> # Or stop a pod
> 
> $ kubectrl down podId
> 
> Again, it is true that functions mentioned above are already implemented
> in several scripts/programs now. However, for an end user with little
> knowledge about how k8s works, the work flow I mentioned above may be
> better for him to get started. Of course he needs to understand more about
> k8s internally, but he does not necessary to remember all different
> programs/scripts names inside k8s.
> 
> Besides, as developers of k8s, if we want to support more platforms (I
> believe we want), we may need to reduce platform-specific code and put
> those code into limited files. It may be hard for shell scripts when things
> getting large. IMHO, It would be way much easier if we could maintain a Go
> program instead of a bunch of shell scripts.
> 
> Here
> http://www.infoworld.com/t/paas/google-plunges-in-docker-management%20-244109
> is an outside news describing k8s. I was exciting to see it but
> disappointed when I saw the following lines:
> 
> While Kubernetes can theoretically run outside of Google Compute Engine,
> that isn't possible yet.
> 
> Well. It's not in theory, I once set up a k8s cluster manually. The only
> reason we cannot support other platforms is the shell scripts which depends
> on gcutils.
> 
> Here is a summary: In my opinion, a separate Go program would be:
> - Easier to maintain and extend, so that we could support other
>   platforms easily.
> - Friendly to end users: It's just a statically linked executable and
>   could be copied around. As I said, I do not want to put several third party
>   shell scripts into my PATH.
> - Smoother work flow for users.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/477#issuecomment-49123803
> .
"
1276	482	monnand	2014-07-16 05:11:09	CONTRIBUTOR	"@brendandburns Thank you! Comment addressed. PTAL.
@lavalamp OK. I'll do it in the future (However, I may not be able to do it in the third piece of #448 because those commits are already intertwined with dependency changes. Sorry about that.)
"
1277	472	thockin	2014-07-16 05:15:43	MEMBER	"I understood:

user tells API server ""I want a pod""

API server saves that to storage and ACKs

scheduler sees that a pod needs scheduling and chooses a host

scheduler calls a new actuator API that says ""put this pod on that machine""

actuator API writes to host-specific storage

kubelet watches host-specific storage

On Tue, Jul 15, 2014 at 9:21 PM, Daniel Smith notifications@github.com
wrote:

> What is ""info service""?
> 
> The hypothetical successor to the podcache that won't spam every kubelet
> from every apiserver.
> 
> I was expecting to see the three-tiers we discussed yesterday: apiserver
> ->(storage)->scheduler->actuator->(storage)->kubelet
> 
> Hm, I understood something a little different; my understanding was that
> scheduler talks only to apiserver, not directly to storage, and I
> understood the kubelet to be the actuator. Let's chat next to a whiteboard
> tomorrow.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/472#issuecomment-49122160
> .
"
1278	188	lexlapax	2014-07-16 05:18:10	NONE	"we did some work on docker openvswitch as proof of concept.. would be interested in those integrations -- i would even venture to say that it will be of interest to cloud providers who are looking to provide higher abstraction services ala,  container / pod deployment in lieu of/in addition to plain IaaS abstractions. 
"
1279	472	lavalamp	2014-07-16 05:19:33	MEMBER	"Ah! OK, yeah, that's my understanding, too, just with the additional detail that the actuation api would be served by apiserver. I wasn't showing the internals of apiserver, maybe I should make another diagram for that.
"
1280	472	thockin	2014-07-16 05:21:14	MEMBER	"I think the fact that it is internal is an implementation detail.  it's an
API and therefore plugable.

On Tue, Jul 15, 2014 at 10:19 PM, Daniel Smith notifications@github.com
wrote:

> Ah! OK, yeah, that's my understanding, too, just with the additional
> detail that the actuation api would be served by apiserver. I wasn't
> showing the internals of apiserver, maybe I should make another diagram for
> that.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/472#issuecomment-49124600
> .
"
1281	477	brendandburns	2014-07-16 05:22:35	CONTRIBUTOR	"Also, what you suggest is not possible without a binary build, and once we
have a binary build, the scripts that create the install will be trivially
small.

--brendan

On Tue, Jul 15, 2014 at 10:10 PM, Tim Hockin notifications@github.com
wrote:

> You're arguing for being a direct consumer of the APIs, which is fine. I'm
> ambivalent about it, and that is not what Brendan recoiled from, as I
> understand.
> 
> On Tue, Jul 15, 2014 at 10:00 PM, monnand notifications@github.com
> wrote:
> 
> > This may be out of topic. But I do think a smoother work flow for end
> > users would be an important thing for an open source project.
> > 
> > I agree that this issue may not be the most important feature we want.
> > However, as a k8s user, I would like to see a smoother work flow to
> > deploy
> > k8s and start/stop pods on my k8s cluster. As a user, I personally do not
> > want to put several intertwined shell scripts from a third party into any
> > directory in my PATH.
> > 
> > To me, here is an ideal work flow for an end user:
> > - To install a k8s system on my cluster, I need to first download an
> > 
> > executable on my machine and put it into my PATH:
> > 
> > # download an executable, say kubectrl, from some website.
> > 
> > $ wget https://some.server/kubectrl
> > 
> > # put kubectrl into my PATH
> > 
> > $ cp kubectrl ~/bin
> > - On my machine, kubectrl is the _only_ executable that I need to
> > 
> > install. After that, I would like to start some VMs, either on GCE or
> > other
> > platform. I think it would be fine to assume that all VMs has an ssh
> > server
> > and a docker daemon. And those are only thing we need for k8s.
> > - Then I would need to install kubelet, kubemaster, etcd, cAdvisor,
> > 
> > etc. on my VMs:
> > 
> > # There may be other flags like ssh port, etc.
> > 
> > # It is also fine for me to put all such information into a yaml config
> > 
> > file.
> > 
> > # kubectrl will use Go's ssh library to execute commands to download and
> > 
> > deploy all stuff.
> > $ kubectrl deploy -master master-ip -minions ""minion-1,minion-2""
> > -ssh-key somekey.pem
> > - Note that in this (my own) ideal universe, an end user do not need
> >   to download all things on to his local machine.
> > - To start a pod, I could use the kubectrl command again:
> > 
> > $ kubectrl up pod.yaml
> > 
> > # Or stop a pod
> > 
> > $ kubectrl down podId
> > 
> > Again, it is true that functions mentioned above are already implemented
> > in several scripts/programs now. However, for an end user with little
> > knowledge about how k8s works, the work flow I mentioned above may be
> > better for him to get started. Of course he needs to understand more
> > about
> > k8s internally, but he does not necessary to remember all different
> > programs/scripts names inside k8s.
> > 
> > Besides, as developers of k8s, if we want to support more platforms (I
> > believe we want), we may need to reduce platform-specific code and put
> > those code into limited files. It may be hard for shell scripts when
> > things
> > getting large. IMHO, It would be way much easier if we could maintain a
> > Go
> > program instead of a bunch of shell scripts.
> > 
> > Here
> > <
> > http://www.infoworld.com/t/paas/google-plunges-in-docker-management%20-244109
> > 
> > is an outside news describing k8s. I was exciting to see it but
> > disappointed when I saw the following lines:
> > 
> > While Kubernetes can theoretically run outside of Google Compute Engine,
> > that isn't possible yet.
> > 
> > Well. It's not in theory, I once set up a k8s cluster manually. The only
> > reason we cannot support other platforms is the shell scripts which
> > depends
> > on gcutils.
> > 
> > Here is a summary: In my opinion, a separate Go program would be:
> > - Easier to maintain and extend, so that we could support other
> >   platforms easily.
> > - Friendly to end users: It's just a statically linked executable and
> > 
> > could be copied around. As I said, I do not want to put several third
> > party
> > shell scripts into my PATH.
> > - Smoother work flow for users.
> > 
> > ## 
> > 
> > Reply to this email directly or view it on GitHub
> > <
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/477#issuecomment-49123803
> > 
> > .
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/477#issuecomment-49124216
> .
"
1282	442	thockin	2014-07-16 05:23:08	MEMBER	"Clayton,

Not ignoring you, just buried in email.  This is an interesting topic.
 You've touched on a number of things we do internally, but I am not an
expert in.  Will come back to this topic soon with some expertise in tow.

On Sun, Jul 13, 2014 at 2:15 PM, Clayton Coleman notifications@github.com
wrote:

> This is subdivision of total cluster resources across one or more tenants,
> vs subdivision of resources in a container.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/442#issuecomment-48852127
> .
"
1283	483	lavalamp	2014-07-16 05:39:07	MEMBER	"LGTM modulo comments.
"
1284	481	bgrant0607	2014-07-16 05:54:57	MEMBER	"Addressed comments. PTAL.
"
1285	460	nyaxt	2014-07-16 06:59:18	CONTRIBUTOR	"Thanks for review! Rebased.
"
1286	477	yugui	2014-07-16 13:24:38	CONTRIBUTOR	"> Scripts ain't so bad, in my book.

I don't think so. At least, _shell_ scripts are.

As I wrote in the objective, I am thinking of setting up VMs as a part of a setup command in our team.
On the other hand, for example, it is hard for me to setup 100 minions in parallel with the shell script.
It would give me much more control if we have a Go function like CreateMaster() or CreateMinion().

For the point of API vs CLI, actually I was thinking of implementing it as a wrapper of gcutil at first, and replacing it with APIs in the second phase.
This is because I already have a Go wrapper library of gcutil, ant it is relatively easy to port the library for Kubernetes.
"
1287	477	smarterclayton	2014-07-16 14:39:21	CONTRIBUTOR	"In general the process of creating masters and minions in ""traditional"" operational environments seems like it's going to be something that you can offer a best practice for, but there will never be a one size fits all solution.  Teams will use ansible, or puppet, or baked images, or run Kube-on-Kubelet (my preference for masters), or a hundred options.  Go libraries would be mostly useless to them (specifically, they can't code it and it isn't in their skill set) - they wouldn't want Go to be controlling it, they'd have a process that instead invokes the specific components.  That doesn't mean they wouldn't take advantage of those tools, just that they aren't critical path for them.

The assumption our OpenShift Online ops team and some of our customers who are on IaaS have raised is (this is for a multi-tenant style deployment, which is partially where we want to help take Kube):
- Out of the box, be able to easily compose an image that represents the ideal minion setup with a known version of the minion, a set of prebaked scripts and configuration for various minion related tasks
- Use cloud-init or similar to launch a new VM from their IaaS with just enough info in the init script to have it be able to securely auto join the cluster (that may involve a secret key to allow minion self registration) and to also pull any necessary configuration from etcd
- Ensure some minions are booted in an ""infrastructure zone"" - isolated from tenant applications
- Bootstrap a kube master and etcd on a single one of those infrastructure minions
- Use that first master to allocate the HA config (including any balancers, proxies, etc that are easily containerize)

This is just one perspective from a particularly biased source.  I think there are real advantages in emphasizing the reusability and composability of kube itself such that IaaS + Kube give you a real container hosting environment, and the tools available to the iaas and Kube both play together to benefit admins and consumers.
"
1288	477	thockin	2014-07-16 15:21:08	MEMBER	"On Wed, Jul 16, 2014 at 6:24 AM, Yuki Yugui Sonoda
notifications@github.com wrote:

> > Scripts ain't so bad, in my book.
> 
> I don't think so. At least, shell scripts are.

But why?  Because you don't like shell programming?  Or is there
actually some deeper problem I am not seeing

> As I wrote in the objective, I am thinking of setting up VMs as a part of a setup command in our team.
> On the other hand, for example, it is hard for me to setup 100 minions in parallel with the shell script.
> It would give me much more control if we have a Go function like CreateMaster() or CreateMinion().

Why is it hard?  It's not complicated to spawn work in parallel in
shell.  It's certainly not as pretty as a ""real"" language, but it
works and is very composable.  A shell script runs on pretty much any
platform and has the benefit of being easily inspectable and hackable
for whatever environment.

> For the point of API vs CLI, actually I was thinking of implementing it as a wrapper of gcutil at first, and replacing it with APIs in the second phase.
> This is because I already have a Go wrapper library of gcutil, ant it is relatively easy to port the library for Kubernetes.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub.
"
1289	488	thockin	2014-07-16 15:30:38	MEMBER	"Thanks for the fix.  Have you signed the CLA (as per CONTRIB.md)?  I can't find your name on the list.
"
1290	488	byxorna	2014-07-16 15:37:16	CONTRIBUTOR	"@thockin Sorry, I missed that. Just signed the CLA electronically.
"
1291	487	brendandburns	2014-07-16 16:43:54	CONTRIBUTOR	"Thanks for the PR!

Generally LGTM, small stuff.
"
1292	488	thockin	2014-07-16 16:44:39	MEMBER	"Thanks!
"
1293	481	lavalamp	2014-07-16 16:51:20	MEMBER	"LGTM. Can you squash the commits?
"
1294	481	brendandburns	2014-07-16 16:53:01	CONTRIBUTOR	"`git rebase -i HEAD~4` (in case you hadn't squashed commits before)
"
1295	472	brendandburns	2014-07-16 16:54:30	CONTRIBUTOR	"We've had this discussion before, I thought the consensus was that the kubelet would run outside of a container (I think this is the right approach, ""who watches the watcher?"")

--brendan
"
1296	472	thockin	2014-07-16 17:15:56	MEMBER	"I have not had this discussion, and I'm not sure this is the right thread.

If _anything_ substantive runs outside of containers then it is very difficult to make guarantees about QoS.

What watches the watcher?  The kernel and init do.
"
1297	483	brendandburns	2014-07-16 17:17:42	CONTRIBUTOR	"Comments addressed.  ptal.

Thanks!
--brendan
"
1298	465	brendandburns	2014-07-16 17:19:37	CONTRIBUTOR	"LGTM.  @lavalamp want to take one last look?
"
1299	483	xiang90	2014-07-16 17:20:19	CONTRIBUTOR	"lgtm 
@brendandburns  It will be helpful, if you add a test for the cache.Insert/remove.
"
1300	469	smarterclayton	2014-07-16 17:20:32	CONTRIBUTOR	"rebased
"
1301	484	brendandburns	2014-07-16 17:21:58	CONTRIBUTOR	"I like this idea, and the idea of making more sophisticated Service objects in general.

Basically, you would bind a Service to a ReplicationController and if size of that replication controller was zero, you'd spin up a new container, and then make the connection.

What do others think?

Do you want to send us a PR?
"
1302	472	lavalamp	2014-07-16 17:23:52	MEMBER	"Here's a version with Tim's requested changes. I can move kubelet out of the docker square if that's the consensus.
"
1303	477	brendandburns	2014-07-16 17:27:35	CONTRIBUTOR	"And indeed gcutil supports parallel requests, so you don't even have to make the shell script do the parallelization, GCE can do it for you.

I agree w/ Clayton's comments that people are going to find a way of doing an install that they prefer (Salt, Puppet, Chef, containers, ...)

Shell scripts are a good least-common-denominator, that sysadmins (the people who are going to be doing this kind of installation stuff) are familiar with.

Even if you were going to program it into a language/library, the appropriate language for that community is likely Python, not Go.

Also, in general, I think the goal is to simplify the installation process so that it is only a couple of instructions, so writing a program to do the install is largely redundant.

I'm going to close this issue for now.  We'd certainly entertain a PR that does this, say in a ""contrib"" directory, but I don't want to replace kube-up.sh, kube-down.sh with go programs. 
"
1304	489	xiang90	2014-07-16 17:30:31	CONTRIBUTOR	"@thockin I do not know if i understand this correctly.
The targeted states store in etcd and the fine-grained progress will have checkpointing locally? So when the Kubelet restarts it can load the checkpoint first and go from there towards the targeted states that stored in etcd?
"
1305	489	thockin	2014-07-16 17:31:28	MEMBER	"@xiangli-cmu more or less.
"
1306	489	xiang90	2014-07-16 17:34:27	CONTRIBUTOR	"@thockin The assumption is that the checkpoint file will never be corrupted? or each subsystem still need to prepare for recovering from a inconsistent state.
"
1307	472	dchen1107	2014-07-16 17:36:54	MEMBER	"I just added one more work item to Tim's spreadsheet related to kubelet and other daemons's containment, and saw discussion here. I agree with Tim, even we don't want to enforce the limit at the first for all daemons, we should have their resource usages / stats.  
"
1308	489	thockin	2014-07-16 17:37:43	MEMBER	"That was my last point.  If you believe that checkpoint corruption is rare enough, you can simply say that it counts as a catastrophic failure, and pods will be destroyed and the state of the host may become suspect.

But there are some things that are just NOT stored anywhere intrinsic - what do we do with those?
"
1309	465	lavalamp	2014-07-16 17:38:55	MEMBER	"On it, ETA ~hours
"
1310	489	lavalamp	2014-07-16 17:43:53	MEMBER	"We may want to define what we mean by ""checkpoint""; I'd never encountered our usage prior to joining <internal google team>.
"
1311	442	erictune	2014-07-16 17:43:53	MEMBER	"Clayton:  
Here is my interpretation of your two uses cases, from the top of your original post. (I've reversed the order).  I'm holding off on commenting on the subsequent paragraphs of your proposal while we discuss the first part.
- Prevent exhaustion of basic kubelet resources (cores, ram, disk bytes, NIC b/w, etc).  Allow projects to be assured that resources will be available in the future even if not in use at the moment.  Analogy is Unix disk quota (with just one filesystem): usage and limit per user.  So, I'll call this  _Quota_.  
-  Prevent one team's pods from being ""near"" any other team's pods.  I'll call this _Exclusion_. Reasons include:
  - teams feel a sense of ownership of resource (""We bought these machines that were added to the cluster, so we get to use them"")
  - to get isolation from effects of other team's pods (e.g. processor cache interference)
  - because company or third-party policies require such a separation
  - to get access to specific hardware or machines with specific host OS tuning on them.

Assuming I got those uses cases right, here are a few thoughts:

Quota can possibly be done at a very fine granularity (fractions of cores, ~kB of RAM, etc).   Exclusion typically happens at a number of coarser granularities (core, VM, physical machine, rack, site, etc). 

Since Quota can be fine grained, it is easy to assign just the right amount to a project.
Since exclusion is coarser grained, waste due to rounding errors is more likely.

One project's quota can be represented concisely (e.g. one integer each for ram, compute, disk).  This representation has additive properties, so it is convenient for producing accounting reports. It can be traded among teams, so there can be an economy of resources.   Exclusion has a more complex representation (list of labels or of physical machine names, etc).   An economy of exclusion-resources sometimes has too much friction.

A basic type of exclusion can be implemented by saying ""run your own k8s cluster if you want to be separate"".  You can get a long ways with a combination of quota and multiple k8s clusters.  This does imply a need to name k8s clusters.  But this is useful for other reasons.
"
1312	489	thockin	2014-07-16 17:45:04	MEMBER	"Sorry.  Checkpoint == ""write something to stable storage (and fsync() it) such that I can recover it the next time my app starts and re-build some in-memory state.""
"
1313	484	KyleAMathews	2014-07-16 17:46:47	CONTRIBUTOR	"I'm a Go neophyte plus getting a startup going so I'm probably not the right person to tackle this atm, unfortunately.

This would go well with in an auto-scaling story. Scale down to zero resources when there's no activity and up to however many containers are needed. So basically the # of replicas set in a ReplicationController would be a max not the set number.
"
1314	489	lavalamp	2014-07-16 17:49:04	MEMBER	"The last time this came up, I was starting to think that checkpointing (as we implemented it) is _extremely_ similar to what etcd is doing for us.
"
1315	476	brendandburns	2014-07-16 17:52:41	CONTRIBUTOR	"Comments addressed, ptal.

Thanks!
--brendan
"
1316	442	bgrant0607	2014-07-16 17:55:43	MEMBER	"Clayton, at a high level this all sounds very reasonable and compatible with our cloud APIs. We'll need to discuss in more detail how users will authenticate and what authorization policies will look like.

A few other comments:
- We call resource limits _quotas_. Aggregate limits work pretty well for the most part, though additional restrictions on maximum pod size and/or shape may be needed to ensure schedulability. If we support multiple quality of service levels ( #147 ), we'll want limits by level.
- Limits on numbers of objects in the system (pods, etc.) are also needed and its good to impose such limits from the beginning rather than try to add them later.
- Expect someone to eventually want to automate management of these limits.
- We can use a variant of constraints ( #367 ) to manage placement restrictions. 
- We should do quota-based _admission control_ (i.e., the decision to accept or reject a request) at the apiserver, and resource/constraint-based admission control at kubelet. 
- How we actually determine resource capacity and consumption is an interesting topic unto itself, especially in the presence of quality of service tiers, SLOs, overcommitment, etc. We likely will need pluggable policies.
"
1317	442	smarterclayton	2014-07-16 17:59:49	CONTRIBUTOR	"- Prevent one team's pods from being ""near"" any other team's pods. I'll call this Exclusion.

This one is probably a 3rd use case.  I'd restate the original more correctly in terms of this:
- Prevent one team from being able to interact with another's teams pods directly.  You might call this Limited Visibility.

For Quota - no disagreement that it is typically fine grained, although I can think of cases where compute impact can be nuanced in terms of how it impacts other users (the two sides of CPU scheduling, percentage of each time interval to which you are allocated a CPU vs maximum contiguous block within that interval which you may execute without interruption).

I think from a use case perspective in our area we see Exclusion as less common, and agree that the basic type often works.  The next step is typically at a large granularity (and thus inefficient), but occurs for either ""important customer/project"" or ""production vs development"".  Fine grained exclusion seems less common.

So Quota and Limited Visibility drive a lot of our thinking, with Exclusion being the less important because you can (as you note) easily say ""run your own cluster"".
"
1318	482	monnand	2014-07-16 17:59:56	CONTRIBUTOR	"Thank you, @brendandburns 
"
1319	469	lavalamp	2014-07-16 18:04:01	MEMBER	"Looks like rebase broke the build.
"
1320	489	thockin	2014-07-16 18:04:24	MEMBER	"If we made the ""how to remove a pod"" protocol be ""set removed = true"" in etcd, that might hold.  The kubelet could then do the actual removal when it was done removing.  But what about config files?  What about internal things that we want to recover that are not part of the API?
"
1321	235	brendandburns	2014-07-16 18:05:34	CONTRIBUTOR	"Closing this issue for now, a full UX on the master is out of scope for the time being, and there are other more specific enhancements either in-progress or completed.
"
1322	442	smarterclayton	2014-07-16 18:06:29	CONTRIBUTOR	"Brian, for admission control do you see that as fundamentally a hard limit, or soft with reconciliation?  Since you're drawing from a pool you need to coordinate the reservation of that resource - if failures occur after you've reserved but before you deploy you then need to undo that reservation, and doing that correctly is difficult to implement correctly.  We struggle with this in practical terms of whether you can allow eventually consistent behavior at the admission control level (create something, 5s later it gets deleted) and whether that compromises experience.  This is getting more into implementation, just fishing for different perspectives here.
"
1323	481	lavalamp	2014-07-16 18:07:17	MEMBER	"Thanks!
"
1324	469	smarterclayton	2014-07-16 18:12:27	CONTRIBUTOR	"Embarrassed
"
1325	465	lavalamp	2014-07-16 18:16:57	MEMBER	"LGTM
"
1326	469	lavalamp	2014-07-16 18:17:50	MEMBER	"Happens to the best of us :)
"
1327	448	monnand	2014-07-16 18:24:10	CONTRIBUTOR	"@lavalamp Sent #491
"
1328	472	thockin	2014-07-16 18:25:54	MEMBER	"Line between actuator and storage?
"
1329	476	thockin	2014-07-16 18:30:51	MEMBER	"Error — The Travis CI build could not complete due to an error

Otherwise LGTM  who is the secondf LGTM on this?  @lavalamp 
"
1330	484	thockin	2014-07-16 18:38:05	MEMBER	"I like the general idea, but I don't know when we'll get to it - we've been
sort of inundated in the last month :)

I'd probably argue that this is a different type of controller or at least
a different parameter to replication controller

On Wed, Jul 16, 2014 at 10:46 AM, Kyle Mathews notifications@github.com
wrote:

> I'm a Go neophyte plus getting a startup going so I'm probably not the
> right person to tackle this atm, unfortunately.
> 
> This would go well with in an auto-scaling story. Scale down to zero
> resources when there's no activity and up to however many containers are
> needed. So basically the # of replicas set in a ReplicationController would
> be a max not the set number.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/484#issuecomment-49201385
> .
"
1331	442	erictune	2014-07-16 18:45:08	MEMBER	"(answering for Brian)
Our assumptions are:
- teams with the largest resource usage are typically better staffed.  So these teams can devote time to understand the complexity of hard limits.   And they can plan their needs carefully and reserve the right amount of resources ahead of time.  
- teams with smaller resource usage just want it to work.  The cost of wasting their admin time on resource planning exceeds the value of setting a precise resource quota for them. 
- If you let a big team go, say 10% over its quota temporarily, it might completely exhaust a cluster resource. 
- if you let a small team go say 10% over its quota temporarily, it is very unlikely to exhaust a clusters resource.  
- there are typically a few big users and lots of small users in a cluster with many users.  You can imagine that it may be a power-law distribution, like income.  Except that, once a user gets big enough, they have incentives to run their own cluster.  But, then they end up subdividing the resources in their private cluster...

One way you could approach this is:
- have hard limits for big teams.
- put the rest of the resources in an ""ALL_OTHER_TEAMS"" bucket.
- sum(big team limits) + ALL_OTHER_TEAMS <= max practical cluster capacity
- give small teams a soft limit.
- sum(small team limits)  <= OVERSUB_FACTOR \* ALL_OTHER_TEAMS

Thoughts?
"
1332	442	erictune	2014-07-16 18:46:07	MEMBER	"btw, I used quota and limit interchangeably in that last post, which wasn't very precise.
"
1333	442	thockin	2014-07-16 18:48:02	MEMBER	"On Wed, Jul 16, 2014 at 10:59 AM, Clayton Coleman
notifications@github.com wrote:

> Prevent one team's pods from being ""near"" any other team's pods. I'll call this Exclusion.
> 
> This one is probably a 3rd use case. I'd restate the original more correctly in terms of this:
> 
> Prevent one team from being able to interact with another's teams pods directly. You might call this Limited Visibility.

Define ""interact"" ?

> For Quota - no disagreement that it is typically fine grained, although I can think of cases where compute impact can be nuanced in terms of how it impacts other users (the two sides of CPU scheduling, percentage of each time interval to which you are allocated a CPU vs maximum contiguous block within that interval which you may execute without interruption).
> 
> I think from a use case perspective in our area we see Exclusion as less common, and agree that the basic type often works. The next step is typically at a large granularity (and thus inefficient), but occurs for either ""important customer/project"" or ""production vs development"". Fine grained exclusion seems less common.
> 
> So Quota and Limited Visibility drive a lot of our thinking, with Exclusion being the less important because you can (as you note) easily say ""run your own cluster"".
> 
> ## 
> 
> Reply to this email directly or view it on GitHub.
"
1334	476	lavalamp	2014-07-16 18:49:06	MEMBER	"General thoughts:

I'm going to throw out a completely different design here.
- Keep the api package as is.
- Make sub-packages for each (old) version, that include only the types.go file (pkg/api/v1beta1).
- api package imports all the old versions.
- Encode/Decode functions in the api package are modified to check the version in the JSONBase, and use the struct from the appropriate package.
- api package keeps a list of conversion functions and therewith provides v1beta1.Pod -> api.Pod functionality.
- apiserver gets a second dimension in its map, that of version.
- The registry classes take a version, and we make one for each version supported.
- api package supports generic ""give me new Pod object of version v1beta1"".
  - This is to make it easy for the same registry class to serve multiple versions of the api.

This is more complicated, but I think more extensible. I'm still thinking about it.
"
1335	442	smarterclayton	2014-07-16 18:54:11	CONTRIBUTOR	"@thockin - Can I (on team A) see the pods you've created (on team B)?  Can I change the replication controllers you've created?  Can I see the environment variables in the pod templates you've created?  Can my pods reach your pods over IPv4 if we're not on the same team?

There are places where you explicitly want distinct teams to coordinate, although you'd prefer they do so through a defined boundary (network host and port, load balancer, dns name, specific API key) that steps outside of infrastructure (traditionally).

The last question is a fairly specific topology request from OpenShift customers - they'd like to blanket drop outgoing packets from containers except those that match certain IPs in their project / team's units.  It requires a willingness to distribute out the graph of connections to endpoints (which imposes its own scale limits), but is something that isn't terribly hard to do if motivated.  That's a separate thread though.
"
1336	484	brendandburns	2014-07-16 19:02:07	CONTRIBUTOR	"Yeah, in terms of the replication controller, I think the # there is always
the truth.  If we want to introduce auto-scaling, we should introduce an
additional controller that is in charge of auto-scaling.

--brendan

On Wed, Jul 16, 2014 at 11:38 AM, Tim Hockin notifications@github.com
wrote:

> I like the general idea, but I don't know when we'll get to it - we've been
> sort of inundated in the last month :)
> 
> I'd probably argue that this is a different type of controller or at least
> a different parameter to replication controller
> 
> On Wed, Jul 16, 2014 at 10:46 AM, Kyle Mathews notifications@github.com
> wrote:
> 
> > I'm a Go neophyte plus getting a startup going so I'm probably not the
> > right person to tackle this atm, unfortunately.
> > 
> > This would go well with in an auto-scaling story. Scale down to zero
> > resources when there's no activity and up to however many containers are
> > needed. So basically the # of replicas set in a ReplicationController
> > would
> > be a max not the set number.
> > 
> > ## 
> > 
> > Reply to this email directly or view it on GitHub
> > <
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/484#issuecomment-49201385
> > 
> > .
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/484#issuecomment-49208204
> .
"
1337	484	lavalamp	2014-07-16 19:05:38	MEMBER	"It would be awesome to make a feedback loop between a service's QPS and the number in the replicas field. We have the proxy, so we could in theory automatically count QPS...
"
1338	484	smarterclayton	2014-07-16 19:08:42	CONTRIBUTOR	"When we went through this debate with Docker + systemd + geard + openshift, it did seem like it's best to model it as a characteristic of whichever proxy you were using (service, external load balancer, etc).  

The challenge with systemd activation on a minion is that the port/interface has to be defined before the pod is started, which today is the pod being scheduled to the host.  However, the pod has to be ""idle"" (stopped) which complicates resource scheduling - the scheduler now can't make resource decisions without double checking to see if things have been unidled, and potentially you would wake too many things.  The complexity for OpenShift didn't seem worth it, so we said that in OpenShift next on Kubernetes we would just do the idling / unidling at the edge traffic proxy, which fits in with what brendan was suggesting.  We also generally prefer the idea of removing unused config from the minions to simplify administration.

The downside being is you have a lot higher latency before you can release traffic from the proxy (service -> apiserver -> scheduler -> kubelet poll -> docker container start).  

However, if the scheduling problems could be solved (i.e. if that feedback loop from minion to host on unidle could be made sound) it could definitely benefit some use cases.
"
1339	484	davidstrauss	2014-07-16 19:08:47	NONE	"Speaking as a systemd maintainer, we're always +1 on new integrations with socket activation. I'll keep watching this thread to see if there's any way I can provide implementation guidance. I know the CoreOS folks have extensive experience with Go and socket activation.
"
1340	484	smarterclayton	2014-07-16 19:10:20	CONTRIBUTOR	"Agree on autoscaling be distinct from replication controller responsibility.
"
1341	469	lavalamp	2014-07-16 19:15:48	MEMBER	"I'm just going to merge, to shrink the pending PR list. We can fix my nit later.
"
1342	476	brendandburns	2014-07-16 19:27:07	CONTRIBUTOR	"I really don't want to have versions in the storage.  I think we need to translate at the edge (e.g. in the apiserver) and no where else.

The intent of this PR is to have:

v1beta1, v1beta2, ... vN in the apiserver
convert them all down into the internal representation
do any and all processing via internal representation
convert back to v1beta<foo> for the result

The version stuff should be a thin skin, nothing deeper.
"
1343	476	lavalamp	2014-07-16 19:50:59	MEMBER	"How do you handle the upgrade of a running cluster? And if that's your intent, then I think you need to add an explicit conversion layer in apiserver, and registry package shouldn't need to import v1beta1...
"
1344	484	erictune	2014-07-16 19:53:53	MEMBER	"David:

Does idle mean you terminate the daemon process?

How do you decide what is the right number of total daemons per machine, before you need to add more physical or virtual machines? 

What do you do when, due to a stroke of bad luck, a bunch of daemons all wake up at the same time, and you OOM?  Or do you have so many daemons per machine that this is very unlikely?  In that case, I guess you know that wakeups are relatively uncorrelated?
"
1345	484	smarterclayton	2014-07-16 20:01:13	CONTRIBUTOR	"I can answer for OpenShift today (not for David, who has a much higher density than us and a slightly different scenario) - idle means terminate daemon, capacity is preplanned and works best at higher densities (lower densities it works less well for), if you get too many wakes you either throttle, OOM, or fail.  In practice in our case they are uncorrelated.
"
1346	476	brendandburns	2014-07-16 20:04:22	CONTRIBUTOR	"Ah, yes, you're right, the interface{} stuff in the storage layer is
masking this from the compiler's perspective, but you're right, we need to
convert, I'll add that code.

pod registry imports v1beta1 for communication with the kubelet.  I could
make that communication be 'internal' but I thought we decided to version
those calls.

--brendan

On Wed, Jul 16, 2014 at 12:51 PM, Daniel Smith notifications@github.com
wrote:

> How do you handle the upgrade of a running cluster? And if that's your
> intent, then I think you need to add an explicit conversion layer in
> apiserver, and registry package shouldn't need to import v1beta1...
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/476#issuecomment-49217337
> .
"
1347	492	brendandburns	2014-07-16 20:06:35	CONTRIBUTOR	"What is the use case?  This seems sort of like auto-scaling policy.  I'd prefer to put those objects in the auto-scaler.
"
1348	476	brendandburns	2014-07-16 20:10:07	CONTRIBUTOR	"I take it back.  This does actually work, because everything comes in as raw JSON into the registry, and then it is converted to internal representation.  This _won't_ work once the types diverge.  Would you prefer that I handle that in this CL, or a future one.

This CL is a pain to keep rebased, so I'd prefer to get it in, and then update the apiserver in a different PR.
"
1349	492	lavalamp	2014-07-16 20:10:38	MEMBER	"It sounds like you want a canary-controller that scales the old version down while scaling the new one up. Combine with a service that spreads traffic across both versions and this should Just Work (tm).
"
1350	472	lavalamp	2014-07-16 20:17:53	MEMBER	"Oops, fixed.
"
1351	476	thockin	2014-07-16 20:18:19	MEMBER	"LGTM

We did agree to split kubelet and apiserver types, which will have to come
as a followup.  We need a handshake and compat matrix.

On Wed, Jul 16, 2014 at 1:10 PM, brendandburns notifications@github.com
wrote:

> I take it back. This does actually work, because everything comes in as
> raw JSON into the registry, and then it is converted to internal
> representation. This _won't_ work once the types diverge. Would you
> prefer that I handle that in this CL, or a future one.
> 
> This CL is a pain to keep rebased, so I'd prefer to get it in, and then
> update the apiserver in a different PR.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/476#issuecomment-49219799
> .
"
1352	492	smarterclayton	2014-07-16 20:27:59	CONTRIBUTOR	"I'm thinking about this from the mechanics of doing a deployment by composing replication controllers.  With controllers as they are today:
- existing replication controller with old template, name=foo,deployment=1 x5
- create new replication controller with new template, name=foo,deployment=2 x0
- alter controller 2 to be x1 (or up by the batch size)
- alter controller 1 to be x4 (or down by the batch size)
- ... repeat until done
- delete controller 1

The thought around atLeast was about explicitly representing the invariant you want during an available deployment:
- existing replication controller with old template, name=foo,deployment=1 x5
- create new replication controller with new template, name=foo, atLeast=5
- set controller to be atMost=5
- delete a pod from the name=foo,deployment=1 query
- ... continue until 0 pods in that query
- delete controller 1
- update controller 2 to change the label query to name=foo,deployment=2

I'm not sure that's better.... just the for loop for deletes seems more reasonable.  Not a huge deal, just trying to work through how we'd model common deploy cases for end users.
"
1353	476	lavalamp	2014-07-16 20:40:02	MEMBER	"(See chat from a few minutes ago in IRC)
"
1354	484	erictune	2014-07-16 20:48:41	MEMBER	"I can see how this is great for the case where you have lots of infrequently used daemons, lots of tenants, and pre-planned resources.  

I can also see that things get complicated fast if you try to mix this model with a more ""VM hosting"" type of model, with guaranteed resources.   

If k8s is going to support both models, then it would be good to carefully define different names for how much memory a daemon can ever use (for kubelet resource limits #168), versus how much is uses at a moment in time, versus how much is assumed to use ""on average"" (for use in scheduling #274 and quota #442).  And then display the right numbers in the right context (#317).
"
1355	472	thockin	2014-07-16 20:59:11	MEMBER	"PNG rendering seems broken...
"
1356	472	thockin	2014-07-16 21:00:52	MEMBER	"The word ""apiserver"" doesn't appear anywhere - should it?
"
1357	442	thockin	2014-07-16 21:03:32	MEMBER	"That's what I thought you meant.  Yeah, that all gets complicated.

On Wed, Jul 16, 2014 at 11:54 AM, Clayton Coleman notifications@github.com
wrote:

> @thockin https://github.com/thockin - Can I see the pods you've
> created? Can I change the replication controllers you've created? Can I see
> the environment variables in the pod templates you've created? Can my pods
> contact your pods if we're not on the same team?
> 
> There are places where you explicitly want distinct teams to coordinate,
> although you'd prefer they do so through a defined boundary (network host
> and port, load balancer, dns name, specific API key) that steps outside of
> infrastructure (traditionally).
> 
> The last question is a fairly specific topology request from OpenShift
> customers - they'd like to blanket drop outgoing packets from containers
> except those that match certain IPs in their project / team's units. It
> requires a willingness to distribute out the graph of connections to
> endpoints (which imposes its own scale limits), but is something that isn't
> terribly hard to do if motivated.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/442#issuecomment-49210245
> .
"
1358	472	lavalamp	2014-07-16 21:06:56	MEMBER	"PNG fixed (save as != export, oops). Some smart guy earlier in the thread pointed out that it's just an implementation detail how we package up our apis... :)
"
1359	472	thockin	2014-07-16 21:09:04	MEMBER	"As a followup, you should link to it from DESIGN.md
"
1360	493	monnand	2014-07-16 21:20:00	CONTRIBUTOR	"LGTM.

Regarding to separate PRs: Sometimes, the reason we want to update a dependency is that it changed its APIs. In that case a separate PR may break the CI because the code in k8s is still using the old API. But I a agree that PR with updating dependency should make least change of existing code.
"
1361	493	lavalamp	2014-07-16 21:20:56	MEMBER	"Yeah, agree, api breakers should come along with the fixes. :)
"
1362	493	smarterclayton	2014-07-16 21:23:47	CONTRIBUTOR	"LGTM.  Although... can't you just do this with regular HTTP for a watch API (like etcd)?  Or is this watch and stream updates?  Which I guess you can also do with HTTP...
"
1363	493	lavalamp	2014-07-16 21:29:05	MEMBER	"I want to do a watch and stream updates, yes.
"
1364	493	smarterclayton	2014-07-16 21:30:37	CONTRIBUTOR	"And couldn't that be done with HTTP in Go just as well?  Downside of having separate pulls is I infer intent prior to actually seeing code :)
"
1365	493	lavalamp	2014-07-16 21:45:49	MEMBER	"See, this is exactly the conversation I want to see before we add a dependency. I'll see if I can do it with regular HTTP first, so hold off on merging this.
"
1366	483	brendandburns	2014-07-16 21:52:54	CONTRIBUTOR	"Tests added.  ptal.

Thanks!
--brendan
"
1367	483	xiang90	2014-07-16 21:56:46	CONTRIBUTOR	"LGTM
"
1368	478	monnand	2014-07-16 22:22:17	CONTRIBUTOR	"@lavalamp Since this PR containers all commits in #448, which makes it very large. However, the only piece you concern may be in pkg/scheduler/leastload.go. Once #491 getting merged, I will clean the commit history. I'm currently writing a design docs for this scheduler and will share it with you once I've done.
"
1369	356	smarterclayton	2014-07-16 23:14:40	CONTRIBUTOR	"@thockin realized there is no update support today - going to model that as a variation of SyncPods where the set of known pods is killed (call killPod(pod)) prior to syncPod being called, with no final killContainer step.  Make sense?

EDIT: Alternatively, I can just ignore update for now.  I'd be able to do coarse update (burn it to the ground) but finer grained updates get more complex (and you'd probably want to think through separating out the source of changes).  So ""no updates"" or ""coarse updates""?
"
1370	496	lavalamp	2014-07-16 23:41:54	MEMBER	"Thanks for the change! Can you sign our CLA? See CONTRIB.md.
"
1371	497	lavalamp	2014-07-16 23:43:25	MEMBER	"LGTM
"
1372	491	vmarmol	2014-07-16 23:59:47	CONTRIBUTOR	"Renaming that in cAdvisor SGTM
"
1373	495	thockin	2014-07-17 00:23:14	MEMBER	"Thanks for the patch.  We need you to sign the CLA as described in CONTRIB.md before we merge this.
"
1374	491	smarterclayton	2014-07-17 00:39:33	CONTRIBUTOR	"LGTM otherwise.
"
1375	491	monnand	2014-07-17 00:40:37	CONTRIBUTOR	"OK. Hold on this PR. Let me do a rename in cAdvisor and send another commit to this PR.
"
1376	483	smarterclayton	2014-07-17 00:51:23	CONTRIBUTOR	"LGTM
"
1377	487	yugui	2014-07-17 03:28:47	CONTRIBUTOR	"PTAL.
"
1378	360	meirf	2014-07-17 04:16:08	CONTRIBUTOR	"It looks like a consensus has been reached here to move from the string-based approach. @lavalamp, do you agree that @thockin's proposal above should be the next course of action? Should other specifications be taken into account based on the conversation at https://github.com/GoogleCloudPlatform/kubernetes/issues/341 when implementing the proposal? I'd be glad to work on this further and heed your feedback.
"
1379	486	yugui	2014-07-17 04:40:16	CONTRIBUTOR	"PTAL
"
1380	478	lavalamp	2014-07-17 04:42:38	MEMBER	"First round of code review done.
"
1381	360	lavalamp	2014-07-17 04:47:53	MEMBER	"I'm 80% on board with @thockin's previous post, but I think it is an in-memory form and there does need to be a canonical text form, since we send these as a query in http requests. I'd like to start with simple changes, perhaps just add support for ""x IN (a, b, c)"" or something-- maybe not write an entire AST parser all at once. :)
"
1382	287	rafael	2014-07-17 05:09:40	CONTRIBUTOR	"Hey guys, sorry for the delay on this one. I will try to get the CLA tomorrow, otherwise feel free to close it and recreate it on your end. 
"
1383	360	thockin	2014-07-17 05:14:39	MEMBER	"I am fine with a string form, but let's define a nice syntax and a proper
parser.

On Wed, Jul 16, 2014 at 9:48 PM, Daniel Smith notifications@github.com
wrote:

> I'm 80% on board with @thockin https://github.com/thockin's previous
> post, but I think it is an in-memory form and there does need to be a
> canonical text form, since we send these as a query in http requests. I'd
> like to start with simple changes, perhaps just add support for ""x IN (a,
> b, c)"" or something-- maybe not write an entire AST parser all at once. :)
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/360#issuecomment-49258087
> .
"
1384	287	thockin	2014-07-17 05:16:24	MEMBER	"Don't be daunted - the individual CLA really takes just a minute to sign :)

On Wed, Jul 16, 2014 at 10:09 PM, Rafael Chacon notifications@github.com
wrote:

> Hey guys, sorry for the delay on this one. I will try to get the CLA
> tomorrow, otherwise feel free to close it and recreate it on your end.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/287#issuecomment-49259050
> .
"
1385	452	thockin	2014-07-17 05:33:30	MEMBER	"Only small nits from me.  Need a second LGTM (@smarterclayton ?) and to squash the commits.
"
1386	499	proppy	2014-07-17 05:36:40	CONTRIBUTOR	"If you're using Google Cloud Platform, one possibility is to use the [`google/docker-registry`](https://index.docker.io/u/google/docker-registry) image to push your images to Google Cloud Storage.

You should then be able to add `google/docker-registry` to one of your pod and pull from `localhost:5000/myimagename`.
"
1387	499	brendandburns	2014-07-17 05:53:22	CONTRIBUTOR	"There's an example of a container manifest which uses a private registry
here:

https://github.com/GoogleCloudPlatform/kubernetes/blob/master/build/master-manifest.yaml

On Wed, Jul 16, 2014 at 10:36 PM, Johan Euphrosine <notifications@github.com

> wrote:
> 
> If you're using Google Cloud Platform, one possibility is to use the
> google/docker-registry https://index.docker.io/u/google/docker-registry
> image to push your images to Google Cloud Storage.
> 
> You should then be able to add google/docker-registry to one of your pod
> and pull from localhost:5000/myimagename.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/499#issuecomment-49260229
> .
"
1388	499	brendandburns	2014-07-17 05:54:09	CONTRIBUTOR	"in case it wasn't clear, if you use the Google storage private registry,
the credentials are supplied with the service account that is available
from your GCE VM.

--brendan

On Wed, Jul 16, 2014 at 10:52 PM, Brendan Burns bburns@google.com wrote:

> There's an example of a container manifest which uses a private registry
> here:
> 
> https://github.com/GoogleCloudPlatform/kubernetes/blob/master/build/master-manifest.yaml
> 
> On Wed, Jul 16, 2014 at 10:36 PM, Johan Euphrosine <
> notifications@github.com> wrote:
> 
> > If you're using Google Cloud Platform, one possibility is to use the
> > google/docker-registry https://index.docker.io/u/google/docker-registry
> > image to push your images to Google Cloud Storage.
> > 
> > You should then be able to add google/docker-registry to one of your pod
> > and pull from localhost:5000/myimagename.
> > 
> > —
> > Reply to this email directly or view it on GitHub
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/499#issuecomment-49260229
> > .
"
1389	499	smarterclayton	2014-07-17 13:36:40	CONTRIBUTOR	"In general we'd want to support a wide range of authentications for private registries.  OAuth is a good first step, but in multi-tenant setups you can't rely on host trust relationships.  Also, folks should be able to use api keys from private repos in the DockerHub, and eventually even passwords.
1. Infrastructure trust with one or more remote registries (the apiserver communicates the pod ""owner's"" identity to the kubelet, the host is set up to pass that identity through Docker to the remote repo)
2. Delegated trust between individual users and registries (user A creates a token enabling access to image B in repo C, that token can be passed down to the Kubelet via the manifest and then on to Docker)
3. Use direct credentials to connect to registries (user/passwords, client certs) 

Lots of complexity here in op shops.  Kerberos and SSL client certs are the most common _complex_ solutions, but an OAuth trust relationship, properly configured, would be better than most of the others.
"
1390	486	smarterclayton	2014-07-17 13:39:32	CONTRIBUTOR	"LGTM
"
1391	478	lavalamp	2014-07-17 14:31:01	MEMBER	"I thought of one more thing. I'm not sure that we want to track resource shapes in the way that cAdvisor reports usage. There's been some discussion that I'll try to find for you.
"
1392	486	lavalamp	2014-07-17 14:49:28	MEMBER	"I'm OK with this, as long as there's the understanding that we'll change it to an error type that carries a suggested HTTP response code with it. I don't want to make an error type for every possible return value and use a switch to decide what to return. I actually was moving in that direction with the api.Status struct, which has a ""Code"" field for that purpose, and the client already has a StatusError type...

Please squash before we commit, though!
"
1393	486	smarterclayton	2014-07-17 14:51:12	CONTRIBUTOR	"@lavalamp - agree, an error associated with a ""rest storage"" interface should logically match REST error conditions as much as possible.
"
1394	501	lavalamp	2014-07-17 16:05:46	MEMBER	"> CA Advisor runs inside docker containers, but I don't believe it's correct to show the Kubelet and Proxy processes as inside the docker box on the minion.

This is the desired state, which is admittedly not the current state.

> ... load balance across minion proxies ...

Yeah, this is a good point. k8s currently relies on its environment for load balancing. Maybe @brendanburns  or @thockin can speak to the desired state here.
"
1395	457	dchen1107	2014-07-17 16:15:12	MEMBER	"@smarterclayton Can I send a separate PR for handling those requests in a operations queue? 

@lavalamp I addressed most of your comments. PTAL? If you prefer, I could split this PR into several different PRs. Thanks!
"
1396	487	brendandburns	2014-07-17 16:19:48	CONTRIBUTOR	"Basically looks good, one small comment and a typo.

Thanks!
--brendan
"
1397	476	brendandburns	2014-07-17 16:21:28	CONTRIBUTOR	"I'm taking a second try at this in a different branch.  Closing this one for now.
"
1398	439	brendandburns	2014-07-17 16:22:31	CONTRIBUTOR	"reverting this until I have time to do it for realz.
"
1399	457	lavalamp	2014-07-17 16:38:15	MEMBER	"Agree adding to operation queue is probably best added in another PR. @dchen1107, I'll take another look soon, hopefully before lunch.
"
1400	493	lavalamp	2014-07-17 16:44:50	MEMBER	"OK, I tried the straight up HTTP method-- it was a pain, so I tried the websocket method and it Just Worked (tm), first time. In the interest of expediency, let's use the websocket.
"
1401	493	lavalamp	2014-07-17 17:15:32	MEMBER	"PR updated with some implementation.
"
1402	493	smarterclayton	2014-07-17 17:16:35	CONTRIBUTOR	"Can you describe why it was a pain?  I just want to have the mental map in my head, because I thought it would just be a stream of JSON.encode(event) to a file writer, with a corresponding JSONDecoder stream loop on the other end?
"
1403	493	lavalamp	2014-07-17 17:21:08	MEMBER	"I thought it would work that way, too, but it blocked forever. I think it actually requires hijacking the response writer, and I wasn't really able to find an equivalent thing for the client. I figure that by the time I got it working, I'd have some code that looked awfully similar to the websocket code.
"
1404	493	smarterclayton	2014-07-17 17:22:25	CONTRIBUTOR	"Oh, you have to flush?  (w).(http.Flushable).Flush()?  This is also how docker works with its event and logging streams (which are json flushed to streams) - so I was fairly certain it was a simple implementation.

https://github.com/dotcloud/docker/blob/master/server/server.go#L277, and then below it they have Write(nil) which does a flush.
"
1405	493	lavalamp	2014-07-17 17:29:45	MEMBER	"Ah, I didn't try that. Maybe it would have worked? I can try again if you feel strongly about it. In the meantime, I just added something to detect the client closing the connection.
"
1406	493	smarterclayton	2014-07-17 17:31:08	CONTRIBUTOR	"@lavalamp I kind of feel strongly about forcing a whole separate client library and implementation _just_ to do something which I _think_ can be done reasonably efficiently in http.  I'm not opposed to there being two transports (for instance, in a browser, the web socket api is WAY easier to integrate with).  But I'd argue that it should be available in both transports since the output is a series of messages delivered to a stream, with logical separators.  In JSON the logical separator is the end of the } and the beginning of the next {

I'm open to counter arguments here - I just think about a simple client having to have two connection logic paths just to watch/stream events, and that feels wrong
"
1407	493	lavalamp	2014-07-17 17:33:12	MEMBER	"OK, I'll look into providing both. Also, I found a reason to have your test code in the same package: if you import your package in the *_test.go files, the coverage numbers come out completely wrong. :(
"
1408	493	smarterclayton	2014-07-17 17:34:13	CONTRIBUTOR	"Ah, hadn't hit that with coverage yet.  Yuck.
"
1409	491	monnand	2014-07-17 17:40:33	CONTRIBUTOR	"Just talked about this in google/cadvisor#92, we will only rename some functions in this PR.
- rename GetMachineInfo to GetRootInfo
- rename GetMachineSpec to GetMachineInfo.

PTAL.
"
1410	502	lavalamp	2014-07-17 17:43:25	MEMBER	"Thanks!
"
1411	491	smarterclayton	2014-07-17 17:45:29	CONTRIBUTOR	"Rename looks good, still need a second review from someone.
"
1412	491	monnand	2014-07-17 17:48:26	CONTRIBUTOR	"Thank you, @smarterclayton . I'll rebase it to solve the conflict
"
1413	452	Sarsate	2014-07-17 18:02:49	CONTRIBUTOR	"Squashed commits, addressed comments, and passes CI. Needs a second review. @lavalamp @smarterclayton
"
1414	500	smarterclayton	2014-07-17 18:58:21	CONTRIBUTOR	"Not today - see #503 though for some discussion at this.  My personal opinion is that this is a higher level concept that does not exist today.  You manage deployments and rollbacks personally with replication controllers in sequence.
"
1415	274	smarterclayton	2014-07-17 19:00:03	CONTRIBUTOR	"First implementation attempt here: #478 
"
1416	504	brendandburns	2014-07-17 19:26:44	CONTRIBUTOR	"How would you distribute the image to all of the host machines?

Note we do support pulling from a private repository (for example backed
with Google Cloud Storage)

--brendan

On Thu, Jul 17, 2014 at 12:22 PM, Peter Schultz notifications@github.com
wrote:

> #469 https://github.com/GoogleCloudPlatform/kubernetes/pull/469
> introduced a change that makes kubelet always pull the image
> https://github.com/smarterclayton/kubernetes/blob/185a97b0374a80e7b02ac6f97225b63bc26342d9/pkg/kubelet/kubelet.go#L585-L589
> for a new container. This makes it impossible to run locally built images
> because they can't be pulled. Is this intentional?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/504.
"
1417	504	pschultz	2014-07-17 19:43:51	NONE	"Docker pulls images automatically if they don't exist on the host yet. That was the behavior before the PR got merged.
"
1418	493	lavalamp	2014-07-17 19:53:42	MEMBER	"Figured it out. Now we serve both versions. This PR is ready for review.
"
1419	360	bgrant0607	2014-07-17 20:43:37	MEMBER	"We need both a JSON AST form and a string form. 

The former is for replicationController and service and other future objects that use label selectors to identify set members.

The latter is for API operations (e.g., GET on resource collections) that need to filter by label selector.

Semantics need to match.

Non-URL-escaped string syntax:

```
selector = requirement { "","" requirement }  .
requirement = KEY [ constraint ] .
constraint = [ ""not"" ] ""in"" ""("" element { "","" element } "")"" .
element = value | range .
value = NUM | STRING .
range = [ NUM ] "":"" NUM | NUM "":"" .
```

If we want to leave out integer ranges for now and stick to strings, I'm fine with that.
"
1420	452	lavalamp	2014-07-17 20:55:52	MEMBER	"This LGTM, modulo comments, which are pretty minor.
"
1421	504	smarterclayton	2014-07-17 20:57:06	CONTRIBUTOR	"Pull should only occur if inspect on the image fails - of course, then you don't get a guarantee that a new change gets picked up, which forces you to specify different image tags for new images.
"
1422	504	brendandburns	2014-07-17 20:58:43	CONTRIBUTOR	"ah, ok, this must be a regression between shelling out to 'docker pull' and
actually talking to the API.

I will take a look.

--brendan

On Thu, Jul 17, 2014 at 1:57 PM, Clayton Coleman notifications@github.com
wrote:

> Pull should only occur if inspect on the image fails - of course, then you
> don't get a guarantee that a new change gets picked up, which forces you to
> specify different image tags for new images.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/504#issuecomment-49364511
> .
"
1423	504	thockin	2014-07-17 20:59:58	MEMBER	"But this means you have to manually pre-distribute images to machines in
order to use them.  Is this really a useful model?

<a few minutes later, after undoing a send) You know, I was just discussing
the possibility of locally built images, and you're right this would not
work.

I don't really like that images that were cached would get stale.  We need
some TTL or something...

On Thu, Jul 17, 2014 at 12:44 PM, Peter Schultz notifications@github.com
wrote:

> Docker pulls images automatically if they don't exist on the host yet.
> That was the behavior before the PR got merged.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/504#issuecomment-49354292
> .
"
1424	491	monnand	2014-07-17 21:26:29	CONTRIBUTOR	"Rebased and squashed all commits. ping @lavalamp @brendandburns 
"
1425	491	lavalamp	2014-07-17 21:34:20	MEMBER	"LGTM
"
1426	491	monnand	2014-07-17 21:41:03	CONTRIBUTOR	"@lavalamp Done. PTAL.
"
1427	491	monnand	2014-07-17 22:05:02	CONTRIBUTOR	"@lavalamp Just did a grep and changed all places using json en/decoder.

However, I noticed another issue: The unit test function names are not changed accordingly when we rename GetMachineInfo to GetRootInfo. I will do it in another commit. So please hold on this PR now.
"
1428	504	smarterclayton	2014-07-17 22:13:54	CONTRIBUTOR	"I really want to encourage patterns that make pull idempotent to the original intent.  For instance - it should be possible to precisely define what image you want (which you can mostly do today with labels) at the container manifest level.  If you're being imprecise, you need to clarify that imprecision and accept the consequences (and benefits).  TTL certainly makes sense.
"
1429	491	monnand	2014-07-17 22:14:26	CONTRIBUTOR	"@lavalamp PTAL. (Remind me to do a squash when it's ready to merge.)
"
1430	491	lavalamp	2014-07-17 22:19:57	MEMBER	"LGTM; squash and let's merge
"
1431	491	monnand	2014-07-17 22:22:31	CONTRIBUTOR	"@lavalamp Squashed. Thank you!
"
1432	491	monnand	2014-07-17 22:59:22	CONTRIBUTOR	"CI passed. ping @lavalamp 
"
1433	491	lavalamp	2014-07-17 23:02:11	MEMBER	"Thanks for the change!
"
1434	493	lavalamp	2014-07-17 23:25:16	MEMBER	"Thanks for the idea, @smarterclayton. Much better now. (grr, I can't squash without putting myself in git history hell, considering that I already started #505)
"
1435	506	monnand	2014-07-17 23:30:42	CONTRIBUTOR	"ping @lavalamp 
"
1436	504	thockin	2014-07-17 23:30:46	MEMBER	"I agree that we want users to specify labels, but some wont.

I'd love to teach docker about TTL.  As long as someone is using an image,
it's alive, but once the refcount hits 0, evaluate the TTL and clean up if
needed.

On Thu, Jul 17, 2014 at 3:14 PM, Clayton Coleman notifications@github.com
wrote:

> I really want to encourage patterns that make pull idempotent to the
> original intent. For instance - it should be possible to precisely define
> what image you want (which you can mostly do today with labels) at the
> container manifest level. If you're being imprecise, you need to clarify
> that imprecision and accept the consequences (and benefits). TTL certainly
> makes sense.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/504#issuecomment-49373840
> .
"
1437	506	monnand	2014-07-17 23:34:25	CONTRIBUTOR	"This PR is related to #478
"
1438	504	brendandburns	2014-07-17 23:44:03	CONTRIBUTOR	"We have an issue to do this in the kubelet.  Maybe we should push it down
into Docker?

--brendan

On Thu, Jul 17, 2014 at 4:30 PM, Tim Hockin notifications@github.com
wrote:

> I agree that we want users to specify labels, but some wont.
> 
> I'd love to teach docker about TTL. As long as someone is using an image,
> it's alive, but once the refcount hits 0, evaluate the TTL and clean up if
> needed.
> 
> On Thu, Jul 17, 2014 at 3:14 PM, Clayton Coleman <notifications@github.com
> 
> > wrote:
> > 
> > I really want to encourage patterns that make pull idempotent to the
> > original intent. For instance - it should be possible to precisely define
> > what image you want (which you can mostly do today with labels) at the
> > container manifest level. If you're being imprecise, you need to clarify
> > that imprecision and accept the consequences (and benefits). TTL
> > certainly
> > makes sense.
> > 
> > ## 
> > 
> > Reply to this email directly or view it on GitHub
> > <
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/504#issuecomment-49373840
> > 
> > .
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/504#issuecomment-49379697
> .
"
1439	506	lavalamp	2014-07-18 00:24:46	MEMBER	"(comments only on go code; will find a reviewer to approve the design)
"
1440	452	Sarsate	2014-07-18 00:25:44	CONTRIBUTOR	"Comments addressed. ping @lavalamp @thockin 
"
1441	506	lavalamp	2014-07-18 00:51:29	MEMBER	"OK more thoughts. This is my suggested roadmap for getting this plumbed through the system.

Move ResourceType to api/types.go. Give it a JSONBase. (JSONBase.ID is the resource's name.) Let's do just that in this PR.

Next: Plumb it through the system. We need a ResourceRegistry, etc. See what we do with PodRegistry for all the places you need to add it in. This will let, for example, a scheduler perform a GET /api/v1beta1/resources and get a list of all resources we support. Users will be able to add and remove resources.

After that, then it'll make sense to add a ResourceRef type, with pods having `Request []ResourceRef` and a `limit []ResourceRef` members. We can add the same thing to minions.

After that, we'll finally be able to plug all that into a simple scheduler.

(Note for others: we're working to get a public doc on resource shapes out.)
"
1442	357	lavalamp	2014-07-18 00:57:19	MEMBER	"Making the scheduler pluggable is my top priority right now. We're working on putting out a resource shape doc for comment. See #506 for the first hints.
"
1443	507	kelseyhightower	2014-07-18 03:37:35	CONTRIBUTOR	"Quick work-around is to

```
touch /tmp/proxy_config
```
"
1444	507	brendandburns	2014-07-18 03:48:34	CONTRIBUTOR	"#496 has the fix, we're waiting on a CLA.

If we don't get it soon, I'll likely just add the fix myself.
"
1445	287	rafael	2014-07-18 04:27:32	CONTRIBUTOR	"Done! 
"
1446	127	thockin	2014-07-18 05:12:48	MEMBER	"All of this sounds reasonable to me, except the part about it being specified per-pod rather than per-container.  I don't think it is far fetched to have an initial loader container that runs to completion when a pod lands on a host and then exits, while the main server is in ""run forever"" mode.

I don't think forcing the spec to be per-pod buys any simplicity, either.  Containers are the things that actually run, why would I spec the policy on the pod?
"
1447	486	yugui	2014-07-18 05:17:31	CONTRIBUTOR	"Squashed.
"
1448	508	smarterclayton	2014-07-18 05:20:05	CONTRIBUTOR	"LGTM - starting to make me feel like we need a more robust integration test set though.
"
1449	127	lexlapax	2014-07-18 05:25:40	NONE	"Been following this thread. I hope my comments are welcome, as I/we are trying to figure out a way to contribute actual code, configurations etc.

The way I was looking at it, it makes sense for the restart behavior to be a the pod level rather than at the container level, keeping in the abstraction around pods expose a service (composed of one or more containers that may communicate between them and may share compute/network/storage resources).. 

For the behavior around singleton containers, you can always have a pod with just one container, which would get you the same thing.

The notion of pods as a service endpoints is much more powerful than the notion of singleton containers as service endpoints. 

This again deviates slightly from the original docker intent - a container is a service encapsulation, which is not entirely true,.. that's why you have docker links, and now things like etcd or dns based inter-container linkages which sort of start breaking down when it comes to dependencies etc.. 

The pod abstraction helps in that regard, and as stated, you could always have one container pods.. 
"
1450	127	thockin	2014-07-18 05:39:48	MEMBER	"You don't have to sell me on pods.  My concern is that attaching restarts to pods feels artificial for very little gain (its not much simpler, really) and makes impossible some easy-to-imagine use cases. 
"
1451	504	thockin	2014-07-18 05:46:13	MEMBER	"It seems like something reasonable for docker to do.  We should at least
offer to di ti at their layer before we wrap it into our own.

On Thu, Jul 17, 2014 at 4:44 PM, brendandburns notifications@github.com
wrote:

> We have an issue to do this in the kubelet. Maybe we should push it down
> into Docker?
> 
> --brendan
> 
> On Thu, Jul 17, 2014 at 4:30 PM, Tim Hockin notifications@github.com
> wrote:
> 
> > I agree that we want users to specify labels, but some wont.
> > 
> > I'd love to teach docker about TTL. As long as someone is using an
> > image,
> > it's alive, but once the refcount hits 0, evaluate the TTL and clean up
> > if
> > needed.
> > 
> > On Thu, Jul 17, 2014 at 3:14 PM, Clayton Coleman <
> > notifications@github.com
> > 
> > > wrote:
> > > 
> > > I really want to encourage patterns that make pull idempotent to the
> > > original intent. For instance - it should be possible to precisely
> > > define
> > > what image you want (which you can mostly do today with labels) at the
> > > container manifest level. If you're being imprecise, you need to
> > > clarify
> > > that imprecision and accept the consequences (and benefits). TTL
> > > certainly
> > > makes sense.
> > > 
> > > ## 
> > > 
> > > Reply to this email directly or view it on GitHub
> > > <
> > 
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/504#issuecomment-49373840
> > 
> > > .
> > 
> > ## 
> > 
> > Reply to this email directly or view it on GitHub
> > <
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/504#issuecomment-49379697>
> > 
> > .
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/504#issuecomment-49380595
> .
"
1452	508	brendandburns	2014-07-18 05:53:33	CONTRIBUTOR	"I'll run e2e tests before submitting.  But yes, we do need more integration
tests...

We often internally have a requirement that every major change comes with a
corresponding integration test.  Definitely slows things down, but also
makes sure you get a good integration test suite.

--brendan

On Thu, Jul 17, 2014 at 10:20 PM, Clayton Coleman notifications@github.com
wrote:

> LGTM - starting to make me feel like we need a more robust integration
> test set though.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/508#issuecomment-49396368
> .
"
1453	188	Lennie	2014-07-18 07:28:40	NONE	"@bgrant0607 there is one thing that seems to be in conflict in your document:

It is the use a range of IP-addresses per Docker host:
""We want traffic between containers to use the pod IP addresses across nodes. Say we have Node A with a container IP space of 10.244.1.0/24 and Node B with a container IP space of 10.244.2.0/24. And we have Container A1 at 10.244.1.1 and Container B1 at 10.244.2.1. We want Container A1 to talk to Container B1 directly with no NAT.""

which seems to conflict with faciliting pod migration and stable IP-addresses:

""We want to be able to assign IP addresses externally from Docker so that we don't need to statically allocate fixed-size IP ranges to each node, so that IP addresses can be made stable across network container restarts, and to facilitate pod migration.""

Sounds to me like you have a choice between:
- IPs really are static: you end up routing /32s after you migrate a pod/container
- IPs are almost always static, unless the pod/container gets migrated
- IPs are pretty much dynamic anyway and you should just stop caring
"
1454	127	lexlapax	2014-07-18 08:15:51	NONE	"simplicity wise, how would this be different conceptually in unix from say a kill signal to a group of processes (pod) vs a kill signal to a singular process (container). implementation wise, it should just cascade down to individual processes.. 
"
1455	127	ironcladlou	2014-07-18 14:40:56	CONTRIBUTOR	"Per-container policies seems the most flexible to me. Another example of a use for per-container policy would be adding a run-once container to an existing pod.

You can compose pod-level behavior using container-level policy, but the inverse is not true.

One disadvantage I can see to per-container is added complexity to the spec. Maybe defaults can help with this. Related: could a pod-scoped default for containers make sense, or would that add more cognitive overhead than it's worth?
"
1456	508	smarterclayton	2014-07-18 14:41:07	CONTRIBUTOR	"Spawned #510 to cover integration tests.
"
1457	509	smarterclayton	2014-07-18 14:41:49	CONTRIBUTOR	"I _think_ all of these are addressed by #356 - it doesn't hurt to put them in now though.
"
1458	127	thockin	2014-07-18 14:47:28	MEMBER	"Dan,  I expect the average number of containers per pod to be low - less
than 5 for the vast majority of case - so I don't think that the logic to
support a pod-level default is worthwhile (yet?).  It would also set a
precedent for the API that we would sort of be expected to follow for other
things, and that will just lead to complexity in the code.

If it turns out to be a pain point, we can always add more API later - but
getting rid of API is harder.

On Fri, Jul 18, 2014 at 7:41 AM, Dan Mace notifications@github.com wrote:

> Per-container policies seems the most flexible to me. Another example of a
> use for per-container policy would be adding a run-once container to an
> existing pod.
> 
> You could compose pod-level behavior using container-level policy, but the
> inverse is not true.
> 
> One disadvantage I can see to per-container is added complexity to the
> spec. Maybe defaults can help with this. Related: could a pod-scoped
> default for containers make sense, or would that add more cognitive
> overhead than it's worth?
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/127#issuecomment-49438244
> .
"
1459	127	ironcladlou	2014-07-18 14:49:07	CONTRIBUTOR	"@thockin Points well taken. I agree that the complexity of an additional pod-level API is premature.
"
1460	127	pmorie	2014-07-18 15:58:20	MEMBER	"I think the policy has to be configurable on a container level but a pod-level default will be convenient to have in the spec.  If the policy is only configurable on the pod level, that seems that it would prevent you from being able to run a transient task (run-once) in a pod of run-forever containers.
"
1461	127	pmorie	2014-07-18 15:59:46	MEMBER	"I only read @thockin 's point after posting the above comment.  I accept these points; can live without pod-default at the moment.
"
1462	486	lavalamp	2014-07-18 16:03:20	MEMBER	"Thanks for the change!
"
1463	496	lavalamp	2014-07-18 16:13:24	MEMBER	"Friendly ping: Any chance we can get you to sign our CLA? We'd really love to pull this change soon! Thanks!
"
1464	513	brendandburns	2014-07-18 16:19:34	CONTRIBUTOR	"I think that this will also be fixed by adding in the sleep in #496 
"
1465	514	smarterclayton	2014-07-18 16:25:10	CONTRIBUTOR	"LGTM
"
1466	139	bgrant0607	2014-07-18 16:26:33	MEMBER	"Re. /run: The point of tmpfs was to avoid pathological disk latency and failure problems. However, we'd need the filesystem to remain live after termination of the main process. We want that for other reasons (e.g., hooks), but it doesn't exist yet.

Solomon expressed some interest in this on #docker-dev:
https://botbot.me/freenode/docker-dev/2014-07-18/?msg=18236306&page=2
"
1467	457	dchen1107	2014-07-18 16:37:10	MEMBER	"PTAL? Thanks
"
1468	516	brendandburns	2014-07-18 16:41:08	CONTRIBUTOR	"LGTM, I'll merge when Travis is done.
"
1469	512	brendandburns	2014-07-18 16:42:44	CONTRIBUTOR	"Small comment, basically LGTM.
"
1470	516	lavalamp	2014-07-18 16:43:40	MEMBER	"LGTM
"
1471	508	lavalamp	2014-07-18 16:47:23	MEMBER	"Small comment, LGTM otherwise.
"
1472	512	lavalamp	2014-07-18 16:50:36	MEMBER	"LGTM modulo comments
"
1473	509	xiang90	2014-07-18 17:14:00	CONTRIBUTOR	"@thockin @lavalamp addressed.
"
1474	513	lavalamp	2014-07-18 17:16:02	MEMBER	"Should be fixed by #514. Please reopen if it isn't. :)
"
1475	496	lavalamp	2014-07-18 17:17:03	MEMBER	"Sorry, but we had to make #514-- too many people were encountering this to let it sit. Thanks for the original identification, though!
"
1476	356	smarterclayton	2014-07-18 17:22:10	CONTRIBUTOR	"Ready for real review, ptal (@thockin)
"
1477	509	lavalamp	2014-07-18 17:25:09	MEMBER	"Thanks!
"
1478	512	smarterclayton	2014-07-18 17:33:43	CONTRIBUTOR	"ptal
"
1479	457	lavalamp	2014-07-18 17:37:48	MEMBER	"Needs rebase. A few more comments, otherwise LGTM .
"
1480	512	lavalamp	2014-07-18 17:42:05	MEMBER	"LGTM, can errors.New's return be constant? I expect not :(
"
1481	507	kelseyhightower	2014-07-18 17:42:30	CONTRIBUTOR	"This is fixed now. Thanks.
"
1482	496	ryfow	2014-07-18 17:46:36	CONTRIBUTOR	"Understood. I'm still working on getting the CLA to you.
"
1483	356	lavalamp	2014-07-18 17:46:43	MEMBER	"I will try to look at this by EOD
"
1484	512	smarterclayton	2014-07-18 17:48:32	CONTRIBUTOR	"They cannot
"
1485	518	brendandburns	2014-07-18 19:35:11	CONTRIBUTOR	"LGTM.
"
1486	519	ryfow	2014-07-18 20:01:10	CONTRIBUTOR	"I have this problem too.

I think this has something to do with [endpoints.go L69](https://github.com/GoogleCloudPlatform/kubernetes/blob/master/pkg/registry/endpoints.go#L69) not having pod_registry/fillPodInfo called on the pods, but I'm not sure how to get the code where it needs to be.
"
1487	519	brendandburns	2014-07-18 20:10:40	CONTRIBUTOR	"I have repro-d this, I should be able to close it down soon.

Sorry!
--brendan

On Fri, Jul 18, 2014 at 1:01 PM, Ryan Fowler notifications@github.com
wrote:

> I have this problem too.
> 
> I think this has something to do with endpoints.go L69
> https://github.com/GoogleCloudPlatform/kubernetes/blob/master/pkg/registry/endpoints.go#L69
> not having pod_registry/fillPodInfo called on the pods, but I'm not sure
> how to get the code where it needs to be.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/519#issuecomment-49473507
> .
"
1488	521	lavalamp	2014-07-18 20:20:17	MEMBER	"What do folks think about structuring the cluster directory as cluster/gce/kube-up.sh and cluster/azure/kube-up.sh instead of suffixing the kube-up.sh script?
"
1489	521	thockin	2014-07-18 20:20:49	MEMBER	"These are all well contained, so my only concern here is the precedent we set with naming and stuff.  Dammit, @lavalamp just beat me to the same point.
"
1490	521	jeffmendoza	2014-07-18 20:23:22	MEMBER	"Yes the naming hurts my tab key, lmk what you decide.
"
1491	521	lavalamp	2014-07-18 20:24:02	MEMBER	"Thanks for this change, it's great to have alternate cloud provider scripts. Have you considered making an azure implementation of the cloudprovider interface?
"
1492	521	jeffmendoza	2014-07-18 20:24:50	MEMBER	"Yes, we'll need that for kube 'services' / load balancers.
"
1493	522	lavalamp	2014-07-18 20:25:44	MEMBER	"LGTM
"
1494	521	lavalamp	2014-07-18 20:27:24	MEMBER	"We could also store the set of scripts somewhere, and link the cluster directory to the one you want to use. That way we wouldn't have to change all of our examples and docs (again).
"
1495	522	thockin	2014-07-18 20:40:53	MEMBER	"I would say that comparing two sets is ""Equals"" and HasAll() would take a slice and check all member against the set.
"
1496	521	thockin	2014-07-18 20:42:29	MEMBER	"I like Daniel's suggestion of subdirs.  We can always re-tool it later.

On Fri, Jul 18, 2014 at 1:27 PM, Daniel Smith notifications@github.com
wrote:

> We could also store the set of scripts somewhere, and link the cluster
> directory to the one you want to use. That way we wouldn't have to change
> all of our examples and docs (again).
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/521#issuecomment-49476007
> .
"
1497	522	jonboulle	2014-07-18 20:45:03	CONTRIBUTOR	"> I would say that comparing two sets is ""Equals"" and HasAll() would take a slice and check all member against the set.

+1, this is more intuitive
"
1498	522	thockin	2014-07-18 20:46:46	MEMBER	"Danny reminded me that this is really ""IsSupersetOf()"" and not ""Equals()"".
 HasAll() should take (string...), IMO.

On Fri, Jul 18, 2014 at 1:45 PM, Jonathan Boulle notifications@github.com
wrote:

> I would say that comparing two sets is ""Equals"" and HasAll() would take a
> slice and check all member against the set.
> 
> +1, this is more intuitive
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/522#issuecomment-49477676
> .
"
1499	524	brendandburns	2014-07-18 20:56:18	CONTRIBUTOR	"Ok, this is ready for merge, ptal
"
1500	519	brendandburns	2014-07-18 20:56:35	CONTRIBUTOR	"please see #524 for the fix.  Sorry for the issue!
"
1501	493	lavalamp	2014-07-18 20:57:43	MEMBER	"Squashed (already in git history hell). Would like to commit and make further changes in other PRs.
"
1502	525	thockin	2014-07-18 20:58:59	MEMBER	"LGTM
"
1503	493	smarterclayton	2014-07-18 20:59:03	CONTRIBUTOR	"LGTM if you spawn an issue to remember :)
"
1504	524	bcwaldon	2014-07-18 21:09:01	CONTRIBUTOR	"What was the reason behind passing in an API client vs a PodRegistry? It seems odd to make an API call back out of the apiserver binary to itself.
"
1505	524	brendandburns	2014-07-18 21:28:33	CONTRIBUTOR	"There is logic in the API Server for reaching out and obtaining live pod status information that was being skipped.  Live data isn't stored in the pod registry, since its transient.
"
1506	524	bcwaldon	2014-07-18 21:30:58	CONTRIBUTOR	"@brendanburns Shouldn't that logic be broken out into a shared data access layer that things in the master/apiserver binary can share? It just seems silly to make an API request to yourself. Unless that's not what's happening here and I'm totally missing something.
"
1507	524	bcwaldon	2014-07-18 21:34:47	CONTRIBUTOR	"Side note - I did verify this PR fixes the bug. Thank you!
"
1508	524	brendandburns	2014-07-18 21:56:41	CONTRIBUTOR	"I agree that it should be broken out.  What I claim is that the RESTful API
_is_ that shared data access layer.  (also, this way we can move this out
into a different binary, or even a different server)

--brendan

On Fri, Jul 18, 2014 at 2:34 PM, Brian Waldon notifications@github.com
wrote:

> Side note - I did verify this PR fixes the bug. Thank you!
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/524#issuecomment-49482263
> .
"
1509	524	bcwaldon	2014-07-18 21:58:33	CONTRIBUTOR	"I had the feeling that it would end up moving out of the API server. Thanks for explaining the justification. 
"
1510	448	monnand	2014-07-18 22:00:37	CONTRIBUTOR	"This PR is split into several PRs. All of them are merged. Close this PR.
"
1511	521	lavalamp	2014-07-18 22:12:30	MEMBER	"Concrete suggestion: put these scripts in `cluster/azure/`. We'll follow up with a PR to move the existing scripts to `cluster/gce/`.
"
1512	522	lavalamp	2014-07-18 22:14:49	MEMBER	"Ah, sorry, I guess I should have let this sit longer. HasAll != Equals, but taking a string... would indeed be better.
"
1513	521	jeffmendoza	2014-07-18 22:23:31	MEMBER	"`cluster/azure/` `release/azure/` and `hack/azure`

will do
"
1514	526	bcwaldon	2014-07-18 22:24:07	CONTRIBUTOR	"Here's where the auth should be getting passed to the docker client: https://github.com/GoogleCloudPlatform/kubernetes/blob/60b6f5b6bddbfc9a39d6e3ccd8ac269fb30bc645/pkg/kubelet/docker.go#L72
"
1515	526	bcwaldon	2014-07-18 22:27:34	CONTRIBUTOR	"I'm not certain, but it looks like authenticated docker pulls would have worked before this patch: 3fa6c9671d6950d6f3feb3d6003729d3f52bc32f
"
1516	526	lavalamp	2014-07-18 23:10:21	MEMBER	"See #499.
"
1517	526	bcwaldon	2014-07-18 23:16:32	CONTRIBUTOR	"ah, search skills failed me again
"
1518	521	jeffmendoza	2014-07-18 23:16:45	MEMBER	"Updated.
"
1519	499	bcwaldon	2014-07-18 23:18:57	CONTRIBUTOR	"It may be helpful in the short term to fail softly in the event that a pull operation fails due to an auth issue. I can easily run `docker login registry.example.com` and pull the images I need manually, and k8s should be able to run them just fine without a successful pull.
"
1520	499	thockin	2014-07-18 23:41:50	MEMBER	"This soft failure mode devolves into #504

On Fri, Jul 18, 2014 at 4:19 PM, Brian Waldon notifications@github.com
wrote:

> It may be helpful in the short term to fail softly in the event that a
> pull operation fails due to an auth issue. I can easily run docker login
> registry.example.com and pull the images I need manually, and k8s should
> be able to run them just fine without a successful pull.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/499#issuecomment-49490333
> .
"
1521	520	lavalamp	2014-07-19 00:15:32	MEMBER	"Thanks!
"
1522	520	lavalamp	2014-07-19 00:17:43	MEMBER	"Argh, I'm sorry, but we need to ask you to sign the CLA before I can merge. Can you do that and re-send this PR?
"
1523	520	jonboulle	2014-07-19 00:20:44	CONTRIBUTOR	"My company (CoreOS) signed the corporate CLA earlier today, does that suffice? (not sure how it's linked to my account exactly) 
"
1524	457	lavalamp	2014-07-19 00:20:59	MEMBER	"I think I'm OK with this. If you can squash some of the commits and figure out why your test is flaky in travis, I think we're good to go.
"
1525	527	lavalamp	2014-07-19 00:22:58	MEMBER	"LGTM, I'll let @thockin take a look, though.
"
1526	520	lavalamp	2014-07-19 00:27:24	MEMBER	"Yeah, that's totally sufficient, thanks! I think we just need to wait for it to show up in the list. Remind me to look again on Monday and I'll escalate if it's still not there.
"
1527	506	monnand	2014-07-19 00:32:10	CONTRIBUTOR	"@lavalamp I made some changes. PTAL.

It is definitely not a final version. I was trying to support arbitrary data type nicely for attribute values. What I'm trying to do is:
- Do not use interface as attribute types.
- Try to not use string. Because I personally don't want to see too much ""string-typed"" thing. For example, I don't thing the following things is what we want:

```
{
""attr1"": ""{\""some_user_defined_obj\"": \""this may not be a good idea\""}""
}
```

What I want to do is just to check if the attribute value is a valid json object, i.e. validate it, then give the raw json object to scheduler and let it unmarshal it with its own data type. I'm not sure if I'm allow to do that with current json library.
"
1528	506	monnand	2014-07-19 01:02:40	CONTRIBUTOR	"@lavalamp I just wrote some toy program with ResourceAttributeValue defined in this PR. It works as expected. You could take a look at [here](http://play.golang.org/p/SoX1qRhrno). (Or, you could read this PR directly.)

@thockin Feel free to chime in.
"
1529	506	lavalamp	2014-07-19 01:07:52	MEMBER	"I think this is headed in the right direction. I'll have more specific comments next week.
"
1530	506	monnand	2014-07-19 01:14:40	CONTRIBUTOR	"@lavalamp Glad you say that :) I make several small changes to make it easy to use for user-defined attribute types. Feel free to make comment (next week).
"
1531	493	lavalamp	2014-07-19 01:50:56	MEMBER	"Fixed merge. @smarterclayton-- I've got another PR ready to go that I think will address your concern, so no issue for the moment.
"
1532	506	thockin	2014-07-19 05:12:55	MEMBER	"I have a bunch of nits I would pick om this one, but let's wait for John's proposal to stabilize before locking it down.
"
1533	503	thockin	2014-07-19 05:20:55	MEMBER	"I think there's value in those abstractions, but to my naive ears they
sound like something built atop the core k8s primitives.  We might still
want to endorse and ambrace them, but they are, by principle, a layer
above.  I think.

On Thu, Jul 17, 2014 at 11:46 AM, Clayton Coleman notifications@github.com
wrote:

> In Kubernetes, the reference from a container manifest to an image is a
> ""name"" - that name is arbitrary and it is up to the user to specific how
> that name interacts with their docker build and docker registry scenarios.
> That includes ensuring that the name and label the user uses to refer to
> their image is not changed accidentally (so that new images aren't
> introduced outside of a controlled deployment process) and that the
> registry DNS that hosts the images is continuously available as long as
> that image may be needed (see docker image discussions for how this might
> change https://github.com/dotcloud/docker/issues/6805).
> 
> That loose coupling is valuable for flexibility, but the lack of a
> concrete process leaves room for error and requires thought and control. In
> addition, the resolution of those names is tightly bound to the execution
> of the container in the Kubelet.
> 
> We think there is value in Kubernetes providing a set of higher level
> concepts above pods/replication controllers that can be used to create
> deployable units of containers. Two concepts we see as valuable are
> ""builds"" and ""deployments"" - the former can be used to compose new images
> (by leveraging the Kubernetes cluster for build slaves with resource
> control) and the latter can manage the process of transitioning between one
> set of podTemplates to another (and can be triggered by builds).
> 
> First, is this something that should be in Kubernetes? Should it be on top
> of Kubernetes as a separate server? Or is it something that could be
> optionally enabled by those who wish to work on it?
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/503.
"
1534	529	lavalamp	2014-07-19 05:37:52	MEMBER	"@thockin: I think your comment got lost when I rebased. I've corrected the comment (it was confusing because it was completely wrong--from an earlier version of the PR) and added a few more.
"
1535	505	lavalamp	2014-07-19 06:18:17	MEMBER	"@smarterclayton: I added auth checking, and rebased on top of #529. Didn't look into why this file uses TLS test server.
"
1536	503	smarterclayton	2014-07-19 13:00:22	CONTRIBUTOR	"Agreed - I don't think pods or replication controllers know anything about builds or deployments, in fact, the layering is reversed - a type of build should be able to use a run once pod to accomplish its goal, while a type of deployment _may_ depend on a particular sequence of calls to replication controllers.
"
1537	505	smarterclayton	2014-07-19 13:14:12	CONTRIBUTOR	"LGTM
"
1538	529	thockin	2014-07-19 18:09:26	MEMBER	"LGTM, but I don't know this area of the code that well.
"
1539	519	kelseyhightower	2014-07-20 02:09:43	CONTRIBUTOR	"This issue is resolved. Closing.
"
1540	127	lexlapax	2014-07-20 04:00:24	NONE	"I would agree as well, as long as we're open to having a way to extend those apis later to the pod-level  , when required. Thanks. 
"
1541	531	jonboulle	2014-07-20 04:56:45	CONTRIBUTOR	"Build still failing (err undefined this time)..
"
1542	531	thockin	2014-07-20 05:04:11	MEMBER	"Build fixed
"
1543	532	thockin	2014-07-20 05:07:22	MEMBER	"I see you're with CoreOS.  I do not yet see a CoreOS CLA on file, nor a personal CLA from you.  I know sometimes corp CLAs take a while to run through (yay lawyers).  we can't commit this until we have one, though.
"
1544	532	jonboulle	2014-07-20 05:09:04	CONTRIBUTOR	"See https://github.com/GoogleCloudPlatform/kubernetes/pull/520#issuecomment-49493906 - it was signed the other day, so happy for this to land whenever it propagates.
"
1545	531	jonboulle	2014-07-20 05:14:36	CONTRIBUTOR	"cool, lgtm now
"
1546	532	thockin	2014-07-20 05:16:59	MEMBER	"Sounds good.  I'll let @lavalamp handle them at the same time.

On Sat, Jul 19, 2014 at 10:09 PM, Jonathan Boulle notifications@github.com
wrote:

> See #520 (comment)
> https://github.com/GoogleCloudPlatform/kubernetes/pull/520#issuecomment-49493906
> - it was signed the other day, so happy for this to land whenever it
>   propagates.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/532#issuecomment-49537386
> .
"
1547	352	kelseyhightower	2014-07-20 05:48:15	CONTRIBUTOR	"I've been testing Kubernetes with Docker 1.1.1, and everything seems to be working fine. I'm not sure if anything special needs to happen. 
"
1548	505	smarterclayton	2014-07-20 13:58:01	CONTRIBUTOR	"Probably need a second reviewer on the apiobject part - since it's part of types.  Anyone still here this week?  :)
"
1549	536	lavalamp	2014-07-20 15:25:17	MEMBER	"Thanks for the change!
"
1550	539	vmarmol	2014-07-20 16:59:42	CONTRIBUTOR	"I'm also including the source for an assembly version which is ~150 bytes. The trouble is for some reason that one does not respond to signals when inside a Docker container :( so it takes ~10s to ""docker stop"". This is the same behavior as today's busybox-based image. The Go-based google/pause does return immediately.
"
1551	538	kelseyhightower	2014-07-20 17:05:59	CONTRIBUTOR	"This failure seems unrelated:

```
--- FAIL: TestWatchControllers (0.01s)
    replication_controller_test.go:473: Expected 1 call but got 0
```
"
1552	539	lavalamp	2014-07-20 17:56:31	MEMBER	"LGTM for the go one, my assembly is pretty rusty so someone else should look at that.
"
1553	538	lavalamp	2014-07-20 17:59:23	MEMBER	"LGTM
"
1554	537	lavalamp	2014-07-20 17:59:53	MEMBER	"Thanks for the cleanup!
"
1555	356	smarterclayton	2014-07-20 18:01:20	CONTRIBUTOR	"All comments addressed except for the refactor of NewConfigSource\* (which I'd like to do in a follow up).  I've made it so that:
- If a manifest.ID is provided, all config sources will carry it forward into the Pod.Name
- If that results in a conflict, PodConfig will detect and drop it.
- Validation added to kubelet/validation.go to ensure that Pod.Name is checked for previous Manifest.ID checks prior to being put in the config map (will be logged and dropped otherwise).
"
1556	505	lavalamp	2014-07-20 18:02:33	MEMBER	"Brendan and Tim are both out next week, so it's looking like just the two of us. :) If this is still sitting on Monday, I may self-merge and fight for it later because it's gonna hold me up.
"
1557	356	lavalamp	2014-07-20 18:04:07	MEMBER	"I still owe this PR a good look. Monday I will make sure I have time to do so.
"
1558	540	lavalamp	2014-07-20 18:04:53	MEMBER	"Awesome, thanks for the speedup :)
"
1559	539	smarterclayton	2014-07-20 18:09:46	CONTRIBUTOR	"Won't compile on Mac (which is expected, but probably wouldn't hurt to add a linux check in the Makefile)

```
nasm -o pause pause.asm
pause.asm:5: error: `64' is not a valid segment size; must be 16 or 32
make: *** [pause] Error 1
```
"
1560	529	smarterclayton	2014-07-20 18:25:33	CONTRIBUTOR	"LGTM - @brendanburns when you're back from vacation take a look at this and we can fix it after.
"
1561	541	lavalamp	2014-07-20 18:31:53	MEMBER	"Thanks so much for cleaning up my mess. :)
"
1562	505	smarterclayton	2014-07-20 18:37:15	CONTRIBUTOR	"Yeah, I merged the other and we can do a follow up after.
"
1563	541	kelseyhightower	2014-07-20 18:37:34	CONTRIBUTOR	"@lavalamp No worries. I'll be at 100% coverage later today
"
1564	539	vmarmol	2014-07-20 18:40:38	CONTRIBUTOR	"@smarterclayton added the condition in the makefile
"
1565	542	lavalamp	2014-07-20 19:43:56	MEMBER	"Awesome, thanks.
"
1566	535	lavalamp	2014-07-20 20:47:17	MEMBER	"@smarterclayton, both comments addressed in #505.
"
1567	540	thockin	2014-07-21 00:44:20	MEMBER	"How is it a speedup?  Isnt this dockers internal default behavior?
On Jul 20, 2014 11:05 AM, ""Daniel Smith"" notifications@github.com wrote:

> Awesome, thanks for the speedup :)
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/540#issuecomment-49554331
> .
"
1568	540	lavalamp	2014-07-21 01:24:49	MEMBER	"From vmarmol's description:

> When no tag is given to Docker pull, it downloads all tags. This could be a significantly large number of images and in the end we only ever run :latest.

Sounds like less time from pod creation to running to me.
"
1569	540	thockin	2014-07-21 01:46:18	MEMBER	"Ah, I missed that.  SGTM
On Jul 20, 2014 6:25 PM, ""Daniel Smith"" notifications@github.com wrote:

> From vmarmol's description:
> 
> When no tag is given to Docker pull, it downloads all tags. This could be
> a significantly large number of images and in the end we only ever run
> :latest.
> 
> Sounds like less time from pod creation to running to me.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/540#issuecomment-49566058
> .
"
1570	534	monnand	2014-07-21 01:58:57	CONTRIBUTOR	"If it could be generated through godoc, then it should be [here](https://godoc.org/github.com/GoogleCloudPlatform/kubernetes)
"
1571	534	rgarcia	2014-07-21 02:08:06	NONE	"godoc does generate and publish docs, e.g.: https://godoc.org/github.com/GoogleCloudPlatform/kubernetes/pkg/kubecfg

Whether this constitutes good CLI documentation, though...
"
1572	547	philips	2014-07-21 02:37:16	CONTRIBUTOR	"this looks much nicer +1
"
1573	546	lavalamp	2014-07-21 03:49:38	MEMBER	"Thanks!
"
1574	547	lavalamp	2014-07-21 03:55:50	MEMBER	"Big improvement, thanks.
"
1575	534	lavalamp	2014-07-21 03:56:52	MEMBER	"Godoc is good for API documentation, but probably not CLI documentation.
"
1576	538	kelseyhightower	2014-07-21 06:01:58	CONTRIBUTOR	"@lavalamp This should be good to go now.
"
1577	545	kelseyhightower	2014-07-21 06:02:57	CONTRIBUTOR	"PTAL
"
1578	538	lavalamp	2014-07-21 06:03:06	MEMBER	"Cool, thanks for all the fixes!
"
1579	545	lavalamp	2014-07-21 06:10:44	MEMBER	"LGTM, but can you squash commits please?
"
1580	545	kelseyhightower	2014-07-21 06:15:21	CONTRIBUTOR	"Commits squashed. PTAL.
"
1581	548	kelseyhightower	2014-07-21 07:08:51	CONTRIBUTOR	"@lavalamp I'll make the necessary changes tomorrow. 
"
1582	544	pmorie	2014-07-21 13:43:54	MEMBER	"Seems like #137 is a dep of this, agree @smarterclayton ?
"
1583	544	ncdc	2014-07-21 13:44:56	MEMBER	"@pmorie makes sense to me, yeah
"
1584	544	smarterclayton	2014-07-21 13:45:42	CONTRIBUTOR	"Part of this will be to put in place the framework for #137 - killed in the triple represents some of that data.  Agree this should set the foundation for that.
"
1585	544	smarterclayton	2014-07-21 13:49:02	CONTRIBUTOR	"I'll probably also introduce PodInstanceID at the same time as ContainerAttemptID (which is the DockerID) from https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/identifiers.md
"
1586	544	ironcladlou	2014-07-21 14:03:18	CONTRIBUTOR	"> on container manifest, add a restart policy value for the pod as a whole

Not sure I understand that statement; in #127 there's an open discussion re: per-pod/per-container policy. Which direction are you headed?
"
1587	545	kelseyhightower	2014-07-21 14:05:07	CONTRIBUTOR	"The build failed because of gvm:

```
gvm install go1.3 --preferbinary
WARNING: Invalid option --preferbinary
Usage: gvm install [version] [options]
    -s,  --source=SOURCE      Install Go from specified source.
    -n,  --name=NAME          Override the default name for this version.
    -pb, --with-protobuf      Install Go protocol buffers.
    -b,  --with-build-tools   Install package build tools.
    -B,  --binary             Only install from binary.
    -h,  --help               Display this message.
The command ""gvm install go1.3 --preferbinary"" failed and exited with 65 during .
```
"
1588	545	kelseyhightower	2014-07-21 14:06:24	CONTRIBUTOR	"@lavalamp All changes have been made. PTAL.
"
1589	544	smarterclayton	2014-07-21 14:18:21	CONTRIBUTOR	"I'll update to say per container.
"
1590	544	thockin	2014-07-21 15:21:05	MEMBER	"I know Dawn was starting to look at this too - might be worth making sure
we don't overlap too much.

I do think restart should be a per-container thing.

I also want to think a bit about HA.  Maybe it is not important here but
worth explicitly ruling out.  Internally, we had some real problems with
the agent becoming unavailable for various reasons, which meant restarts
were not being processed.

We solved this by moving restarts out of the agent proper and into a third
party (very much like systemd) which was simpler, had a single purpose, and
was updated less frequently.

In the first containervm we solved it by wrapping each container run in a
shall loop which would do restarts no matter what happened on the main
agent.

Like I said - not sure it matters here, but worth thinking about.  What is
our target SLA for container restarts?
On Jul 21, 2014 7:18 AM, ""Clayton Coleman"" notifications@github.com wrote:

> I'll update to say per container.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/544#issuecomment-49610903
> .
"
1591	544	smarterclayton	2014-07-21 15:28:15	CONTRIBUTOR	"As I was thinking about this problem, I think I'd argue for a PodControl abstraction that the sync loop invokes to manage the details of the actual pod - that allows a separate subsystem to manage restart policy as well down the road.

Dawn, let me know what you'll cover and whether there are pieces you want to split up.
"
1592	543	smarterclayton	2014-07-21 15:30:54	CONTRIBUTOR	"@derekwaynecarr can you reconcile your Vagrantfile with this pull
"
1593	543	derekwaynecarr	2014-07-21 15:43:29	MEMBER	"@smarterclayton will do, thx for the heads-up
"
1594	539	vmarmol	2014-07-21 17:29:59	CONTRIBUTOR	"@smarterclayton ah forgot that comment, made into a const.
"
1595	187	derekwaynecarr	2014-07-21 17:40:50	MEMBER	"I submitted a PR with the approach here: 
https://github.com/GoogleCloudPlatform/kubernetes/pull/550
"
1596	527	Sarsate	2014-07-21 17:59:50	CONTRIBUTOR	"Comments addressed, ptal @thockin.
"
1597	543	derekwaynecarr	2014-07-21 18:04:15	MEMBER	"@rafael I submitted a PR here: https://github.com/GoogleCloudPlatform/kubernetes/pull/550

It would be awesome if we can reconcile and if you can provide your feedback.

The PR I submitted lets you work better with replicas in that you can provision multiple minions.  In addition, it reuses the existing Salt State configuration shared with GCE environments.  Editing source and pushing out to each VM works well because you can just run vagrant provision again, and very quickly each node in the cluster is updated.
"
1598	543	rafael	2014-07-21 18:18:25	CONTRIBUTOR	"@derekwaynecarr Oh I like what you did in your vagrant setup. Nice! I think the easiest way would be to merge your branch first and I adapt the wiki instructions on my PR to use your vagrant setup. What do you think? 
"
1599	543	derekwaynecarr	2014-07-21 18:23:13	MEMBER	"Works for me!
"
1600	543	smarterclayton	2014-07-21 18:41:51	CONTRIBUTOR	"@rafael can you help review the PR and give any feedback?  Bunch of folks are out this week so any extra eyes help :)
"
1601	551	lavalamp	2014-07-21 18:55:16	MEMBER	"This is something I've wanted for a while, but we may consider waiting until we get #356 in-- one of you  is going to be in merge hell for a while, and @smarterclayton has already been there for quite some time. I'm in the middle of reading that one right now, will look more carefully at this later today.
"
1602	543	rafael	2014-07-21 18:57:17	CONTRIBUTOR	"@smarterclayton sure! Will do. I'll get a chance to dig into it by the end of the afternoon. I'll provide feedback today :) 
"
1603	520	jonboulle	2014-07-21 19:05:48	CONTRIBUTOR	"@lavalamp any luck?
"
1604	520	lavalamp	2014-07-21 19:11:40	MEMBER	"@jonboulle I sent an email. Will let you know later today, hopefully.
"
1605	457	dchen1107	2014-07-21 19:15:31	MEMBER	"I did rebase, also replace httputil.DumpResponse with ioutil.ReadAll since there are a race issue found in dump.go in go1.2. Travis build is failed due to the command ""gvm install"" failed. I noticed that several other PR failed due to the same reason. I ran the tests many times on my local machine, and all succeed!
"
1606	356	lavalamp	2014-07-21 20:17:15	MEMBER	"I'm pretty happy with what this PR does in terms of separating responsibilities. In the interest of making progress here, I recommend that you look through my comments, fix any that you find important/easy, make TODOs for others, rebase, and we'll merge and iterate from there. I'll try not to merge anything that conflicts in the meantime.
"
1607	543	lavalamp	2014-07-21 20:31:46	MEMBER	"This is great! I do have one overall suggestion, though--what do you think about just making a set of scripts that launch a vagrant cluster? Then you don't have to duplicate any of the example code.

See: https://github.com/GoogleCloudPlatform/kubernetes/commit/95ec94514bcaa7dbaf114199d56b17088601975a for all the places where the azure startup scripts were added.
"
1608	543	derekwaynecarr	2014-07-21 20:47:24	MEMBER	"@lavalamp I submitted a PR here: #550 

This creates a vagrant multi-machine cluster to run locally.
"
1609	545	kelseyhightower	2014-07-21 21:17:33	CONTRIBUTOR	"Is there an easy way to rerun Travis on this PR?
"
1610	545	lavalamp	2014-07-21 21:19:41	MEMBER	"Supposedly travis is fixed now: https://github.com/travis-ci/travis-ci/issues/2547

I don't know of an easy way to trigger a re-run, though. Make a trivial change and push? :(
"
1611	539	smarterclayton	2014-07-21 21:23:21	CONTRIBUTOR	"LGTM
"
1612	539	lavalamp	2014-07-21 21:27:52	MEMBER	"Dare I merge while travis is down (all go builds failing, though supposedly a fix has been merged)?
"
1613	539	smarterclayton	2014-07-21 21:29:22	CONTRIBUTOR	"That statement is tempting the build break / revert gods.
"
1614	550	lavalamp	2014-07-21 21:30:27	MEMBER	"Would love to merge this, but maybe someone with more familiarity with salt and/or our config files should LGTM first.
"
1615	545	proppy	2014-07-21 21:39:21	CONTRIBUTOR	"restarted the build
"
1616	356	smarterclayton	2014-07-21 21:39:51	CONTRIBUTOR	"Tried to capture everything as a fix except the go routine issues and the config resync (but added todos in both places).  ptal
"
1617	457	lavalamp	2014-07-21 21:55:27	MEMBER	"I restarted travis. Looks like it's still broken for go :(
"
1618	442	smarterclayton	2014-07-21 22:00:20	CONTRIBUTOR	"Updated the description at top to capture the three scenarios.

@erictune - the approach you described makes a lot of sense - hard limits for big, fungible limits for soft.  There's also benefit to allowing temporary resource usage over soft limits for small teams (to within limits that they perhaps define).
"
1619	551	erictune	2014-07-21 22:07:48	MEMBER	"happy to wait for Clayton's PR.
"
1620	356	lavalamp	2014-07-21 22:13:13	MEMBER	"I hope travis gets fixed soon :(
"
1621	356	smarterclayton	2014-07-21 22:18:35	CONTRIBUTOR	"Time to switch to drone
"
1622	544	dchen1107	2014-07-21 22:19:35	MEMBER	"Clayton, I started a PR simply adding a restart policy for the container to container manifest with a default that is equivalent to the current API behavior (restart always).

Initially I planed to put more effort on restart policy, but I am fine that you take over the whole issue or handle over the entire one to me. 
"
1623	544	smarterclayton	2014-07-21 22:26:46	CONTRIBUTOR	"Up to you - if you've got other items you'd like to take I'm happy to cover this one.
"
1624	508	smarterclayton	2014-07-21 22:27:59	CONTRIBUTOR	"I'm going to rebase this for Brendan once travis comes up so we can merge.
"
1625	550	smarterclayton	2014-07-21 22:31:38	CONTRIBUTOR	"From a clean VM box this fails with:

```
[master] Booting VM...
[master] Waiting for machine to boot. This may take a few minutes...
[master] Machine booted and ready!
[master] Setting hostname...
The following SSH command responded with a non-zero exit status.
Vagrant assumes that this means the command failed!

service network restart

Stdout from the command:

Restarting network (via systemctl):  [FAILED]

kubernetes-master network[1155]: RTNETLINK answers: File exists
Jul 21 22:30:58 kubernetes-master systemd[1]: network.service: control process exited, code=exited status=1
Jul 21 22:30:58 kubernetes-master systemd[1]: Failed to start LSB: Bring up/down networking
```
"
1626	544	dchen1107	2014-07-21 22:35:42	MEMBER	"If you don't mind, I would like to take over this working item. I am still new to kubernete and golang, want to take this over to familiar with the entire workflow and codebase. Thanks!
"
1627	550	derekwaynecarr	2014-07-21 22:37:58	MEMBER	"@smarterclayton you are first to see that error, but let's synch in the morning so I can improve the documentation.  What version of vagrant are you running?  I was running vagrant 1.6.2.  In addition, I removed the debian option from the Vagrantfile because it was not yet tested enough by me, and in provision-minion.sh, there is a systemctl restart call to get around a race when installing the salt-minion that I would need to update.  I can look at this tomorrow.
"
1628	550	derekwaynecarr	2014-07-21 22:40:12	MEMBER	"@smarterclayton also I tested from Fedora 20 laptop using VirtualBox, maybe your Mac setup is causing a behavior difference?
"
1629	505	lavalamp	2014-07-21 22:47:03	MEMBER	"@smarterclayton: Addressed comments. PTAL.
"
1630	545	lavalamp	2014-07-21 22:49:12	MEMBER	"Thanks again for the changes!
"
1631	457	dchen1107	2014-07-21 23:00:27	MEMBER	"@lavalamp Travis is back to normal, and I think this PR is ready for merge now?
"
1632	356	lavalamp	2014-07-21 23:01:05	MEMBER	"Argh, I merged Victor's change and I think it must conflict, which I didn't realize. Can you rebase? :(
"
1633	457	lavalamp	2014-07-21 23:02:46	MEMBER	"Thanks for the change!
"
1634	506	erictune	2014-07-21 23:31:34	MEMBER	"Was there a discussion of the relative merits of of generic resources (like this PR proposes) vs a more concrete resource struct (where ""cpu"", ""memory"", etc are part of the go language identifiers).  I have opinions on this, but want to know the history before I jump in.
"
1635	356	smarterclayton	2014-07-21 23:38:13	CONTRIBUTOR	"I'll rebase and merge if you're not around when I get back.
"
1636	506	monnand	2014-07-21 23:43:11	CONTRIBUTOR	"@erictune I'll talk with you offline tomorrow.
"
1637	506	lavalamp	2014-07-21 23:46:59	MEMBER	"Please invite me to that conversation.
"
1638	506	monnand	2014-07-21 23:48:23	CONTRIBUTOR	"@lavalamp Definitely we will.
"
1639	552	lavalamp	2014-07-21 23:59:22	MEMBER	"> change around the if grains['cloud'] != 'azure' to if grains['cloud'] == 'gce'?

Yes, please!

I don't know salt config scripts at all. I'm worried about us ending up with an maintainable ""if cloud == blah"" soup as we add more providers... :/
"
1640	506	jonboulle	2014-07-21 23:59:40	CONTRIBUTOR	"Could you post a summary of the conversation for those of us not invited offline? :-)
"
1641	506	lavalamp	2014-07-22 00:05:27	MEMBER	"We're working on getting an internal doc published, which should make our thinking on this plain. Trying to get that out for comment soon, that will be even better than this conversation :)
"
1642	552	jeffmendoza	2014-07-22 00:06:47	MEMBER	"no problem will do
"
1643	506	monnand	2014-07-22 00:07:21	CONTRIBUTOR	"Oops. I made some stupid mistakes and pushed an old version of code when I did a rebase. I'll try to push the latest version later. Sorry about that.
"
1644	553	mohit	2014-07-22 00:09:18	NONE	"This addresses https://github.com/GoogleCloudPlatform/kubernetes/issues/499 and https://github.com/GoogleCloudPlatform/kubernetes/issues/526
"
1645	506	lavalamp	2014-07-22 00:12:03	MEMBER	"@monnand, you may want to just wait until we can get this doc published and have some reaction. I have many comments on this PR but have avoided saying them so far, hopefully we can get this doc out soon.
"
1646	552	derekwaynecarr	2014-07-22 00:17:33	MEMBER	"@lavalamp @jeffmendoza in general, I like the Salt configuration right now as it made it easier to support alternate Linux distributions with fewer changes in pull #550 .  I just want to make sure that we do no co-mingle the cloud provider with the underlying os type when making changes to avoid having a messy soup.
"
1647	553	lavalamp	2014-07-22 00:18:17	MEMBER	"Thanks for the patch :) but this is somewhat complex:

This involves changing public api, so it needs a wide review. Unfortunately @brendanburns, @thockin, and @jbeda are all out right now, and @smarterclayton and I do not make a quorum, so this may have to sit until next week.

I will note that, as written, this is going to cause passwords to be recorded in plain text in etcd, which doesn't sound like a good thing to me. We don't really have a good auth solution right now, unfortunately.

This also duplicates credentials for every pod. I think that, if we decide to do this, we'd need to make DockerRegistry a first class object, with the credentials stored only once.
"
1648	553	thockin	2014-07-22 00:18:41	MEMBER	"I would like to review this.  Please do not merge yet.  Eta tonight.
"
1649	520	lavalamp	2014-07-22 00:28:47	MEMBER	"@jonboulle we've received it, but it's not been processed yet, and I'm told I need to wait until it's made its way through our system. Unfortunately, ETA is a couple days. I guess the corporate ones take a bit longer than the individual ones. :(
"
1650	520	jonboulle	2014-07-22 00:30:21	CONTRIBUTOR	"@lavalamp doh, OK then. Not a huge deal, but I'll hold off on other patches for now. Thanks for the update.
"
1651	520	lavalamp	2014-07-22 00:31:13	MEMBER	"Sorry! :(
"
1652	554	thockin	2014-07-22 00:32:45	MEMBER	"The only reason I can see is ""trying to be goan"".  But if I engage my C
brain, which is closer to Go, yeah pointers...
On Jul 21, 2014 5:30 PM, ""monnand"" notifications@github.com wrote:

> I noticed that there're lots of methods' parameters are passed by value.
> For example, the scheduler interface is defined as
> 
> type Scheduler interface {
>     Schedule(pod api.Pod, minionList MinionLister) (selectedMachine string, err error)
> }
> 
> pod is passed by value. Is there any particular reason why we want to do
> that?
> 
> I can see one ""benefit"" which is immutability. However, api.Pod contains
> a map member, which is a reference type and this makes the immutability
> invalid --- even the caller passed the pod by value to Schedule(), the
> caller can still change the map member. Similar thing happens in
> ContainerManifest, which has slice members.
> 
> The obvious disadvantage of passing by value is the cost of copying large
> data structures. We have several large data structures passed around by
> value (api.Pod, manifest, etc.) and it would be an expensive operation.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/554.
"
1653	553	mohit	2014-07-22 00:32:51	NONE	"Thanks! I should describe the options we considered for this:
1. (this PR) Allowing each pod to be pulled from a different registry:
   - Will continue to work as an override when another default authentication method has been implemented
   - Does not change behavior for existing containers and minimal impact on internal communication/state between API and Kubelet.
   - From our perspective, `Pod` configs already contain `env` variables that are considered secure. This means we are considering _etcd_ security more broadly.  
2. Minion setup is responsible for setting up docker registry and auth (I have an implementation of this I can share if that would help)
   - This would require the use of a config file. This could be either `.dockercfg` or the kubelet config 
   - The Kubelet would use this configuration when making requests to the docker host.
   - More secure because kubernetes does not have to worry about authentication state.
   - Again simple change that does not affect existing state/communication
   - Does not allow for overrides/custom registry for pods (unless 1. is also implemented).

I'd love to know what other considerations or ideas have been floating around.
"
1654	554	lavalamp	2014-07-22 00:39:13	MEMBER	"There's no benefit for objects with a map, it's just a bug. :)

I'd actually make the argument that, for consistency, all our api objects should be passed around as pointers. This is especially important when you store things in an interface{}, because *api.Pod and api.Pod are different types and it's silly to have to check for both. Also, if you do anything with reflection, you almost certainly want to start with a pointer (or you have to go through CanAddr() hell).
"
1655	553	smarterclayton	2014-07-22 01:00:40	CONTRIBUTOR	"One general concern I have with anything auth related is the incoming provenance / trust changes that will happen with registry names and namespaces (registry hosts will be decoupled from the ""namespace"" of the image in the locator).  I get very nervous about leading that in a quasi stable API, because of all the implications around image identity.  We should probably try to be very specific about what each field we add around images means.

I do think there's room for a mapping between the identifier in the container manifest for an image and the real image / registry.
"
1656	554	monnand	2014-07-22 01:03:52	CONTRIBUTOR	"I believe this is a bug. However, changing all places will be really a pain. I'm trying to fix the scheduler package now and will send a PR later. Is there any automatic way to do this conversion? `gofmt -r` may help to a certain extent but has to involve some human work. Does @nf have any suggestion here?
"
1657	554	lavalamp	2014-07-22 01:06:26	MEMBER	"This is why it hasn't already been fixed :)
"
1658	554	nf	2014-07-22 01:11:00	CONTRIBUTOR	"Yeah, I agree they should be passed around as pointers.

It's probably best to just change it all by hand. I think it'd just require a single pass over the codebase, and then if it compiles it's OK.
"
1659	553	smarterclayton	2014-07-22 01:11:56	CONTRIBUTOR	"Also, I'm generally in favor of this pull, but the state of auth in Docker is terrible right now and it _will_ change, so I get scared about future proofing.  Registries in real environments will use every form of auth ever invented, so trying to be judicious about the API spec is relevant.
"
1660	553	smarterclayton	2014-07-22 01:14:50	CONTRIBUTOR	"Regarding etcd security there are two facets we need to consider - encrypt at rest of environment and other secure items, and defending the data store from compromise of a host.  Both should be separate issues - lavalamp and I discussed the latter a bit today and the Watch interface abstraction will help over time to abstract kubelets from being direct accessors of etcd (except in homogenous trust environments or where there is value in direct access).
"
1661	554	monnand	2014-07-22 01:16:06	CONTRIBUTOR	"Thank you @nf. That's what I'm doing now. Some places use empty interface, which makes it hard to find.

@lavalamp I see. I think it should be fixed soon. Otherwise, people will treat this as a convention and make it even harder to fix. I'll send a PR later.
"
1662	554	lavalamp	2014-07-22 01:23:22	MEMBER	"Thanks @monnand, that would be wonderful.
"
1663	550	smarterclayton	2014-07-22 01:26:55	CONTRIBUTOR	"Add .vagrant to .gitignore
"
1664	550	derekwaynecarr	2014-07-22 01:32:12	MEMBER	"I already did :-)

Sent from my iPhone

> On Jul 21, 2014, at 9:27 PM, Clayton Coleman notifications@github.com wrote:
> 
> Add .vagrant to .gitignore
> 
> —
> Reply to this email directly or view it on GitHub.
"
1665	550	rafael	2014-07-22 01:36:04	CONTRIBUTOR	"@derekwaynecarr  I provided some feedback related to Vagrant that is the part I'm most familiarized with. I didn't experience the problem that @smarterclayton  mentioned. The provisioning worked for me! I'm on Mac OS, Virtual Box 4.3.12 and Vagrant 1.6.3. 

I'm going to play with the cluster to see if I can get the examples running! 
"
1666	550	derekwaynecarr	2014-07-22 01:37:01	MEMBER	"Awesome.  Thanks for the feedback!

Sent from my iPhone

> On Jul 21, 2014, at 9:36 PM, Rafael Chacon notifications@github.com wrote:
> 
> @derekwaynecarr I provided some feedback related to Vagrant that is the part I'm most familiarized with. I didn't experience the problem that @smarterclayton mentioned. The provisioning worked for me! I'm on Mac OS, Virtual Box 4.3.12 and Vagrant 1.6.3.
> 
> I'm going to play with the cluster to see if I can get the examples running!
> 
> —
> Reply to this email directly or view it on GitHub.
"
1667	550	smarterclayton	2014-07-22 01:37:23	CONTRIBUTOR	"I'm on vagrant 1.4.3
"
1668	550	derekwaynecarr	2014-07-22 01:38:58	MEMBER	"Time to upgrade.  Will make sure the doc prereqs the right vagrant version.  Sounds like 1.6.3 worked for Rafael 

Sent from my iPhone

> On Jul 21, 2014, at 9:37 PM, Clayton Coleman notifications@github.com wrote:
> 
> I'm on vagrant 1.4.3
> 
> —
> Reply to this email directly or view it on GitHub.
"
1669	356	lavalamp	2014-07-22 01:41:14	MEMBER	"Alright, I'm merging. Thanks for the change :)
"
1670	550	smarterclayton	2014-07-22 01:42:36	CONTRIBUTOR	"Have the Vagrantfile crash and burn on older versions so folks don't have to read the docs.

> On Jul 21, 2014, at 9:39 PM, Derek Carr notifications@github.com wrote:
> 
> Time to upgrade. Will make sure the doc prereqs the right vagrant version. Sounds like 1.6.3 worked for Rafael 
> 
> Sent from my iPhone 
> 
> > On Jul 21, 2014, at 9:37 PM, Clayton Coleman notifications@github.com wrote: 
> > 
> > I'm on vagrant 1.4.3 
> > 
> > — 
> > Reply to this email directly or view it on GitHub. 
> > —
> > Reply to this email directly or view it on GitHub.
"
1671	552	lavalamp	2014-07-22 01:45:41	MEMBER	"@derekwaynecarr like I said, I don't know salt at all... Agree that cloud provider and os type should be independent, otherwise there's a combinatorial explosion involved in supporting every combination...
"
1672	527	smarterclayton	2014-07-22 02:07:39	CONTRIBUTOR	"Would /var/lib/kubernetes be better?  It's more ""correct"" (in that lib is already where major software puts its state) and I think will be easier to convince packagers on.  Also, can we make this a const so package maintainers can change it at build time if they need to?
"
1673	505	smarterclayton	2014-07-22 02:11:00	CONTRIBUTOR	"LGTM.  We should have Tim or Brendan go over this entire chain when they get back for additional feedback, but in the meantime...
"
1674	550	smarterclayton	2014-07-22 02:38:44	CONTRIBUTOR	"I don't think you need .vagrant/**, just do /.vagrant in the .gitignore
"
1675	556	smarterclayton	2014-07-22 02:40:11	CONTRIBUTOR	"There's a good set of validations defined in pkg/api/validation.go, but could definitely use more.
"
1676	550	rafael	2014-07-22 02:40:49	CONTRIBUTOR	"I'm having some issues starting up the pods. The kubernetes master show them as created, but when I connect to the minion they are not running. For instance, this is what the master shows for redis-master:

```
{
    ""kind"": ""PodList"",
    ""items"": [
        {
            ""id"": ""97f3d1a0-1148-11e4-80a3-0800279696e1"",
            ""labels"": {
                ""name"": ""redis-master"",
                ""replicationController"": ""redisMasterController""
            },
            ""desiredState"": {
                ""manifest"": {
                    ""version"": ""v1beta1"",
                    ""id"": ""97f3d1a0-1148-11e4-80a3-0800279696e1"",
                    ""volumes"": null,
                    ""containers"": [
                        {
                            ""name"": """",
                            ""image"": ""dockerfile/redis"",
                            ""ports"": [
                                {
                                    ""hostPort"": 6379,
                                    ""containerPort"": 6379
                                }
                            ]
                        }
                    ]
                }
            },
            ""currentState"": {
                ""manifest"": {
                    ""version"": """",
                    ""id"": """",
                    ""volumes"": null,
                    ""containers"": null
                },
                ""host"": ""10.245.2.2""
            }
        }
    ]
}
```

And the minion:

```
[root@kubernetes-minion-1 vagrant]# ifconfig |grep 10.245.2.2
        inet 10.245.2.2  netmask 255.255.255.0  broadcast 10.245.2.255
[root@kubernetes-minion-1 vagrant]# docker ps
CONTAINER ID        IMAGE                       COMMAND                CREATED             STATUS              PORTS                    NAMES
386fab0cf858        busybox:buildroot-2014.02   sh -c 'rm -f nap &&    7 minutes ago       Up 7 minutes        0.0.0.0:5000->8080/tcp   k8s--net--cadvisor_-_agent--66949c04
```

Also, for some reason the minion-2 doesn't have docker installed:

```
[root@kubernetes-minion-2 vagrant]# docker ps
bash: docker: command not found
[root@kubernetes-minion-2 vagrant]# 
```

Do you know where are the logs for kubernetes in the minions? I was trying /var/log but couldn't find them there.
"
1677	550	derekwaynecarr	2014-07-22 02:41:27	MEMBER	"Good idea.  Will update now that we know it does crash and burn.  

Sent from my iPhone

> On Jul 21, 2014, at 9:42 PM, Clayton Coleman notifications@github.com wrote:
> 
> Have the Vagrantfile crash and burn on older versions so folks don't have to read the docs. 
> 
> > On Jul 21, 2014, at 9:39 PM, Derek Carr notifications@github.com wrote: 
> > 
> > Time to upgrade. Will make sure the doc prereqs the right vagrant version. Sounds like 1.6.3 worked for Rafael 
> > 
> > Sent from my iPhone 
> > 
> > > On Jul 21, 2014, at 9:37 PM, Clayton Coleman notifications@github.com wrote: 
> > > 
> > > I'm on vagrant 1.4.3 
> > > 
> > > — 
> > > Reply to this email directly or view it on GitHub. 
> > > — 
> > > Reply to this email directly or view it on GitHub. 
> > > —
> > > Reply to this email directly or view it on GitHub.
"
1678	550	derekwaynecarr	2014-07-22 02:45:11	MEMBER	"Run docker images on each minion and you may still see the minion pulling the image you requested.  

You can see logs using journalctl -u kubelet  or journalctl -u apiserver or journalctl -u controller-manager 

If you do not see docker on your minion, just run vagrant provision again and it will update the cluster again. 

Note after the kubelet starts it pulls a lot of images that you can see calling docker images so it can take a few for your initial pod to appear.  

Let me know if you see the issue repeat. 

Sent from my iPhone

> On Jul 21, 2014, at 10:41 PM, Rafael Chacon notifications@github.com wrote:
> 
> I'm having some issues starting up the pods. The kubernetes master show them as created, but when I connect to the minion they are not running. For instance, this is what the master shows for redis-master:
> 
> {
>     ""kind"": ""PodList"",
>     ""items"": [
>         {
>             ""id"": ""97f3d1a0-1148-11e4-80a3-0800279696e1"",
>             ""labels"": {
>                 ""name"": ""redis-master"",
>                 ""replicationController"": ""redisMasterController""
>             },
>             ""desiredState"": {
>                 ""manifest"": {
>                     ""version"": ""v1beta1"",
>                     ""id"": ""97f3d1a0-1148-11e4-80a3-0800279696e1"",
>                     ""volumes"": null,
>                     ""containers"": [
>                         {
>                             ""name"": """",
>                             ""image"": ""dockerfile/redis"",
>                             ""ports"": [
>                                 {
>                                     ""hostPort"": 6379,
>                                     ""containerPort"": 6379
>                                 }
>                             ]
>                         }
>                     ]
>                 }
>             },
>             ""currentState"": {
>                 ""manifest"": {
>                     ""version"": """",
>                     ""id"": """",
>                     ""volumes"": null,
>                     ""containers"": null
>                 },
>                 ""host"": ""10.245.2.2""
>             }
>         }
>     ]
> }
> And the minion:
> 
> [root@kubernetes-minion-1 vagrant]# ifconfig |grep 10.245.2.2
>         inet 10.245.2.2  netmask 255.255.255.0  broadcast 10.245.2.255
> [root@kubernetes-minion-1 vagrant]# docker ps
> CONTAINER ID        IMAGE                       COMMAND                CREATED             STATUS              PORTS                    NAMES
> 386fab0cf858        busybox:buildroot-2014.02   sh -c 'rm -f nap &&    7 minutes ago       Up 7 minutes        0.0.0.0:5000->8080/tcp   k8s--net--cadvisor_-_agent--66949c04
> Also, for some reason the minion-2 doesn't have docker installed:
> 
> [root@kubernetes-minion-2 vagrant]# docker ps
> bash: docker: command not found
> [root@kubernetes-minion-2 vagrant]# 
> Do you know where are the logs for kubernetes in the minions? I was trying /var/log but couldn't find them there.
> 
> —
> Reply to this email directly or view it on GitHub.
"
1679	550	derekwaynecarr	2014-07-22 02:46:31	MEMBER	"Also recommend calling which docker when logged in as vagrant user.  It may not be in the root user PATH.

You can always see docker is there using

 systemctl status docker

Sent from my iPhone

> On Jul 21, 2014, at 10:41 PM, Rafael Chacon notifications@github.com wrote:
> 
> I'm having some issues starting up the pods. The kubernetes master show them as created, but when I connect to the minion they are not running. For instance, this is what the master shows for redis-master:
> 
> {
>     ""kind"": ""PodList"",
>     ""items"": [
>         {
>             ""id"": ""97f3d1a0-1148-11e4-80a3-0800279696e1"",
>             ""labels"": {
>                 ""name"": ""redis-master"",
>                 ""replicationController"": ""redisMasterController""
>             },
>             ""desiredState"": {
>                 ""manifest"": {
>                     ""version"": ""v1beta1"",
>                     ""id"": ""97f3d1a0-1148-11e4-80a3-0800279696e1"",
>                     ""volumes"": null,
>                     ""containers"": [
>                         {
>                             ""name"": """",
>                             ""image"": ""dockerfile/redis"",
>                             ""ports"": [
>                                 {
>                                     ""hostPort"": 6379,
>                                     ""containerPort"": 6379
>                                 }
>                             ]
>                         }
>                     ]
>                 }
>             },
>             ""currentState"": {
>                 ""manifest"": {
>                     ""version"": """",
>                     ""id"": """",
>                     ""volumes"": null,
>                     ""containers"": null
>                 },
>                 ""host"": ""10.245.2.2""
>             }
>         }
>     ]
> }
> And the minion:
> 
> [root@kubernetes-minion-1 vagrant]# ifconfig |grep 10.245.2.2
>         inet 10.245.2.2  netmask 255.255.255.0  broadcast 10.245.2.255
> [root@kubernetes-minion-1 vagrant]# docker ps
> CONTAINER ID        IMAGE                       COMMAND                CREATED             STATUS              PORTS                    NAMES
> 386fab0cf858        busybox:buildroot-2014.02   sh -c 'rm -f nap &&    7 minutes ago       Up 7 minutes        0.0.0.0:5000->8080/tcp   k8s--net--cadvisor_-_agent--66949c04
> Also, for some reason the minion-2 doesn't have docker installed:
> 
> [root@kubernetes-minion-2 vagrant]# docker ps
> bash: docker: command not found
> [root@kubernetes-minion-2 vagrant]# 
> Do you know where are the logs for kubernetes in the minions? I was trying /var/log but couldn't find them there.
> 
> —
> Reply to this email directly or view it on GitHub.
"
1680	550	rafael	2014-07-22 03:19:20	CONTRIBUTOR	"Thanks @derekwaynecarr ! Will try to debug further on my end. 
"
1681	553	thockin	2014-07-22 04:27:21	MEMBER	"On Mon, Jul 21, 2014 at 5:33 PM, mohit notifications@github.com wrote:

> Thanks! I should describe the options we considered for this:
> 
> (this PR) Allowing each pod to be pulled from a different registry:
> 
> Will continue to work as an override when another default authentication method has been implemented
> Does not change behavior for existing containers and minimal impact on internal communication/state between API and Kubelet.
> 
> From our perspective, Pod configs already contain env variables that are considered secure. This means we are considering etcd security more broadly.

Why said env vars are secure?  I would argue that NOTHING in
kubernetes is particularly secure at the moment - we don't even really
have a concept of identity.

As it is we do not even try to protect the contents of a manifest -
storing anythign secure in there is probably not a great idea, long
term.

> Minion setup is responsible for setting up docker registry and auth (I have an implementation of this I can share if that would help)
> 
> This would require the use of a config file. This could be either .dockercfg or the kubelet config
> The Kubelet would use this configuration when making requests to the docker host.
> More secure because kubernetes does not have to worry about authentication state.
> Again simple change that does not affect existing state/communication
> Does not allow for overrides/custom registry for pods (unless 1. is also implemented).
> 
> I'd love to know what other considerations or ideas have been floating around.

Idea: Introduce a concept of identity.  When user 'alice' runs a job,
we copy her credentials (intentionally vague) to the minion on which
her job will run.  Those credentials include registry access keys.

Similar: Define set of static registries and copy that to each minion.
Allow a manifest reference a registry by name, but make that an
out-of-band definiiton
"
1682	535	lavalamp	2014-07-22 04:35:22	MEMBER	"I think my implementation here isn't quite right-- I fixed it in #549. Can just combine with that (and drop this PR) if you want.
"
1683	554	monnand	2014-07-22 04:57:16	CONTRIBUTOR	"@lavalamp This is even harder than I thought. There's lots of empty interfaces making everything harder. We should have a policy to restrict the use of empty interface. Otherwise, the code will go wild.

I will try to send a PR by tonight. It will be a large PR, but won't change any feature. So I hope it is easy to review for you. If you have time, would you please review and merge it by tomorrow morning? Because that PR will be very hard to rebase.
"
1684	554	thockin	2014-07-22 04:58:41	MEMBER	"You'll be better off sending a bunch of small changes.

On Mon, Jul 21, 2014 at 9:57 PM, monnand notifications@github.com wrote:

> @lavalamp https://github.com/lavalamp This is even harder than I
> thought. There's lots of empty interfaces makes everything harder.
> 
> I will try to send a PR by tonight. It will be a large PR, but won't
> change any feature. So I hope it is easy to review for you. If you have
> time, would you please review and merge it by tomorrow morning? Because
> that PR will be very hard to rebase.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/554#issuecomment-49698307
> .
"
1685	554	monnand	2014-07-22 05:00:15	CONTRIBUTOR	"@thockin I wish I could. But it's hard to make small changes while pass all tests. I'll try to split it.
"
1686	554	lavalamp	2014-07-22 05:06:15	MEMBER	"Yeah, I understand it's pretty hard to change a bunch of refs all over and still have something that builds. I'll probably be up for another hour, maybe two. I'll try not to merge stuff in the meantime.
"
1687	539	thockin	2014-07-22 05:26:09	MEMBER	"I'm all in favor of cleverness, but I do not think there's any place for assembly in this codebase.  This could be a 3 line C program, which is a thousand times more maintainable.
"
1688	554	lavalamp	2014-07-22 05:26:47	MEMBER	"@monnand, I see you're a few commits behind master-- you might want to rebase sooner rather than later :)
"
1689	554	monnand	2014-07-22 05:28:14	CONTRIBUTOR	"@lavalamp Yes. I'm rebasing now. Everything breaks during the rebase. Let me try to fix it.
"
1690	554	monnand	2014-07-22 05:46:23	CONTRIBUTOR	"@lavalamp OK. I give up. Rebasing is hard. But I do think we need to mark this issue as a bug and see who wants to fix it.
"
1691	554	lavalamp	2014-07-22 05:47:40	MEMBER	"You had the misfortune of starting while sync'd right before @smarterclayton's monster CL. :(
"
1692	554	monnand	2014-07-22 05:57:03	CONTRIBUTOR	"To those who are brave enough to solve this problem, here are the steps which I used to train the dragon:
- Change the scheduler interface to `Scheduler(pod *api.Pod, minionList MinionList) (string, error)`
- Change all `api.Pod` to `*api.Pod` whenever you saw it. `gofmt -r` won't help. `grep` is your friend.
- Run unit tests and solve whatever error you saw from the compiler --- You need to first make a compilable version.
- If you are brave enough to make a compilable version, run the unit tests. they will fail and you need to solve them.
- Be aware: the `registry` package is a monster filled with empty interfaces --- your unit tests will panic in many places. (This is another big challenge we need to fix. Let's reduce the usage of empty interface and bring the type system back into our code. Burn this witch!)
- Repeat the same process for `api.Container`, `api.ReplicationControllerState`, `api.PodTemplate`, `api.ReplicationController`, `kubelet.Pod`. I only got here and failed. There's definitely more than that.
- Good luck! May the force be with you.
"
1693	554	lavalamp	2014-07-22 05:59:38	MEMBER	"You may have better luck changing one thing at a time-- eg, change Scheduler, make it build & pass tests, change the next thing, etc.
"
1694	554	monnand	2014-07-22 06:01:39	CONTRIBUTOR	"@lavalamp Yes. I was too ambitious. Maybe tomorrow night, I'll try to fix the scheduler.
"
1695	554	dchen1107	2014-07-22 06:08:15	MEMBER	"@monnand and @lavalamp, we might help monnand on this tomorrow morning by
each one of us own a package, and make the changes, and pass the tests.
Before we finish the updates, hold all merges. Not sure if it is a good
idea or not, but thought we should change it as soon as possible before it
gets worse.

On Mon, Jul 21, 2014 at 11:01 PM, monnand notifications@github.com wrote:

> @lavalamp https://github.com/lavalamp Yes. I was too ambitious. Maybe
> tomorrow night, I'll try to fix the scheduler.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/554#issuecomment-49701347
> .
"
1696	557	lavalamp	2014-07-22 06:16:08	MEMBER	"I'd like @bgrant0607 to take a look at the PodStatuses I'm adding.
"
1697	551	lavalamp	2014-07-22 06:18:38	MEMBER	"Clayton's PR is now merged. Have fun rebasing...
"
1698	549	lavalamp	2014-07-22 06:28:43	MEMBER	"My rebase broke this somehow. Will figure it out tomorrow.
"
1699	554	smarterclayton	2014-07-22 12:39:04	CONTRIBUTOR	"I'll make the change in the kubelet packages separately.  I need to introduce a proper deep copy to ensure internal pod config state is not malleable.  Going to grab https://godoc.org/code.google.com/p/rog-go/exp/deepcopy
"
1700	554	smarterclayton	2014-07-22 12:42:43	CONTRIBUTOR	"Actually not that package - probably just do it directly.  Disregard.
"
1701	558	corrieb	2014-07-22 13:59:23	NONE	"Note that hacking the script in the way I described only works when invoking release.sh directly and breaks the when invoked indirectly from ./dev-build-and-up.sh, so it's clearly not an appropriate fix.
"
1702	550	derekwaynecarr	2014-07-22 14:15:39	MEMBER	"@smarterclayton updated .gitignore, and modified Vagrantfile to do Vagrant.require_version "">= 1.6.2""
"
1703	558	corrieb	2014-07-22 14:37:04	NONE	"Ahhh... so all scripts need to be run from the root and then all the relative directory assumptions work. Closing as not a bug.
"
1704	550	rafael	2014-07-22 15:03:04	CONTRIBUTOR	"@derekwaynecarr After waiting some time pods are starting. I think it was just the waiting time while it was pulling the images. 
"
1705	550	derekwaynecarr	2014-07-22 15:07:00	MEMBER	"@rafael i suspected as much.  If you spin up multiple nodes in your cluster there is a lot to download intiially as each box asks the kubernetes-master to provision it via the salt config.  Then the kubelet on each minion pulls all the prerequisite images.  The good thing is this is one time, and you can preserve your cluser with vagrant halt and update the cluster with vagrant provision, where the second provision is much quicker at applying updates.  Thanks so much for your input.  I am updating the README.md with more details based on your questions.
"
1706	356	thockin	2014-07-22 16:22:57	MEMBER	"LGTM overall
"
1707	559	dchen1107	2014-07-22 16:47:17	MEMBER	"@ryfow Ryan, I haven't figured out the root cause of your issue yet, but I think I can help you rule out those validation failures on the cadvisor manifest. I saw the exact same validation errors on my kubelet.log, but I still can query /api/v1beta1/pods through apiserver. Both my apiserver and kubelet from yesterday afternoon, not include the latest merged PRs last night. Will try to see if I could reproduce your issue here. 
"
1708	559	dchen1107	2014-07-22 16:53:00	MEMBER	"I just pulled the code from HEAD, and ran into the same problem as Ryan. I need to attend a meeting now. Right after that, I will look into it if no one else are working it. 
"
1709	554	lavalamp	2014-07-22 16:56:46	MEMBER	"One more thought-- IMO, when an api type is embedded in another (like Pods in a PodList), the embedding should _not_ be a pointer.
"
1710	554	thockin	2014-07-22 17:03:49	MEMBER	"What is the GC behavior if I have a slice of T and I store a pointer to one
of those T?  Does it extend the life of the whole slice?
On Jul 22, 2014 9:56 AM, ""Daniel Smith"" notifications@github.com wrote:

> One more thought-- IMO, when an api type is embedded in another (like Pods
> in a PodList), the embedding should _not_ be a pointer.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/554#issuecomment-49767401
> .
"
1711	553	smarterclayton	2014-07-22 17:08:12	CONTRIBUTOR	"To idea 1 (keys tied to identity) I think that makes sense EXCEPT that the image might be owned by a different identity than the identity that created the pod.  Instead, the owning identity has to be the subgrouping of resources (the project, to borrow from #442, or the entire deployment as per today) and so there would have to be a set of static registries defined per project that pods created in that project have access to.  By definition a ""project"" is the group of resources which have a common access control set.

Token based authentication to docker repositories is substantially less fraught from a security perspective than credentials or client certs - with appropriately scoped tokens (to read or write to a single repository) you can minimize the risk of compromise of any individual host or minion.  If the risk of compromise of a minion is high, and appropriate security measures are taken to limit direct access to etcd, the token being stored in the manifest isn't really a significant problem because it should provide only a limited right to that image.
"
1712	554	lavalamp	2014-07-22 17:13:15	MEMBER	"> What is the GC behavior if I have a slice of T and I store a pointer to one of those T?  Does it extend the life of the whole slice?

I'd be surprised if it were smart enough not to. But I think we should optimize our API types for readability and correctness. If GC becomes a major issue for us, that implies a lot of success on other fronts and I'll be happy to personally fix it. :)
"
1713	553	mohit	2014-07-22 17:17:50	NONE	"Adding a full identity system into Kubernetes seems like a long term (or out of scope) resolution to registry authentication.

Distributing out of band static registry credentials would suffice for most use cases. However, there would be added complexity of referencing those identities in the Manifests - making them dependent on the current state of each minion. My current understanding is that all state is stored in `etcd`. If not as part of the host setup process (salt etc), can you point me to how these out-of-band credentials could be distributed?

I do hope that Docker starts supporting Token based auth (hopefully tokens can be created using an API), because that would mean that each Manifest could contain a tightly scoped generated token. 
"
1714	554	thockin	2014-07-22 17:18:12	MEMBER	"I don't see much readability difference between pointers and values,
though.  Not disagreeing with you just saying...
On Jul 22, 2014 10:13 AM, ""Daniel Smith"" notifications@github.com wrote:

> What is the GC behavior if I have a slice of T and I store a pointer to
> one of those T? Does it extend the life of the whole slice?
> 
> I'd be surprised if it were smart enough not to. But I think we should
> optimize our API types for readability and correctness. If GC becomes a
> major issue for us, that implies a lot of success on other fronts and I'll
> be happy to personally fix it. :)
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/554#issuecomment-49769698
> .
"
1715	554	lavalamp	2014-07-22 17:21:14	MEMBER	"Fair enough :)

It's also worth pointing out that, from a GC perspective, storing pointers in a list is not a clear win, because it produces way more garbage overall. Would have to do an experiment.
"
1716	558	lavalamp	2014-07-22 17:25:13	MEMBER	"I still consider it a bug-- I'll happily merge PRs that remove working directory assumptions.
"
1717	559	lavalamp	2014-07-22 17:29:46	MEMBER	"@smarterclayton I seem to recall a cAdvisor path change in your PR? Related?
"
1718	559	smarterclayton	2014-07-22 17:31:16	CONTRIBUTOR	"I will look
"
1719	553	smarterclayton	2014-07-22 17:38:15	CONTRIBUTOR	"The actual docker registry image auth is actually token based - the client has to get a token first by doing basic auth.

Static credentials could be distributed via etcd, just via a separate sync loop from manifests.
"
1720	552	jeffmendoza	2014-07-22 18:05:09	MEMBER	"@lavalamp Ok, updated. I added the cloud grain to the default salt configs, and clarified the if statements.
"
1721	554	monnand	2014-07-22 18:34:20	CONTRIBUTOR	"Thank you, all! I'll join you tonight to avoid merging large PRs.

[The code in my branch](https://github.com/monnand/kubernetes/compare/pass-by-ref-1) may still be useful. 

@smarterclayton Restricting changes in one package is hard because those data structures and methods are spread between packages. I would suggest to focus on data structures. e.g. Make all `api.Pod` be pointers whenever it should be. But feel free to use any method as you saw fit.

Since the deepcopy package is in exp/, not sure it is ready to use.

@thockin I have not read the code of GC in Go, but I think my understanding is correct:

```
a := make([]MyA, 100)
// Assign some values into a
x := a[10]
```

When a is unaccessible, and x is available, then all members stored in a's data field will be marked as free and a (the slice structure) is also marked.

```
a := make([]*MyA, 100)
// Assign some values into a
x := a[10]
```

When a is unaccessible, and x is available, then all members stored in a's data field will be marked as free because they are pointers and a (the slice structure) is also marked. The space pointed by x is not marked because it is used by x. However, other members pointed through a will be released if there's no pointer points it them.

TL;DR: It works same as a correct C++ code.

@lavalamp Go 1.3's GC is precise now, so I'm not sure what do you mean by ""more garbage"". We do need to allocate more memory because each structure will take its size + a pointer. But I don't think this is a disadvantage. 

In fact, _not_ storing pointers in a slice will generate _more_ garbage. Consider the following case:

```
a := make([]MyA, 100)
// Assign some values into a
x := &a[10]
```

When a is unaccessible, and x is available, then all members stored in a's data field will _not_ be marked as free because they are allocated as one chunk and have to be freed as one single chunk.

I think we may need @nf or other people from Go team to answer some questions we have:
- Is `code.google.com/p/rog-go/exp/deepcopy` a recommended way of doing deepcopy? If not, then what's the best way to do a deep copy?
- Should we store data structures into a slice of structures, or a slice of pointers to structures? (I believe the answer will be: small data structures are fine to store them into slice; large ones would be better to store them as pointers.) 
- @thockin and @lavalamp's discussion about GC.
"
1722	559	dchen1107	2014-07-22 18:42:03	MEMBER	"Just back from the meeting. 

@ryfow I think the issue is caused by incompatibility between apiserver and kubelete. I noticed that there is a error msg in the initial apiserver.log your posed: E0722 14:55:52.425200 14134 pod_cache.go:79] Error synchronizing container list: &url.Error{Op:""Get"", URL:""http://kubernetes-minion-4.c.first-try-ryfow.internal:10250/healthz"", Err:(*net.OpError)(0xc2103ca2c0)}

It is the same error I observed on my apiserver html page. Looks like apiserver failed to retrieve healthz/ from minions, then failed the query of /api/v1beta1/pods. 

To prove my hypothesis, I teared down my entire cluster and built a new one, and everything runs fine. I guess you just simply push the new kubelet binary over to some of minions in your cluster? 

Unfortunately I haven't figured out the exact where healthz/ failed yet. 
"
1723	554	lavalamp	2014-07-22 19:00:27	MEMBER	"My point was that `a := make([]*MyA, 100)` makes (potentially) 101 pieces of garbage that can be cleaned up independently, and `a := make([]MyA, 100)` makes 1 piece of garbage that must be cleaned up all at once. It's not clear to me that one is better than the other in general with respect to GC.

The latter offers fewer chances to forget a != nil check, and fewer such checks everywhere, which I consider a readability improvement. :)
"
1724	554	smarterclayton	2014-07-22 19:01:15	CONTRIBUTOR	"+1 to that sentiment
"
1725	527	Sarsate	2014-07-22 19:12:55	CONTRIBUTOR	"@smarterclayton Good idea, I think /var/lib/kubelet makes the most sense here. I've also added the const.

@thockin comments addressed, ptal
"
1726	562	lavalamp	2014-07-22 19:33:27	MEMBER	"Oh, whoops, I did this. Perhaps instead we should add a ""/"" handler in apiserver that can redirect to ""/"" to ""/index.html""?
"
1727	562	dchen1107	2014-07-22 19:55:53	MEMBER	"Thought you were doing that on purpose. Ok, will send you PR in a minute. 
"
1728	562	dchen1107	2014-07-22 20:11:18	MEMBER	"Done!
"
1729	551	erictune	2014-07-22 20:19:29	MEMBER	"PTAL
"
1730	562	dchen1107	2014-07-22 20:20:11	MEMBER	"Looking into travis failures. 
"
1731	551	smarterclayton	2014-07-22 20:25:54	CONTRIBUTOR	"LGTM aside from the question about mocking in an integration environment.
"
1732	562	smarterclayton	2014-07-22 20:26:31	CONTRIBUTOR	"LGTM aside from travis failures.
"
1733	551	erictune	2014-07-22 20:32:51	MEMBER	"Do you mind waiting until such an integration test is written to add that
argument?

On Tue, Jul 22, 2014 at 1:26 PM, Clayton Coleman notifications@github.com
wrote:

> LGTM aside from the question about mocking in an integration environment.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/551#issuecomment-49795589
> .
"
1734	551	smarterclayton	2014-07-22 20:38:38	CONTRIBUTOR	"Sure - just wanted to clarify.  A comment on the factory method would help (expose members for integration testing)
"
1735	562	dchen1107	2014-07-22 20:39:24	MEMBER	"I understood the failure in TestBadPath in apiserver_test.go. ServeMux tries to match the URL of each request against a list of registered patterns, and  calls the handler for the pattern that MOST CLOSELY MATCHES the URL. I guess this is the reason @lavalamp give notFound handler to ""/"" so that all arbitrary requests beginning with ""/"" wouldn't serve index.html. 

There are two ways to fix this: 1) Take my initial commit to point to the right path at the end of building a cluster or 2) Modify the test above, and allow accept any arbitrary requests being redirect to index.html. I personally prefer 2), how do you guys think?
"
1736	562	dchen1107	2014-07-22 20:43:14	MEMBER	"Just saw @lavalamp's updates. Will send PR soon. 
"
1737	562	smarterclayton	2014-07-22 20:43:58	CONTRIBUTOR	"I tend to prefer not found always, except for special paths - / feels special.  Maybe have the default handler (/) check for the root path and return not found otherwise.
"
1738	562	lavalamp	2014-07-22 20:48:51	MEMBER	"Yeah. Arbitrary paths should give an error-- please don't change that test!
"
1739	562	dchen1107	2014-07-22 20:55:21	MEMBER	"Done!
"
1740	562	lavalamp	2014-07-22 20:56:42	MEMBER	"One nit, LGTM otherwise. Thanks!
"
1741	551	erictune	2014-07-22 21:05:32	MEMBER	"Ok, added a comment.
"
1742	260	smarterclayton	2014-07-22 21:08:47	CONTRIBUTOR	"Service design I think deserves some discussion as Brian notes.  Currently it couples an infrastructure abstraction (local proxy) with a mechanism for exposure (environment variables in all containers) with a label query.  There is an equally valid use case for an edge proxy that takes L7 hosts/paths and balances them to a label query, as well as supporting protocols like http(s) and web sockets.  In addition, services have a hard scale limit today of 60k backends, shared across the entire cluster (the amount of IPs allocated).  It should be possible to run a local proxy on a minion that proxies only the services the containers on that host need, and also to avoid containers having to know about the external port.  We can move this discussion to #494 if necessary.
"
1743	551	lavalamp	2014-07-22 21:10:02	MEMBER	"LGTM, but please squash commits before we merge.
"
1744	565	lavalamp	2014-07-22 21:44:07	MEMBER	"Will try to review quickly, then. Aren't you supposed to be on vacation? :)
"
1745	565	brendandburns	2014-07-22 21:49:46	CONTRIBUTOR	"want to get this in.  rebasing it for a week is going to kill me...

(integration tests failing, though.  looking into it...)
"
1746	551	erictune	2014-07-22 21:51:13	MEMBER	"This was my first attempt at squashing and my history was a bit complex, so beware.
"
1747	566	lavalamp	2014-07-22 22:01:36	MEMBER	"@smarterclayton any way this could be due to your config changes?
"
1748	566	smarterclayton	2014-07-22 22:07:22	CONTRIBUTOR	"Possible, testing.
"
1749	567	lavalamp	2014-07-22 22:09:10	MEMBER	"LGTM - I'm going to merge this as soon as travis finishes because I consider it an emergency bug fix. Not sure how these weren't Encode/Decode already.
"
1750	567	lavalamp	2014-07-22 22:44:17	MEMBER	"(taking this over while brendan goes back on vacation)

I see the problem. We store []ContainerManifests in etcd for kubelet. Thinking about this.
"
1751	567	dchen1107	2014-07-22 23:54:16	MEMBER	"Can someone point to me to the bug / issue # for this PR? Thanks!
"
1752	566	smarterclayton	2014-07-22 23:57:48	CONTRIBUTOR	"I was able to get update to work, but it looks like delete of a service is not purging the service from the proxy map.  Did you update the service via the REST API or kubecfg (because it looks like kubecfg is broken for update, or at least... doesn't work for me, which I'm fixing in a related pull)?
"
1753	566	smarterclayton	2014-07-23 00:08:52	CONTRIBUTOR	"Did a create on port 9999, then an update to 9998 (no endpoints in this case), and got this dump (which shows that the config is properly set, so probably not my change).  The proxier may not be expecting a port change.  Will look tomorrow.  Also, proxy/config/etcd.go needs a deeper rewrite to match kubelet/config/etcd.go at some point.  It's possible for the channels to race and cause out of order updates (delete delivered before add, for example).

```
I0723 00:03:34.063100 04224 logs.go:39] etcd DEBUG: [recv.success. http://127.0.0.1:4001/v2/keys/registry/services/endpoints/frontend?consistent=true&recursive=false&sorted=true]
I0723 00:03:34.063125 04224 etcd.go:140] Got service: frontend on localport 9998 mapping to: {frontend []}
I0723 00:03:34.063144 04224 config.go:233] Setting services {[{{ frontend   0} 9998 map[] map[name:frontend] false {0 0 }}] 0}
I0723 00:03:34.063164 04224 config.go:138] Setting endpoints {[{frontend []}] 0}
I0723 00:03:34.063170 04224 proxier.go:127] Received update notice: [{JSONBase:{Kind: ID:frontend CreationTimestamp: SelfLink: ResourceVersion:0} Port:9998 Labels:map[] Selector:map[name:frontend] CreateExternalLoadBalancer:false ContainerPort:{Kind:0 IntVal:0 StrVal:}}]
I0723 00:03:35.400360 04224 logs.go:39] etcd DEBUG: [recv.response.from http://127.0.0.1:4001/v2/keys/registry/services?consistent=true&recursive=true&wait=true&waitIndex=269]
I0723 00:03:35.400391 04224 logs.go:39] etcd DEBUG: [recv.success. http://127.0.0.1:4001/v2/keys/registry/services?consistent=true&recursive=true&wait=true&waitIndex=269]
I0723 00:03:35.400444 04224 logs.go:39] etcd DEBUG: get [/registry/services/ http://127.0.0.1:4001] [%!s(MISSING)]
I0723 00:03:35.400468 04224 logs.go:39] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/services?consistent=true&recursive=true&wait=true&waitIndex=270]
I0723 00:03:35.400478 04224 logs.go:39] etcd DEBUG: [send.request.to  http://127.0.0.1:4001/v2/keys/registry/services?consistent=true&recursive=true&wait=true&waitIndex=270  | method  GET]
I0723 00:03:35.400498 04224 etcd.go:190] Processing a change in service configuration... {set %!s(*etcd.Node=&{/registry/services/endpoints/frontend {""Name"":""frontend"",""Endpoints"":[]} false <nil> 0 [] 269 269}) %!s(*etcd.Node=&{/registry/services/endpoints/frontend {""Name"":""frontend"",""Endpoints"":[]} false <nil> 0 [] 268 268}) %!s(uint64=268) %!s(uint64=5423) %!s(uint64=0)}
I0723 00:03:35.400523 04224 etcd.go:221] Processing a change in endpoint configuration... {set %!s(*etcd.Node=&{/registry/services/endpoints/frontend {""Name"":""frontend"",""Endpoints"":[]} false <nil> 0 [] 269 269}) %!s(*etcd.Node=&{/registry/services/endpoints/frontend {""Name"":""frontend"",""Endpoints"":[]} false <nil> 0 [] 268 268}) %!s(uint64=268) %!s(uint64=5423) %!s(uint64=0)}
I0723 00:03:35.400616 04224 config.go:128] Adding new endpoint from source etcd : [{frontend []}]
```
"
1754	568	dchen1107	2014-07-23 00:28:31	MEMBER	"LGTM. 
"
1755	569	smarterclayton	2014-07-23 00:29:42	CONTRIBUTOR	"This was a flaw in my checkin the other night and _could_ result in kubelets being out of sync with the master in some cases.
"
1756	567	smarterclayton	2014-07-23 00:31:01	CONTRIBUTOR	"Tim and I were talking about changing etcd to store container manifest pod but it's a big change.  We theoretically want to store a kubelet-api object for the kubelet, not an apiserver-api object.
"
1757	570	lavalamp	2014-07-23 01:21:11	MEMBER	"Thanks for the fix!
"
1758	567	lavalamp	2014-07-23 01:25:35	MEMBER	"This is a prereq for brendan's api versioning change. I got it partly working but had to go to a meeting.

Here's what I'm doing: I'm making Endpoints be an official api type with a JSONBase, so that Encode and Decode work properly with it. I'm also making a new ContainerManifestList as a replacement for the plain []ContainerManifest we use at the moment. I don't think the change will be too horrible. I hope to put out a working one today.
"
1759	567	lavalamp	2014-07-23 01:58:03	MEMBER	"Closing this in favor of #571.
"
1760	551	lavalamp	2014-07-23 01:58:50	MEMBER	"Thanks!
"
1761	561	smarterclayton	2014-07-23 02:35:45	CONTRIBUTOR	"Discussed a bit in #260 already.  I've got some folks looking at adding arbitrary load balancer units and label based query backends for http(s), sni, and websockets - will have them describe some of what they're working on soon.
"
1762	559	smarterclayton	2014-07-23 02:37:33	CONTRIBUTOR	"We started validating manifests in the kubelet (using Tim's code) with my pull.
"
1763	571	lavalamp	2014-07-23 03:41:16	MEMBER	"Well, I was making it consistent with PodList, ServiceList, etc... Arguably, Endpoints is the inconsistent one.
"
1764	571	lavalamp	2014-07-23 03:43:16	MEMBER	"Whoops, missed a file. Hopefully travis will pass now.
"
1765	566	lavalamp	2014-07-23 04:18:31	MEMBER	"Heh, #468 seems to be reporting the same thing.
"
1766	565	brendandburns	2014-07-23 04:36:09	CONTRIBUTOR	"Ok, comments addressed.  unit tests & integration tests pass on my machine.
"
1767	565	brendandburns	2014-07-23 05:08:56	CONTRIBUTOR	"e2e tests are failing.  Looks like the validation code in the kubelet expects a name and a version.

Let's hold this until the etcd changes are in.
"
1768	571	thockin	2014-07-23 05:30:27	MEMBER	"LGTM overall - I really want to split the API type between Kubelet and Master - I agree with the net result here, but I'm finding it increasingly hard to keep straight.
"
1769	565	lavalamp	2014-07-23 05:33:58	MEMBER	"Oh, I bet e2e test broke with @smarterclayton's mega-PR, which starts using @thockin's validation thingy in the kubelet. I think I saw some chatter about this in an issue or on IRC or somewhere.
"
1770	571	lavalamp	2014-07-23 05:38:56	MEMBER	"I'll still wait for an LGTM from @brendandburns or @smarterclayton before merging this, just because it does change api/types.
"
1771	527	thockin	2014-07-23 05:40:47	MEMBER	"LGTM - someone should double that up.
"
1772	552	lavalamp	2014-07-23 05:44:01	MEMBER	"LGTM, but maybe @smarterclayton or @derekwaynecarr can also give it a look?
"
1773	570	brendandburns	2014-07-23 05:47:19	CONTRIBUTOR	"This breaks e2e tests.  Please see #572
"
1774	572	lavalamp	2014-07-23 05:52:41	MEMBER	"Oh, I should have caught that in review. Found another potential problem there, too, left a comment on #570. 
"
1775	549	lavalamp	2014-07-23 06:40:02	MEMBER	"Ah this actually depends on #571. Will push my rebased version once that is merged. Not sure how it was working for me without that.
"
1776	576	brendandburns	2014-07-23 06:54:13	CONTRIBUTOR	"Emergency bug fix.
"
1777	548	kelseyhightower	2014-07-23 07:26:51	CONTRIBUTOR	"@lavalamp Suggested changed made. PTAL.
"
1778	575	ryfow	2014-07-23 13:43:39	CONTRIBUTOR	"This is related to #559 
"
1779	577	smarterclayton	2014-07-23 14:19:38	CONTRIBUTOR	"Brendan got a fix in earlier in #572  - going to simplify this code down in the future so it doesn't happen again as part of a follow on to #570 
"
1780	577	smarterclayton	2014-07-23 14:20:07	CONTRIBUTOR	"Thanks for the pull
"
1781	578	smarterclayton	2014-07-23 14:30:45	CONTRIBUTOR	"LGTM
"
1782	565	smarterclayton	2014-07-23 14:42:53	CONTRIBUTOR	"brendan got the bulk of the e2e failures - I'm going to add some tests so I can't break this in the future this badly.
"
1783	575	vmarmol	2014-07-23 14:55:54	CONTRIBUTOR	"Closing in favor of #559 
"
1784	559	vmarmol	2014-07-23 14:56:20	CONTRIBUTOR	"Looking into fixing the cAdvisor manifest. Assigning to myself.
"
1785	580	smarterclayton	2014-07-23 14:58:33	CONTRIBUTOR	"Nevermind, my go processes locked up on fedora and were dead.  No concrete repro steps.
"
1786	584	thockin	2014-07-23 16:19:12	MEMBER	"I looked at this a while back.  I think the -f flag to hostname is POSIX
required, so if it doesn't work it is CoreOS that should be fixed.

I don't have a problem with this change, I just want to be super careful
about breakages.
On Jul 23, 2014 9:04 AM, ""Kelsey Hightower"" notifications@github.com
wrote:

> Currently the kubelet command executes the external hostname
> command with the -f flag in order to get the fully qualified
> hostname of the system. This is not reliable and does not
> work correctly on all Linux platforms, even when the FQDN is
> set correctly.
> 
> For example, on CoreOS the proper way to set the hostname
> is by updating the /etc/hostname file or running the following
> command:
> 
> sudo hostnamectl set-hostname standalone.example.com
> 
> Both methods of updating the hostname result in a /etc/hostname
> file with the following contents:
> 
> standalone.example.com
> 
> Running the hostname command we get unexpected results
> 
> hostnamestandalone.example.com
> 
> hostname -f
> hostname: Unknown host
> 
> The above results are related to how the hostname is handled
> on systemd based distros. The recommended way to get the
> hostname is to use the hostnamectl command:
> 
> hostnamectl
>        Static hostname: standalone.example.com
>          Icon name: computer-vm
>            Chassis: vm
>         Machine ID: ffdb68e30b03488ebe746b38db2a934f
>            Boot ID: 02badde8679c443db990d538c3621ab3
>     Virtualization: vmware
>   Operating System: CoreOS 379.3.0
>             Kernel: Linux 3.15.5+
>       Architecture: x86-64
> 
> To get just the hostname use the --static flag:
> 
> hostnamectl --staticstandalone.example.com
> 
> It should also be noted that some users will naturally want
> to avoid setting the domain on a system and instead rely on
> DNS search domains commonly configured in /etc/resolve.conf.
> The current kubelet command does not permit this use case.
> 
> Resolve these issues by using os.Hostname() which handles the
> hostname properly across all Linux distros. If the FQDN is set
> os.Hostname() will use it by default. Using os.Hostname()
> enables the use case where users prefer to omit the domain and
> rely on DNS search domains.
> 
> As a fallback option use the existing -hostname_override flag
> to set the hostname explicitly.
> 
> This patch introduces a change in behavior. Previously on
> systems where the kubelet service failed to start due to the
> inability to get the FQDN will now start up successfully and
> 
> ## use the hostname reported by the system.
> 
> You can merge this Pull Request by running
> 
>   git pull https://github.com/kelseyhightower/kubernetes hostname
> 
> Or view, comment on, or merge it at:
> 
>   https://github.com/GoogleCloudPlatform/kubernetes/pull/584
> Commit Summary
> - kubelet: Use os.Hostname() to get the system hostname
> 
> File Changes
> - _M_ cmd/kubelet/kubelet.go
>   https://github.com/GoogleCloudPlatform/kubernetes/pull/584/files#diff-0
>   (11)
> 
> Patch Links:
> - https://github.com/GoogleCloudPlatform/kubernetes/pull/584.patch
> - https://github.com/GoogleCloudPlatform/kubernetes/pull/584.diff
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/584.
"
1787	584	kelseyhightower	2014-07-23 16:24:37	CONTRIBUTOR	"That's what I get for only running the tests and not the build and integration too.
"
1788	583	ryfow	2014-07-23 16:26:05	CONTRIBUTOR	"I just tried it. Definitely fixes #559
"
1789	443	erictune	2014-07-23 16:31:30	MEMBER	"OAuth2 seems to offer lots of different usage models.  Getting it right seems like it could take some time.  

Is there a way we could unblock development on all the other things that rely on identity, while authn work is underway?  What do people think about using a list of user/passwds created at cluster startup + http basic auth to identify the principals that are making API calls.  Obviously this is not a good final solution, but it could be useful in test scenarios and it would unblock work on things that depend on authn.  
"
1790	581	lavalamp	2014-07-23 16:41:17	MEMBER	"Merging to fix bugs-- LGTM but maybe @thockin or @brendandburns can double check next time they're shirking their vacations.
"
1791	584	kelseyhightower	2014-07-23 16:45:06	CONTRIBUTOR	"@thockin Good points and I think you are right with regards to hostname -f being a POSIX requirement.

However, I see value in one less thing ""shelling out"" to the command line. I think we also offer a consistent experience for the end-user if we use os.Hostname.
"
1792	584	lavalamp	2014-07-23 16:46:21	MEMBER	"Hm... Does this work everywhere? I'm confused because the existing comment specifically said that os.Hostname() didn't give the fqdn?

The issue is that our scripts add the minions to the master _with_ the fqdn (or get them from the cloud provider that way), so if this doesn't match exactly, the minions won't be able to see the manifests that they're assigned in etcd.
"
1793	584	lavalamp	2014-07-23 16:47:25	MEMBER	"(Don't get me wrong, shelling out feels pretty gross and I'd like to stop.)
"
1794	583	lavalamp	2014-07-23 16:49:20	MEMBER	"Thanks! Also thanks @ryfow for the ""it works"", makes my life very easy. :)
"
1795	548	lavalamp	2014-07-23 16:50:28	MEMBER	"Thanks!
"
1796	584	kelseyhightower	2014-07-23 16:50:41	CONTRIBUTOR	"@lavalamp I've been using the following code to test on OS X and CoreOS

http://play.golang.org/p/ph23K6z1nJ
"
1797	584	thockin	2014-07-23 16:51:06	MEMBER	"I agree about shelling out, but disagree on consistency.  Without shelling
out, some users will see ""hostname"" and some will see ""hostname.domain.com"".
 That string is used in the etcd key, too, so th emaster sort of has to
know what the nodes are sticking in there.

hostname -f is as close to consistent as possible, unless we call
os.Hostname(), split on ""."", and use the first token.
On Jul 23, 2014 9:45 AM, ""Kelsey Hightower"" notifications@github.com
wrote:

> @thockin https://github.com/thockin Good points and I think you are
> right with regards to hostname -f being a POSIX requirement.
> 
> However, I see value in one less thing ""shelling out"" to the command line.
> I think we also offer a consistent experience for the end-user if we use
> os.Hostname.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/584#issuecomment-49900998
> .
"
1798	584	kelseyhightower	2014-07-23 16:53:06	CONTRIBUTOR	"@thockin I like the idea of using the split on ""."", seems the be what we are after. If people like that direction, I'll update the patch.

But I think we need to be sure we want to limit the options on DNS here. If the user really wants to use short names for DNS I think we should allow it. I think the effects on the hostname reach deeper than etcd namespace, it also seems to effect the proxy info given to pods I think. So if the hostname does not resolve then things break. 

I could be wrong about the Proxy and DNS, but I do recall seeing and error in the logs about not being able to resolve ""tcp://core.example.com:8000"" before. Thats when I realized DNS must be setup correctly. 
"
1799	581	thockin	2014-07-23 16:58:18	MEMBER	"LGTM
"
1800	584	thockin	2014-07-23 17:02:14	MEMBER	"I didn't write the code initially, so I don't know if there are broader
implications for just using the ""basename"" of the FQDN - e.g. do we expect
to support a cluster where two hosts have the same base hostname and
different FQDNs e.g. ""foo.one.mydomain.com"" and ""foo.two.mydomain.com"".
 Probably not an important use case.

To be devil's advocate, though, shouldn't we just fix 'hostname' to work
properly?  We shell out exactly once at kubelet startup.  Not a very big
deal..

On Wed, Jul 23, 2014 at 9:53 AM, Kelsey Hightower notifications@github.com
wrote:

> @thockin https://github.com/thockin I like the idea of using the split
> on ""."", seems the be what we are after. If people like that direction, I'll
> update the patch.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/584#issuecomment-49902230
> .
"
1801	578	lavalamp	2014-07-23 17:24:37	MEMBER	"It'd be cool if .md let us embed .json files located elsewhere so that we could locate this separately and write a test to keep it in sync.
"
1802	584	kelseyhightower	2014-07-23 17:33:59	CONTRIBUTOR	"So, this is not so straight forward. The existing comment is correct. os.Hostname() will only return the contents of /proc/sys/kernel/hostname. This file is populated by systemd after running the hostnamectl command. 

hostname -f will work even on CoreOS, but you'll need an entry in /etc/hosts. Seems like we need to continue to shell out and live with the requirement that users must have an /etc/hosts entry all platforms, which I don't think will be a huge problem. We might want to mention the /etc/hosts requirement in the docs.
"
1803	584	kelseyhightower	2014-07-23 17:35:23	CONTRIBUTOR	"The /etc/hosts added by GCE is why we don't see issues with hostname -f on debian:

```
root@debian:~# cat /etc/hosts
127.0.0.1   localhost
::1     localhost ip6-localhost ip6-loopback
fe00::0     ip6-localnet
ff00::0     ip6-mcastprefix
ff02::1     ip6-allnodes
ff02::2     ip6-allrouters

10.240.5.205 debian.c.hightower-labs.internal debian  # Added by Google
```
"
1804	585	lavalamp	2014-07-23 17:37:10	MEMBER	"Thanks for the report.
"
1805	584	kelseyhightower	2014-07-23 17:39:58	CONTRIBUTOR	"I'm going to spend effort in the stdlib to see if we can get support for os.DomainName or something added.  Please feel to close this out.
"
1806	587	lavalamp	2014-07-23 18:00:08	MEMBER	"minor comments, LGTM otherwise.
"
1807	527	Sarsate	2014-07-23 18:04:03	CONTRIBUTOR	"Rebased and Travis passes. ping @lavalamp for second review.
"
1808	587	VojtechVitek	2014-07-23 18:09:02	CONTRIBUTOR	"Rebased. Could you please review once again?
"
1809	527	lavalamp	2014-07-23 18:21:29	MEMBER	"LGTM, minor comments except for the bit about returning an error from SetUp(). I'm OK if you want to do that in another PR, though.
"
1810	587	lavalamp	2014-07-23 18:24:40	MEMBER	"LGTM once you add back the clearCalls() method to fix travis :)
"
1811	584	thockin	2014-07-23 18:26:39	MEMBER	"I though 'hostname -f' would try a number of different things to get FQDN,
including, but not limited to, /etc/hosts.

Thanks for continuing to dig.

On Wed, Jul 23, 2014 at 10:40 AM, Kelsey Hightower <notifications@github.com

> wrote:
> 
> I'm going to spend effort in the stdlib to see if we can get support for
> os.DomainName or something added. Please feel to close this out.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/584#issuecomment-49908425
> .
"
1812	527	Sarsate	2014-07-23 18:39:42	CONTRIBUTOR	"I think I'd prefer to make it a separate PR since it doesn't directly relate to the rootDir flag.
"
1813	587	VojtechVitek	2014-07-23 18:45:52	CONTRIBUTOR	":) fixed
"
1814	578	smarterclayton	2014-07-23 18:53:11	CONTRIBUTOR	"Agreed - not aware of anything that can do that now.  My other pull at least tests that the json is valid.
"
1815	590	lavalamp	2014-07-23 18:54:59	MEMBER	"LGTM, just one thought.
"
1816	590	lavalamp	2014-07-23 19:04:31	MEMBER	"Looks like you may have to make a content-free go file to make some versions of go happy. :/
"
1817	587	smarterclayton	2014-07-23 19:14:38	CONTRIBUTOR	"LGTM
"
1818	443	smarterclayton	2014-07-23 19:29:04	CONTRIBUTOR	"So in general we feel that no API going forward should ever be protected by anything other than client certs, token auth, or kerberos ticket exchange.  Basic opens the door to browser CSRF, and also typically has undesirable performance characteristics.  How about supporting simple bearer auth tokens from a file configured on the apiserver?  That's as easy to implement as user/passwords, and is directly compatible with future OAuth.
"
1819	559	smarterclayton	2014-07-23 19:29:59	CONTRIBUTOR	"Actually we started validating before with Tim's pull - I added a unit test for validating manifests.  We really need to get API validation in - I may try to get to that soon.
"
1820	588	vmarmol	2014-07-23 20:00:46	CONTRIBUTOR	"Made changes. I plan to follow this PR with one that spawns a long-lived worker per pod to do the syncing as we discussed in the comments. This is a start on that which moves things to being async. PTAL @smarterclayton @lavalamp 
"
1821	587	lavalamp	2014-07-23 20:01:07	MEMBER	"Thanks for the change!
"
1822	590	smarterclayton	2014-07-23 20:37:25	CONTRIBUTOR	"fixed, ptal
"
1823	590	lavalamp	2014-07-23 20:59:38	MEMBER	"This is great to have tested. Thanks!
"
1824	170	erictune	2014-07-23 20:59:40	MEMBER	"I'll take a shot at this.

@bgrant0607 
@lavalamp 
Please share any other thoughts on podTemplates.   

Brian mentioned cron.  This makes me think he wants to use podTemplates for delegation.
That is, is there some mechanism where principal A can define a /podTemplate, and then grant principal B permission to create /pods which derive from a certain /podTemplate, but which run as A.  (I guess a replicationController is effectively ""another principal"" which A can delegate to?)

Will the delegation of power be part of the /podTemplate message, or will that be stored in some sideband ACL?  
Does the PUT /pods method get extended to allow creating a /pod from a template instead of using the desiredState?  Or is there a new non-REST method?
"
1825	170	lavalamp	2014-07-23 21:02:46	MEMBER	"PUT /pods should _only_ take pods, IMO. We should potentially offer something that ""fills out"" a pod template, but I think that should work like our current replication controller, which is an external component.
"
1826	589	monnand	2014-07-23 21:04:07	CONTRIBUTOR	"Do not merge this PR until fsouza/go-dockerclient#119 is solved/addressed/(replied?).
"
1827	585	dchen1107	2014-07-23 21:50:59	MEMBER	"In today's validation code, we check if evn's name and value is C Identifier. Hyphen is not valid in C Identifier. Now the question is why we require C Identifier here?
"
1828	535	lavalamp	2014-07-23 22:02:12	MEMBER	"@smarterclayton: Comment addressed (ew -> w).
"
1829	571	lavalamp	2014-07-23 22:27:54	MEMBER	"I'm running the e2e test on this.
"
1830	571	lavalamp	2014-07-23 22:41:54	MEMBER	"e2e passes on this as well as it does at head (the guestbook example runs).
"
1831	550	ogrisel	2014-07-23 22:59:22	NONE	"I tested under OSX 10.9 with a recent vagrant and the default number of minions (3) and the provisioning works well (it took some time to download everything though) but then the CPU usage on my macbook air is almost 100% without having deployed any container yet.

When I ssh into minion-1 here is are the top lines of top:

```
  PID USER      PR  NI    VIRT    RES    SHR S %CPU %MEM     TIME+ COMMAND
  355 root      20   0  231668  49924  49640 R 52.3  9.9   8:32.95 systemd-journal
14279 root      20   0  265676   3476   2396 S  8.9  0.7   1:32.05 kube-proxy
 1975 root      20   0  692464  41972   6240 S  6.0  8.3   0:59.47 docker
    3 root      20   0       0      0      0 R  1.3  0.0   0:12.23 ksoftirqd/0
    9 root      20   0       0      0      0 R  0.7  0.0   0:07.44 rcu_sched
 1987 root       0 -20       0      0      0 S  0.3  0.0   0:00.57 loop1
 8650 root      20   0  193144   4020   2792 S  0.3  0.8   0:00.35 kubelet
12482 root      20   0       0      0      0 S  0.3  0.0   0:00.61 kworker/0:2
    1 root      20   0   47504   3332   1904 S  0.0  0.7   0:01.71 systemd
    2 root      20   0       0      0      0 S  0.0  0.0   0:00.00 kthreadd
    5 root       0 -20       0      0      0 S  0.0  0.0   0:00.00 kworker/0:0H
    7 root      rt   0       0      0      0 S  0.0  0.0   0:00.00 migratio
```

So systemd-journal is using more than 50% of CPU (on each minion) for seemingly doing nothing useful.

Also: would a debian wheezy vagrant box be smaller / faster to download?
"
1832	550	smarterclayton	2014-07-23 23:02:57	CONTRIBUTOR	"CPU issue is a bug that was fixed in master but not in this branch yet.  Will be resolved when rebased
"
1833	592	lavalamp	2014-07-23 23:16:38	MEMBER	"@bgrant0607, this was what I was thinking in terms of a scheduler API.

I was thinking the scheduler would run a watch to get unassigned pods, and then write these binding objects to the apiserver to cause the pods to get assigned. Let me know if you see any problems with this approach.
"
1834	593	lavalamp	2014-07-23 23:38:11	MEMBER	"LGTM
"
1835	594	lavalamp	2014-07-23 23:39:45	MEMBER	"IIRC, we turned this off because of the security ramifications of allowing everything on the system to talk to docker.
"
1836	594	ryfow	2014-07-23 23:50:57	CONTRIBUTOR	"Are docker containers able to access the host's loopback? I'm not sure how you'd do that. If they can, the concern makes sense to me.

It'd be nice to have tcp as a config option for those of us who trust our containers. Something like how you set MINION_SCOPES would be fine.
"
1837	586	lavalamp	2014-07-23 23:52:57	MEMBER	"This seems reasonable to me.

For the default, I'd suggest prompting the first time and then storing a file. (e.g., ~/.kubernetes_default_provider)
"
1838	552	lavalamp	2014-07-24 00:03:50	MEMBER	"@brendanburns @brendandburns Maybe you can take a look at this when you get back?
"
1839	586	derekwaynecarr	2014-07-24 00:04:49	MEMBER	"Thanks for feedback.  I can do that.  

I have this almost finished so will submit it in PR tomorrow. 

Sent from my iPhone

> On Jul 23, 2014, at 7:53 PM, Daniel Smith notifications@github.com wrote:
> 
> This seems reasonable to me.
> 
> For the default, I'd suggest prompting the first time and then storing a file. (e.g., ~/.kubernetes_default_provider)
> 
> —
> Reply to this email directly or view it on GitHub.
"
1840	589	monnand	2014-07-24 00:12:51	CONTRIBUTOR	"@lavalamp Good news:  fsouza/go-dockerclient will change their API and use uint64 for memory. I will update this PR once @fsouza changed the code.
"
1841	589	lavalamp	2014-07-24 00:15:54	MEMBER	"Great!
"
1842	589	monnand	2014-07-24 00:43:25	CONTRIBUTOR	"@lavalamp fsouza/go-dockerclient#119 closed by changing to uint64. I updated the code accordingly. PTAL.

(Thank you, @fsouza)
"
1843	589	lavalamp	2014-07-24 01:00:30	MEMBER	"Thanks!
"
1844	593	lavalamp	2014-07-24 01:07:05	MEMBER	"One thing to note is that this also tests that the master has the right name for the minion. I'm pretty sure that's a feature?
"
1845	565	lavalamp	2014-07-24 01:17:08	MEMBER	"@brendandburns I'm thinking this PR should go in before #571, which will need a few things added to your switches. That way we only make one breaking change to the data we store in etcd, which is going to break existing clusters. But I don't want to rebase on top of your commit and write that if it's going to change a lot.
"
1846	550	smarterclayton	2014-07-24 01:37:47	CONTRIBUTOR	"@ogrisel we'd definitely take a pull for debian as an additional vagrant env.
"
1847	585	smarterclayton	2014-07-24 01:39:33	CONTRIBUTOR	"Environment variables don't allow dashes - service names probably need to be uniquely translatable to env prefixes (no collisions)
"
1848	565	brendandburns	2014-07-24 02:24:09	CONTRIBUTOR	"I'm around.  I'll rebase tonight.

Brendan
On Jul 23, 2014 6:17 PM, ""Daniel Smith"" notifications@github.com wrote:

> @brendandburns https://github.com/brendandburns I'm thinking this PR
> should go in before #571
> https://github.com/GoogleCloudPlatform/kubernetes/pull/571, which will
> need a few things added to your switches. That way we only make one
> breaking change to the data we store in etcd, which is going to break
> existing clusters. But I don't want to rebase on top of your commit and
> write that if it's going to change a lot.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/565#issuecomment-49956957
> .
"
1849	170	bgrant0607	2014-07-24 03:30:06	MEMBER	"The point of this proposal is to further narrow the responsibility and API of the replicationController to its bare essentials. The replicationController should just spawn new replicas. 

Right now, essentially a full copy of the pod API is embedded in the replicationController API. As an external, independently versioned API, it would be challenging to keep synchronized with the core API. Additionally, with the replicationController creating pods by value rather than by reference, it needs to be delegated the authority to create ~arbitrary pods as ~arbitrary users (once we support multi-tenancy) -- this would mean it could do anything as anybody. This is even more of an issue once we introduce an auto-scaler layered on the replicationController.

It's very difficult to develop a signature-based approach layered on a literal pod creation API that could be made both usable and secure. OTOH, if the replicationController could only spawn instances from templates owned by the core, then it's power could be restricted. Think of this as the principle of least privilege and separation of concerns.

Including the template by reference in the replicationController API would also facilitate rollbacks to previous pod configurations. A standalone template could be used for cron and other forms of deferred execution. 

Even if we were to add a more general templating/configuration-generation mechanism in the future, we can't have turtles all the way down. A pod template would be useful for spawning the config generator, among other things.

As with the current replicationController API, pods would have no relationship to the template from which they were generated other than their labels and any other provenance information we kept. Changes to the template would have no effect on pods created from it previously.

I'd be fine with a separate API endpoint for creating pods from a template, just as we have for replicationController today.
"
1850	552	brendandburns	2014-07-24 04:41:02	CONTRIBUTOR	"LGTM.  Can someone validate that this still correctly turns up a cluster in GCE & Azure?

Thanks!
--brendan
"
1851	571	thockin	2014-07-24 06:05:51	MEMBER	"I really want to think more about this, but I will not have time the rest
of this week.

On Wed, Jul 23, 2014 at 3:42 PM, Daniel Smith notifications@github.com
wrote:

> e2e passes on this as well as it does at head (the guestbook example runs).
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/571#issuecomment-49946208
> .
"
1852	566	pixie79	2014-07-24 07:01:01	NONE	"Hi,

I was using kubecfg.

Regards
Mark
"
1853	550	ogrisel	2014-07-24 08:35:00	NONE	"> CPU issue is a bug that was fixed in master but not in this branch yet. Will be resolved when rebased.

I confirm that rebasing this branch on master solved the systemd-journal CPU usage issue.

> @ogrisel we'd definitely take a pull for debian as an additional vagrant env.

Actually a CoreOS vagrant box would probably be even smaller / faster to setup. I won't have the time to work on that this week though. Maybe later if no one else did it in the mean time.

Thanks for the fedora-based vagrant setup, it's now working great for me. I just tested it from a Ubuntu host.
"
1854	550	smarterclayton	2014-07-24 13:16:36	CONTRIBUTOR	"LGTM (tested and verified on OS X), although I'm not a salt expert.  Would like to get one more review as well with Salt experience.

Squash and rebase if you don't have any other changes to this.
"
1855	535	smarterclayton	2014-07-24 13:50:24	CONTRIBUTOR	"LGTM with answers to the two questions
"
1856	588	smarterclayton	2014-07-24 13:55:03	CONTRIBUTOR	"To be clear, the change you made was in SyncPods.  I was suggesting that in this pull you can do this correctly in syncPod by explicitly killing any extra containers that correspond to this pod's podFullName at the end of the loop: https://github.com/vmarmol/kubernetes/blob/0ee4b4c4f03683e23f8b92516f92da1ce888d76c/pkg/kubelet/kubelet.go#L367

So in syncPod track all of the container IDs you are using, then kill any container in DockerContainers that matches podFullName but is not tracked in your map.

The only thing you can't do is mutate the DockerContainers map, but you don't need to (since the loop at the end of SyncPods correctly kills unrecognized pods and ignores any pod that syncPod was invoked on).
"
1857	550	mfojtik	2014-07-24 14:30:02	CONTRIBUTOR	"LGTM, tested on Arch Linux with latest vagrant and it works without problems.
"
1858	170	erictune	2014-07-24 14:48:02	MEMBER	"Okay, putting together above comments and my own thoughts...

For the initial PR, we just need to have a pod template type which unambiguously and completely defines a /pod.  Later PRs can extend /podTemplate as needed to support authorization of delegated use.

Here are a few examples with delegation and the types of expansion of templates that might occur:
      - cron service runs a /pod, but passes to pod's environment a string identifying the datecode for this run.
      - third-party auto-scaler makes more of a pod, or makes pods that request more or less resources.
      - map-reduce service makes several pods, setting environment variables that control input and output file paths.
      - ABTester service makes pods with two different values for an Environment variable that controls a new feature, and two different values of another Environment variable that controls a tag added to the logs of these pods (e.g. experiment_27354_mode_a)

Considerations for podTemplate:
1. Ease and succinctness of definition of podTemplate
2. Ease of reasoning about the security implications of giving a user permission to instantiate pods from a podTemplate
3. Work with YAML as well as json.
4. allow templates to generate many different kinds of pods. 
   Item 4 seems much less important than 1 and 2.  Therefore, this rules out a podTemplate which holds a schema definition, or jpath expression, or anything else which allows fully general manipulation of json-type data.  Item 3 above further reinforces this.

Therefore, a /podTemplate will look something like this:

``` json
{ ""id"": ""awesomePodTemplate"",
  ""pod"": ""<object exactly following /pod schema>"", 
   ""allowExtraEnvVars"": [
     ""MOTD"": 
       ""Today's pod brought to you by a replication controller.""],
   ""allowModifiedResourcesRequestsAndLimits"": 1,
   ""delegatedPodMakers"": [""alice@example.com"", ""replicationcontroller@kubernetes.io""],
}
```

Note the specific, capability-like descriptions of allowed modifications to the /pod object.

However, the first PR will just have:

``` json
{ ""id"": ""myPodTemplate"",
  ""pod"": ""<object exactly following /pod schema>"", 
}
```

The /pod schema will get a new member ""actsAsUser"".  This affects which user the pod acts as.
Initially, this will have no affect.  As we add authentication (#443), the following authorization code can be added to the apiserver:

``` go
if authenticatedUser == request.pod.actAsUser { return auth.Authorized }
return auth.notAuthorized
```

In a later PRs, the /pod schema will be extended to have a ""fromPodTemplateId"" member which references the id of the /podTemplate that this /pod is modeled on.    This adds an interesting twist:  we can't use the user-provided name alone to identify the /podTemplate.  We need to specify which user's namespace the name lies in.  Maybe ""actAsUser"" identifies this or maybe we need a globally unique id for a podTemplate.

With that member added, the authorization check for creating a /pod would look like this:

``` go
if authenticatedUser == request.pod.actAsUser { return Authorized }
if auth.Can(authenticatedUser, auth.MakePodsFor, request.pod.actAsUser) {
    tpl := findPodTemplate(request.fromPodTemplateId)
    if tpl != nil {
      if tpl.Generates(request.pod) {
         return auth.Authorized
      }
    }
  }
}
return auth.NotAuthorized
```
"
1859	588	vmarmol	2014-07-24 15:26:15	CONTRIBUTOR	"@smarterclayton done, PTAL.
"
1860	598	erictune	2014-07-24 15:49:18	MEMBER	"If something happens to minion1, then your pod can't run.  Kubernetes tries
to abstract away dependency on specific minions.

Is it practical for you to do one of the following:
- install the data on all hosts?
- make a docker image which includes this data so that it can be installed
  on any minion?
- serve it off NFS from a machine which is not a minion, and then access it
  as a docker remote volume?
  I'm guessing not, or you wouldn't have asked, but it would be helpful to
  understand your use case more.

On Thu, Jul 24, 2014 at 12:24 AM, Mark Olliver notifications@github.com
wrote:

> Is there a way to pin a pod to a minion?
> 
> For example we have some data that is stored on the host disk that is
> persistent between reboots, as such I need to tell the replication
> controller that this container should be pinned for example to minion1.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/598.
"
1861	597	erictune	2014-07-24 15:52:14	MEMBER	"The pod template for the pod could specify that the external port can come
from a dynamically allocated range.  Then the replication controller or
scheduler can assign a port.  This seems related to podTemplates
https://github.com/GoogleCloudPlatform/kubernetes/issues/170

On Wed, Jul 23, 2014 at 11:30 PM, Mark Olliver notifications@github.com
wrote:

> Not sure how this could be done but for our application we could run the
> same container 10 times on the same minion quite happily using different
> ports as it is lightweight. To load balance though we would need to map the
> ports back to appear as one service port. To allow other machines to talk
> to it easily.
> 
> May be it could hang of the container port name and a matching regex on
> the pod name. So all pods that start Fred- with a port called http would be
> load balanced by the same service?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/597.
"
1862	597	erictune	2014-07-24 15:56:50	MEMBER	"Mention #170 
"
1863	170	erictune	2014-07-24 15:58:45	MEMBER	"Other use case: pod's port can come from a range, to allow duplicate pods on the same host.  Would this go in the template?
"
1864	597	erictune	2014-07-24 16:00:05	MEMBER	"This might be a duplicate of #390 
"
1865	588	smarterclayton	2014-07-24 16:33:51	CONTRIBUTOR	"LGTM minus the test case failure.
"
1866	597	lavalamp	2014-07-24 16:45:25	MEMBER	"We're trying very, very hard to avoid dynamically allocated ports.

Since our model is IP-per-pod, this actually is OK and we can co-locate things with conflicting external ports. Unfortunately it's not completely implemented yet and I think they do still conflict. But we don't need dynamic ports to fix it. :)
"
1867	544	ironcladlou	2014-07-24 16:47:52	CONTRIBUTOR	"Two more important pieces:
1. `kubulet.go#GetPodInfo` needs adapted to do a full container list using the docker client in order to correctly report `currentState` for stopped containers. It's also worth noting that the docker container state details aren't even reported by the kubelet unless cadvisor is present.
2. It becomes very important to delete old containers which are actively removed from the pod (in response to an active container delete from the pod, or the deletion of the pod itself). The current `killContainer` implementation just stops containers. Due to the naming conventions, old stopped containers from previous pods can become part of the effective state of new pods whose container and pod IDs align. (I could be mistaken on that last example; I thought I encountered it early on and am going from memory.)

[This branch](https://github.com/ironcladlou/kubernetes/compare/build-poc) contains a very quick n' dirty run-once policy implementation and also handles container cleanup. It doesn't implement the more efficient/comprehensive design outlined above, but it at least demonstrates a lot of points of modification for the overall change. The hacky policy implementation is just incidental to the other POC work going on, so mostly just check out the modifications to `kubelet.go`.
"
1868	598	lavalamp	2014-07-24 16:48:55	MEMBER	"What Eric said. We may be forced to add such constraints in the future, but we're going to try hard not to. :)

We (@Sarsate) are working on additional volume types to make this easy.
"
1869	599	lavalamp	2014-07-24 16:54:40	MEMBER	"Thanks for the report. I think this is just a bug-- we should check whether it exists and not pollute the log. I'm pretty sure we don't currently even use that file.

Also we should NOT be reading the proxy's config from a file in /tmp where anybody can write it. Bad default location.
"
1870	588	vmarmol	2014-07-24 17:19:19	CONTRIBUTOR	"Fixed tests, we were double killing unhealthy containers.
"
1871	588	smarterclayton	2014-07-24 17:21:42	CONTRIBUTOR	"Full LGTM
"
1872	170	lavalamp	2014-07-24 17:24:34	MEMBER	"I wonder if we should maybe add an ""owner"" field to the JSONBase, so that all objects in the system could have an owning user. If so, no need to specifically add that field to the PodTemplate.

> In a later PRs, the /pod schema will be extended to have a ""fromPodTemplateId"" member which references the id of the /podTemplate that this /pod is modeled on. This adds an interesting twist: we can't use the user-provided name alone to identify the /podTemplate.

This could be done with a label, which is what our current replicationController does.

I think a step that should come shortly after adding PodTemplate as a resource is changing the replication controller struct to take a podTemplateID instead of a hardcoded PodTemplate.

Port shouldn't be dynamic.

May want @brendanburns to take a look at this when he gets back.
"
1873	170	erictune	2014-07-24 17:37:14	MEMBER	"On Thu, Jul 24, 2014 at 10:24 AM, Daniel Smith notifications@github.com
wrote:

> I wonder if we should maybe add an ""owner"" field to the JSONBase, so that
> all objects in the system could have an owning user. If so, no need to
> specifically add that field to the PodTemplate.
> 
> In a later PRs, the /pod schema will be extended to have a
> ""fromPodTemplateId"" member which references the id of the /podTemplate that
> this /pod is modeled on. This adds an interesting twist: we can't use the
> user-provided name alone to identify the /podTemplate.
> 
> This could be done with a label, which is what our current
> replicationController does.
> 
> can a label selector select a different user's objects?
> 
> I think a step that should come shortly after adding PodTemplate as a
> resource is changing the replication controller struct to take a
> podTemplateID instead of a hardcoded PodTemplate.
> 
> Okay, but again the namespace/user issue is unresolved.
> 
> Port shouldn't be dynamic.
> 
> May want @brendanburns https://github.com/brendanburns to take a look
> at this when he gets back.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/170#issuecomment-50049816
> .
"
1874	599	smarterclayton	2014-07-24 17:39:06	CONTRIBUTOR	"Proxy config sounds like an /etc thing, same for kubelet's container manifests from disk
"
1875	597	smarterclayton	2014-07-24 17:45:19	CONTRIBUTOR	"The conflict right now is (IIRC) assigning service ports and the container ports that are exposed on the host on each minion.  The former seems like it's something that _should_ be possible for the consumer to not care about what port the service is exposed on (create service, get back an IP and port to connect to), and the latter is just an artifact of the network configuration on the host and Docker (forcing the ports to be exposed on the container and the host).

Agree that the per pod ports should not be dynamic if we can build non-IaaS network solutions that make IP-per-container usable.
"
1876	550	derekwaynecarr	2014-07-24 17:56:01	MEMBER	"@smarterclayton rebased and included the support for running e2e-test.sh on vagrant managed envs.
"
1877	550	smarterclayton	2014-07-24 18:02:32	CONTRIBUTOR	"Has anyone tried GCE on top of this after the refactor?
"
1878	550	derekwaynecarr	2014-07-24 18:06:27	MEMBER	"@smarterclayton i think it would be good to try it out if someone can.  I am getting an error now on healthz checks from apiserver that I need to resolve after rebase.
"
1879	550	derekwaynecarr	2014-07-24 18:08:38	MEMBER	"@smarterclayton disregard my previous comment, the apiserver was attempting to contact the minions prior them being fully provisioned.  things are fine.
"
1880	550	lavalamp	2014-07-24 18:15:22	MEMBER	"Let me pull this and run e2e test on GCE. If that works, I'll merge, since this seems to have a few LGTMs.
"
1881	600	erictune	2014-07-24 18:36:45	MEMBER	"cadvisor also uses ""resource"" for the second use case.
"
1882	527	Sarsate	2014-07-24 18:52:49	CONTRIBUTOR	"Reverted back to passing the directory to factory. ping @lavalamp @thockin 
"
1883	109	erictune	2014-07-24 18:53:32	MEMBER	"Seems fixed.
"
1884	550	lavalamp	2014-07-24 18:55:11	MEMBER	"I get:

```
$ hack/e2e-test.sh
+++ Building proxy
+++ Building integration
+++ Building apiserver
+++ Building controller-manager
+++ Building kubelet
+++ Building kubecfg
hack/../release/release.sh: line 30: hack/../release/../cluster/config-test.sh: No such file or directory
```
"
1885	552	lavalamp	2014-07-24 19:06:22	MEMBER	"I'm going to make sure that GCE e2e still works and then merge this if so. Presumably @jeffmendoza has verified that it still works on azure. :)
"
1886	527	lavalamp	2014-07-24 19:17:20	MEMBER	"LGTM-- can you add a TODO about returning that error before I merge? Sorry, just want to make sure we don't forget.
"
1887	550	derekwaynecarr	2014-07-24 19:18:23	MEMBER	"@lavalamp - pushed an update.  get to a point now that requires billing when i try locally.  can you retry with latest?
"
1888	550	lavalamp	2014-07-24 19:21:55	MEMBER	"Thanks! Will re-run as soon as I finish trying to run it on #552.
"
1889	598	pixie79	2014-07-24 19:33:41	NONE	"Unfortunatly it is not that simple as the application in the docker will be updating the data all the time and we can not use NFS as that is much to much overhead for the access latencies we need.

Ideally in the future this data would be stored on an SSD volume mounted to the minion. But for now I am happy with it being on the host but it does need to be pinned.
"
1890	552	jeffmendoza	2014-07-24 19:36:18	MEMBER	"Yep, Azure is good. Not finished though. I have openvpn coming next.
"
1891	597	pixie79	2014-07-24 19:37:30	NONE	"No I think you have mistaken me, but it might also be my understanding.

For example say I have 4 minions I want to run my same docker image 16 times so 4 copies on each. Is this possible I am happy with them all having the same host and container ports. 

The idea then is to run the service lb over the top to ensue requests are distributed among all 16.
"
1892	535	smarterclayton	2014-07-24 19:42:06	CONTRIBUTOR	"Ok, LGTM and this is a net new interface so folks on vacation can register their discontent in subsequent pulls
"
1893	552	lavalamp	2014-07-24 19:46:43	MEMBER	"Seems like it still works.
"
1894	598	lavalamp	2014-07-24 19:52:08	MEMBER	"Yeah, SSD access is one of the things that will probably force us to add some sort of constraint to keep your pod co-located with its SSD.
"
1895	597	lavalamp	2014-07-24 19:59:07	MEMBER	"Yeah, that's the idea. IP per pod, so ports won't conflict.
"
1896	535	lavalamp	2014-07-24 20:02:20	MEMBER	"Awesome, I can fix further complaints in #549 which will be pending for a while because it's blocked on #571.
"
1897	527	Sarsate	2014-07-24 20:05:11	CONTRIBUTOR	"Sure thing, added.
"
1898	550	lavalamp	2014-07-24 20:06:37	MEMBER	"OK, seems to work. Unfortunately I merged #552 which I think is causing a conflict. Can you rebase? Thanks!
"
1899	527	lavalamp	2014-07-24 20:13:02	MEMBER	"Thanks for the change!
"
1900	604	lavalamp	2014-07-24 20:30:51	MEMBER	"Agree. We should fix this.
"
1901	550	derekwaynecarr	2014-07-24 20:35:57	MEMBER	"@lavalamp done.  this should be good to go.
"
1902	595	lavalamp	2014-07-24 20:36:10	MEMBER	"Here's an argument for keeping things in the plural:

http://www.vinaysahni.com/best-practices-for-a-pragmatic-restful-api#restful
"
1903	588	vmarmol	2014-07-24 20:49:58	CONTRIBUTOR	"PTAL,rebased as well.
"
1904	595	smarterclayton	2014-07-24 20:50:22	CONTRIBUTOR	"While I agree with everything Vinay says normally, I think I disagree on this one point.  If you have a namespace that can have human readable names, and you want to also represent queries on that namespace with human readable names, you will have a conflict using the plural.  For instance, he cites:

> Aliases for common queries
> 
> To make the API experience more pleasant for the average consumer, consider packaging up sets of conditions into easily accessible RESTful paths. For example, the recently closed tickets query above could be packaged up as GET /tickets/recently_closed

However, if tickets can have non integer names, now he needs to prevent tickets from being assigned the ""recently_closed"" name.

I feel Rails put a bad precedent in place because rails assumes integer IDs, does type coercion to check for them in the routing engine, and then encourages people to also use subqueries for restful behavior.  So as soon as your ID space is != integers you're in trouble.  In the Kube case that's services, replicationControllers, and potentially podTemplates so far.
"
1905	550	lavalamp	2014-07-24 20:51:57	MEMBER	"Running e2e again, just because I'm paranoid.

I'm not going to ask for it in this PR because it's sat long enough, but sometime soon can we move the vagrant instructions into their own file? README.md is getting quite long; it seems like a good idea to have each cloud provider thingy have its own README/HOWTO.
"
1906	550	smarterclayton	2014-07-24 20:54:09	CONTRIBUTOR	"Agreed.  Also, regarding the e2e comment you made the other day, we plan on ramping up the existing OpenShift CI infrastructure to do automated CI tests on Kube pulls - that will cover the vagrant based minion testing scenarios and we can do deeper integration tests there.  Won't solve GCE though.
"
1907	605	lavalamp	2014-07-24 20:59:20	MEMBER	"LGTM. I'm not sure what could be done about an error from TearDown. Log it and continue? :/
"
1908	550	lavalamp	2014-07-24 21:00:16	MEMBER	"@smarterclayton that sounds awesome!
"
1909	588	lavalamp	2014-07-24 21:00:50	MEMBER	"Thanks!
"
1910	520	lavalamp	2014-07-24 21:02:49	MEMBER	"CoreOS CLA showed up in the list! \o/
"
1911	605	Sarsate	2014-07-24 21:03:03	CONTRIBUTOR	"That's the best I can think of too. At the very least we should report what goes wrong.
"
1912	520	jonboulle	2014-07-24 21:04:14	CONTRIBUTOR	"wahooo
"
1913	532	lavalamp	2014-07-24 21:05:21	MEMBER	"CLA came in. Thanks for the fixes!
"
1914	550	lavalamp	2014-07-24 21:05:53	MEMBER	"Thanks for getting this working!
"
1915	520	lavalamp	2014-07-24 21:06:49	MEMBER	"Looks like I can't auto-revert my earlier reversion, but I'd be happy to merge this if you can resend it. :)
"
1916	520	jonboulle	2014-07-24 21:09:44	CONTRIBUTOR	"#606
"
1917	606	lavalamp	2014-07-24 21:12:57	MEMBER	"Thanks (again)! Merging as soon as travis blesses it.
"
1918	605	lavalamp	2014-07-24 21:13:35	MEMBER	"Thanks!
"
1919	596	lavalamp	2014-07-24 21:17:12	MEMBER	"I'm not sure, but I think that the idea is that we have IP-per-pod, and pods can open whatever port they want on that IP. Then we add the Pods' IP addresses to the LB pool, and everything should automagically work. I'm not sure to what extent this desired state matches our current state. Marking this as a question; @brendanburns may have a more complete answer when he gets back.
"
1920	146	smarterclayton	2014-07-24 21:44:03	CONTRIBUTOR	"It would also be nice if higher level services can participate with dns resolution and decorate lower level services where necessary.  Also, registration of DNS with external parties is important in larger organizations where important services must be registered to a corporate DNS.  DNS also plays into a number of components like protocol aware load balancers where ports are shared and hosts determine routing - we'd want to be able to tie things as CNAMEs as well.
"
1921	596	brendandburns	2014-07-24 21:59:55	CONTRIBUTOR	"No, sadly the LB can only target a single port.

Why does the proxy need 80/443?  That doesn't seem to be necessary.  If it
is actually exporting some data there, we should just move the port.

--brendan

On Thu, Jul 24, 2014 at 2:17 PM, Daniel Smith notifications@github.com
wrote:

> I'm not sure, but I think that the idea is that we have IP-per-pod, and
> pods can open whatever port they want on that IP. Then we add the Pods' IP
> addresses to the LB pool, and everything should automagically work. I'm not
> sure to what extent this desired state matches our current state. Marking
> this as a question; @brendanburns https://github.com/brendanburns may
> have a more complete answer when he gets back.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/596#issuecomment-50079578
> .
"
1922	596	pixie79	2014-07-24 22:04:30	NONE	"Ok

I was proxying as that runs on every server and the only thing I saw that could open the firewall and Google lb. We have some webservices that need to have load balanced due to traffic volume so I was considering running two instances. How do I tell the the Google lb where it is?
"
1923	598	lavalamp	2014-07-24 22:06:55	MEMBER	"Paging @bgrant0607.
"
1924	608	dchen1107	2014-07-24 22:12:19	MEMBER	"Turns out it is not a new regression from HEAD. The problem should be exist since the day one when we integrated with cAdvisor. There is a race between kubelet startup, and instantiate cAdvisorClient object and config the machine with port 5000 for cAdvisor:

Code snippet at kubelet instantiate time:

```
cadvisorClient, err := cadvisor.NewClient(""http://127.0.0.1:5000"")
if err != nil {
    glog.Errorf(""Error on creating cadvisor client: %v"", err)
}
```
"
1925	608	lavalamp	2014-07-24 22:12:53	MEMBER	"Stacktrace makes it look like the crash came from cadvisor client. @vmarmol, look familiar?

EDIT: nvm, github hadn't shown me Dawn's previous comment.
"
1926	607	lavalamp	2014-07-24 22:27:40	MEMBER	"Thanks!
"
1927	596	brendandburns	2014-07-24 22:38:53	CONTRIBUTOR	"The easiest way to do that is to set up a service that points at the
containers, and then point the LB at all of the VMs in your kubernetes
cluster, and the port that you chose for your service.

The proxy will create an open port on each host VM, and the LB can connect
to that port.

Let me know if you have more questions.

--brendan

On Thu, Jul 24, 2014 at 3:04 PM, Mark Olliver notifications@github.com
wrote:

> Ok
> 
> I was proxying as that runs on every server and the only thing I saw that
> could open the firewall and Google lb. We have some webservices that need
> to have load balanced due to traffic volume so I was considering running
> two instances. How do I tell the the Google lb where it is?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/596#issuecomment-50084652
> .
"
1928	354	ConnorDoyle	2014-07-24 22:43:14	MEMBER	"@lavalamp :+1: for your sentiments regarding the public scheduler API
"
1929	354	lavalamp	2014-07-24 22:47:21	MEMBER	"Thanks, I think pretty much everyone is on board with having the scheduler use only the public API.

I'm starting to work on this-- feel free to take a look at and/or comment on #592 and #557 (Note, #557 is going to get changed extensively before submission). Also very close to having watch capability in the apiserver, which is another prereq.
"
1930	592	bgrant0607	2014-07-24 22:53:14	MEMBER	"Overall, this seems reasonable as a starting point. I like that the API is simple and narrow.

How hard would it be to move bindings from the main API to a separate API (e.g., by exposing a new HTTP server on another port)? I'd like to separate user operations and infrastructure operations into distinct APIs. That would reduce the API surface most people need to deal with, would make authorization policy easier to specify and enforce down the road, and would allow us to independently version the 2 APIs, which would be useful because I expect many more uses of the user operations than of the infrastructure operations. 

How is the scheduler going to discover pods that need to be scheduled? This is related to the pod status PR, I guess. :-)

Some other thoughts on future issues that will come up, but which I don't think affect this PR:
- Once we implement the resource model, the core should validate feasibility and not just rely on the scheduler. Once we support multi-tenancy, if we have exclusion/dedication concepts like sole tenancy, the core will need to enforce those policies, too.
- Host resource and binding info: Is there any way for schedulers to watch how much resources are available on all the hosts? Is there any way for schedulers to ask what pods are bound to each host?
- Consistency in the case of multiple scheduler replicas: HA schedulers will run with warm spares. They'll likely use master election (e.g., using etcd), but there could still be cases where a scheduler replica that had lost its lease and had stale information was still submitting bindings. We may want to add an explicit lease mechanism in the future to support this, in which case the binding would refer to the lease object and would be rejected if it referred to an invalid lease. 
- Multiple schedulers: Each scheduler would want its own lease. We'd need some policies about resource sharing and how to resolve conflicts (priorities or somesuch). And we'd need a mechanism for deciding and/or specifying which scheduler to use for a particular request. If we want users to be able to provide schedulers in a multi-tenant environment, that opens up a big can of worms.
- Preemptions: We may want to add preemptions in the future. Preemptions need to be ACKed before the binding can be made, which means there would at least need to be a dependency between the 2 operations. This is one reason why I think we're going to need mutations to return per-object sequence numbers and both reads and writes could optionally take sequence numbers to explicitly specify operation ordering in a way that works across proxies, etc. Also, it's useful to record the reason for the preemption and possibly the binding that triggered the preemption. I suppose we could just give schedulers the power to stop pods and provide a way to record the reason.
- Batches of bindings: We may want to support batches of bindings for higher throughput. Alternatively, we could add sequence numbers and just allow the scheduler to send multiple outstanding bindings asynchronously.
- Feasibility checks: We'll want a way to validate feasibility without actually binding a pod.
"
1931	592	lavalamp	2014-07-24 23:02:04	MEMBER	"> How hard would it be to move bindings from the main API to a separate API (e.g., by exposing a new HTTP server on another port)?

Not hard at all, but I was thinking that:
- Once we have some concept of identity, we'd only allow schedulers to write to this api endpoint.
- We have an api version in our URI path already, and there's no reason we can't run multiple api versions in parallel.

> How is the scheduler going to discover pods that need to be scheduled? This is related to the pod status PR, I guess. :-)

Yes :) Specifically, the scheduler is going to watch some endpoint on the apiserver. For the first take, I was thinking I'd just have it watch all add/update/delete of pods. We can later push the filtering (of only unassigned pods) down the stack (maybe all the way into etcd?) once we're ready to optimize things.
"
1932	602	lavalamp	2014-07-24 23:07:02	MEMBER	"This is a subset of #556.
"
1933	556	lavalamp	2014-07-24 23:08:03	MEMBER	"I would LOVE to see this hooked up. @rafael, still interested in making it happen? Let me know! (See #602.)
"
1934	596	pixie79	2014-07-24 23:08:14	NONE	"That is what I thought and the whole point of my bug.

Kube-proxy service will not listen on port 80 and Google load balancer will not redirect traffic from 80 to 10800.

So if you want to serve standard http traffic you can not.
"
1935	556	smarterclayton	2014-07-24 23:11:21	CONTRIBUTOR	"I'll jump in here too - @rafael, let me know what you want to grab.
"
1936	463	lavalamp	2014-07-24 23:15:00	MEMBER	"Marking this as closed, since @vmarmol's change should at least be a big improvement. Please reopen if you see this behavior again.
"
1937	380	lavalamp	2014-07-24 23:20:45	MEMBER	"3) and 4) are in #602 #556
"
1938	352	lavalamp	2014-07-24 23:26:06	MEMBER	"I think this is just some script file changes-- different platforms may need different things.
"
1939	146	bgrant0607	2014-07-24 23:27:29	MEMBER	"@smarterclayton I'd be interested in hearing more about your requirements.

I'm currently thinking that, for the most part, pod DNS is not so useful, except as a shorthand for ipv6 addresses in debugging scenarios.

Except for possible future special cases, pods are relatively disposable and won't have stable addresses or even stable names. In particular, replicationControllers treat pods as fungible, and as discussed in #557 we don't currently plan to reschedule pods, in order to leave future flexibility for layers on top to manage rescheduling, migration, a variety of rolling update schemes, etc.

We could implement dynamic DNS, but would need to hammer on DNS caching problems, in linux, in language platforms and libraries, in applications, etc.

So, I'm thinking we should support IP per service (I think I mentioned that in the new networking.md doc) and DNS for services.
"
1940	380	smarterclayton	2014-07-24 23:29:18	CONTRIBUTOR	"We at least reject it in the kubelet for 4 so the sync loop will never see it.  
"
1941	556	rafael	2014-07-24 23:29:29	CONTRIBUTOR	"Yes! Still interested here. I will have time to work on this during the weekend. Any more guidance beside pkg/api/validation.go? Where is the code for kubelet that @lavalamp mentions in #602 ? 

This is the first time I'm going to jump into the Kubernetes codebase and I've been teaching myself GO  during the last week, but feeling motivated and excited :D 
"
1942	285	lavalamp	2014-07-24 23:31:46	MEMBER	"This keeps coming up. We should prioritize it. See #602...
"
1943	268	lavalamp	2014-07-24 23:32:21	MEMBER	"I believe this is fixed now, thanks to a few PRs from @dchen1107. :)
"
1944	380	lavalamp	2014-07-24 23:36:38	MEMBER	"Pods that came in over etcd need to have already been rejected by apiserver...
"
1945	368	lavalamp	2014-07-24 23:38:31	MEMBER	"It sounds like this issue got a little unfocused, and it also sounds like the original request is going to happen (at least for GCE) when we grab the next version of the image we use. If my understanding is incorrect, please reopen and add detail of what would satisfy this request.
"
1946	302	lavalamp	2014-07-24 23:41:36	MEMBER	"Assigning to myself to get this behavior in the etcd implementation of our generic watch.Interface.
"
1947	592	bgrant0607	2014-07-24 23:42:43	MEMBER	"I'd like to establish the pattern of partitioning the API by anticipated type of client. For instance:
- user objects: pods
- scheduler objects: bindings
- machine management objects: hosts

It may not be much of an issue now, but I'm looking ahead to when we have lots more object types, more ecosystem components and API layers, more implementations of the APIs, etc. I could imagine splitting these APIs into separate services, for example.
"
1948	301	lavalamp	2014-07-24 23:43:14	MEMBER	"@vmarmol do you feel like your recent changes are sufficient to close this or do you have more in the pipeline?
"
1949	296	lavalamp	2014-07-24 23:45:18	MEMBER	"Added ""across restarts"" to the issue title for clarity.
"
1950	380	smarterclayton	2014-07-24 23:47:16	CONTRIBUTOR	"I know @ironcladlou raised 2) in #544 but it probably deserves its own issue.
"
1951	603	smarterclayton	2014-07-24 23:48:29	CONTRIBUTOR	"@derekwaynecarr mentioned he thinks this is a transient docker container failure to start - happens on both Debian and fedora.  We should check with upstream on it.
"
1952	245	lavalamp	2014-07-24 23:48:33	MEMBER	"I think we'll be most of the way to closing this when #565 goes in.
"
1953	413	lavalamp	2014-07-24 23:50:12	MEMBER	"Pretty sure this is done now.
"
1954	301	vmarmol	2014-07-24 23:50:33	CONTRIBUTOR	"@lavalamp, closing since this should be enough. I'd still like to move to a long-running thread, but that's a plus.
"
1955	608	dchen1107	2014-07-24 23:50:59	MEMBER	"https://github.com/google/cadvisor/pull/125 was out to fix the issue here. 
"
1956	594	smarterclayton	2014-07-24 23:51:18	CONTRIBUTOR	"Andy from my team is prototyping a generic ""docker build on kube cluster"" using a pod with docker in docker running privileged.  We'd like to have API components that let you build using the same mechanisms as containers and then push to a registry on success.  I think they'll have something tomorrow to show.
"
1957	556	smarterclayton	2014-07-24 23:53:13	CONTRIBUTOR	"Check out pkg/kubelet/config/config.go and examples/examples_test.go.

I'll do service and replication controller validation.
"
1958	556	lavalamp	2014-07-25 00:00:13	MEMBER	"Basically we want to run this validation logic:

https://github.com/GoogleCloudPlatform/kubernetes/blob/master/pkg/api/validation.go#L267

On the container manifest inside the pod we get here:

https://github.com/GoogleCloudPlatform/kubernetes/blob/master/pkg/registry/pod_registry.go#L190
"
1959	146	smarterclayton	2014-07-25 00:00:49	CONTRIBUTOR	"IP per service can get very expensive - we run today roughly 1 load balancer per container.  So in some cases we may want services to be cheap (which might be a different type of service, really).  IP per service seems like it could be optional - I would argue if you're doing local proxying you'd be better off hiding the remote proxy port and ip from the containers anyway to prevent people from baking assumptions in about the remote destination, and if you _need_ a service at a known ip and port you're typically talking more about an edge service.  

However, when you need it, it should be possible to do easily.
"
1960	146	smarterclayton	2014-07-25 00:01:33	CONTRIBUTOR	"(above predicated on ipv4 and current state of networking in a lot of environments)
"
1961	380	smarterclayton	2014-07-25 00:02:24	CONTRIBUTOR	"Not disagreeing - pointing out the original problem can no longer occur.
"
1962	592	lavalamp	2014-07-25 00:05:18	MEMBER	"OK, I can make that work. The scheduler endpoint will need to also run some of the user object endpoint apis (scheduler needs to list pods and machines), but I don't see a problem with that. For the moment I think I'll serve from a different port on the same binary unless you see a problem with that.
"
1963	592	smarterclayton	2014-07-25 00:13:55	CONTRIBUTOR	"That separation of APIs is already something we were going to ask about - creating scopes of responsibility under different API contracts.  Does it have to be on a separate port or can it be multiplexed into the same API server and separated by path?  After all, I assume the split is most real on the client side and that there is no overlap between API resource names when namespaced by version.  In a lot of simple deployments managing multiple ports and services is going to be unpleasant - id rather not support that except at the largest scale.  The separation you mention can be done via code level discipline, can't it?
"
1964	556	rafael	2014-07-25 00:16:29	CONTRIBUTOR	"Cool, I'll do pod validation! 
"
1965	608	dchen1107	2014-07-25 00:25:16	MEMBER	"The issue should be fixed by PR #125 in cadvisor
"
1966	543	rafael	2014-07-25 00:31:27	CONTRIBUTOR	"Hey @smarterclayton, @derekwaynecarr on a side note, after playing a while with the new Vagrant setup (which is awesome, thanks @derekwaynecarr ), one good thing about the Vagrant file specific for the example, is that is kind of faster to get to a point where you can start seeing the pods running. I'm cheating in this Vagrant file by pulling the images in the provisioning, but it makes the magic for the example.

I think I like the feedback loop of finishing the provisioning and get the example working right away, I feel it more close to the way it works when you run it GCE.

Do you have thoughts on that? In any case, I'm happy to rebase this branch with master and just adapt the wiki page to use the main Vagrant setup. 
"
1967	603	derekwaynecarr	2014-07-25 00:35:49	MEMBER	"The container will show as started but is slow to accept network traffic.  If I wait and then recurl it tends to work again.   

Sent from my iPhone

> On Jul 24, 2014, at 7:48 PM, Clayton Coleman notifications@github.com wrote:
> 
> @derekwaynecarr mentioned he thinks this is a transient docker container failure to start - happens on both Debian and fedora. We should check with upstream on it.
> 
> —
> Reply to this email directly or view it on GitHub.
"
1968	592	smarterclayton	2014-07-25 00:47:30	CONTRIBUTOR	"Re: sequence numbers, is there any reason to not add those now (I'm assuming you're referring to a monotonic increasing number per resource modification)?
"
1969	598	bgrant0607	2014-07-25 00:51:51	MEMBER	"I renamed this issue to narrow it to the specific use case. 

Support for durable local storage is an issue that has been raised by several partners in discussions, and is evident in every example application we've looked at (Guestbook, Acme Air, Drupal). This is a requirement for running a database, other storage system (e.g., HDFS, Zookeeper, etcd), SSD-based cache, etc. 

Support for more types of volumes ( #97 ) is maybe necessary but definitely not sufficient. We also need to represent the storage devices as allocatable resources ( #168 ).

As I mentioned in #146 , pods are currently relatively disposable and don't have durable identities. So, I think the main design question is this: do we conflate the identity of the storage with the identity of the pod and try to increase the durability of the pod, or do we represent durable local volumes as objects with an identity and lifetime independent of the pods? The latter would permit/require creation of a new pod that could be attached to pre-existing storage.

The latter is somewhat attractive, but would obstruct local restarts, which is desirable for high availability and bootstrapping, and wouldn't interact well with replicationController, due to the need to create/manage an additional object and also to match individual pods and volumes, which would reduce the fungibility of the pods.

So, I'm going to suggest we go with pod durability.

Rather than a single pin/persistence bit, I suggest we go with _forgiveness_: a list of (event type, optional duration, optional rate) of disruption events (e.g., host unreachability) the pod will tolerate. We could support an _any_ event type and infinite duration for pods that want to be pinned regardless of what happens.

This approach would generalize nicely for cases where, for example, applications wanted to endure reboots but give up in the case of extended outages or in the case that the disk goes bad. We're also going to want to use a similar spec for availability requirements / failure tolerances of sets of pods.

Ideally, the pod could be restarted/recreated by Kubelet directly. This would likely require checkpointing #489 , but initially we'd at least need to be able to:
- not transition the pod to a stopped state and/or delete it, or at least be able to recreate it with the same identity
- recover pre-existing storage from a well known place

Regarding the former, we probably need to introduce some indication of outages into the pod status -- probably not the primary state enum, but in a separate readiness field.

Regarding the latter, there are cases where it is convenient to place the storage in the host in a user-specified location, to facilitate debugging, data recovery, etc. without needing to look up long host-specific system-generated identifiers, though that's probably not a requirement for v0.

It might be nice to have a way for a durable pod to have a way to request to delete itself without making an API call. Some people have suggested that run-until-success (i.e., exit 0) is not a sufficiently reliable way to convey this. Perhaps we could use an empty volume on exit as the signal. Certainly that would mean there wasn't any valuable data to worry about, and it would be easy for an application to drop an empty file there if it just wanted to stay put.

Support for raw SSD should be filed as a separate issue, if desired.

@thockin @johnwilkes
"
1970	592	lavalamp	2014-07-25 00:53:03	MEMBER	"We already have etcd resource number in JSONBase, which is used to implement atomic updates. If @bgrant0607 is talking about a global sequence number, that's much harder...
"
1971	592	bgrant0607	2014-07-25 01:03:51	MEMBER	"@lavalamp: I meant per object, but we'd need a way to specify cross-object
preconditions/prerequisites. Do we return that now in ServerOp in response
to every mutating operation?

@smarterclayton: No reason not to do this now.
"
1972	592	lavalamp	2014-07-25 01:07:10	MEMBER	"@bgrant0607 We return the current revision of the object with every GET, and we (atomically) check that no one else changed it with every PUT. Creation (POST) is still racy, but that will be fixed eventually.

We have no model to make this transactional across multiple resource types.
"
1973	594	ryfow	2014-07-25 01:09:58	CONTRIBUTOR	"@smarterclayton That sounds fantastic. I'd be happy to test it out this weekend.
"
1974	611	lavalamp	2014-07-25 01:36:40	MEMBER	"No, release script is failing, so I think I'm just getting whatever random stuff it last managed to build.
"
1975	615	lavalamp	2014-07-25 01:52:13	MEMBER	"Self-merging this important bug fix.
"
1976	611	lavalamp	2014-07-25 02:10:55	MEMBER	"Fixing the release script fixed this for me.
"
1977	601	lavalamp	2014-07-25 02:11:16	MEMBER	"Can't reproduce.
"
1978	610	smarterclayton	2014-07-25 02:50:38	CONTRIBUTOR	"LGTM
"
1979	609	smarterclayton	2014-07-25 02:51:11	CONTRIBUTOR	"LGTM
"
1980	592	bgrant0607	2014-07-25 03:20:41	MEMBER	"@smarterclayton: Yes, a path prefix could work.

@lavalamp: The specific scenario I'm thinking about is killing a pod followed by a new binding to the same host. 
"
1981	146	smarterclayton	2014-07-25 04:14:09	CONTRIBUTOR	"It's probably also worth distinguishing between details an api consumer is required to provide, vs those the infrastructure picks for her.  For instance, when creating a service a consumer may omit the service port, but expect to get back a value for port and a value for IP, chosen by the infrastructure.  I have an assumption that software running in the infrastructure _generally_ is flexible to remote ports (especially if injected), so not every consumer of a service needs an IP.  However, if a specific port is requested, it may be at the discretion of the infrastructure whether to satisfy that request or reject it.
"
1982	598	thockin	2014-07-25 04:16:03	MEMBER	"Can we start with clear statements of requirement?  What we have with local
volumes is already pretty durable, as long as the pod stays put.  That may
be a side effect of the implementation, but maybe we should keep it.

Alternatively, maybe the answer is ""don't use local storage"".  Just like
GCE has a PD associated with a VM, we could have something similar with
pods.

But that is getting ahead - I don't feel like I really understand the
required aspects of this.

On Thu, Jul 24, 2014 at 5:52 PM, bgrant0607 notifications@github.com
wrote:

> I renamed this issue to narrow it to the specific use case.
> 
> Support for durable local storage is an issue that has been raised by
> several partners in discussions, and is evident in every example
> application we've looked at (Guestbook, Acme Air, Drupal). This is a
> requirement for running a database, other storage system (e.g., HDFS,
> Zookeeper, etcd), SSD-based cache, etc.
> 
> Support for more types of volumes ( #97
> https://github.com/GoogleCloudPlatform/kubernetes/issues/97 ) is maybe
> necessary but definitely not sufficient. We also need to represent the
> storage devices as allocatable resources ( #168
> https://github.com/GoogleCloudPlatform/kubernetes/issues/168 ).
> 
> As I mentioned in #146
> https://github.com/GoogleCloudPlatform/kubernetes/issues/146 , pods are
> currently relatively disposable and don't have durable identities. So, I
> think the main design question is this: do we conflate the identity of the
> storage with the identity of the pod and try to increase the durability of
> the pod, or do we represent durable local volumes as objects with an
> identity and lifetime independent of the pods? The latter would
> permit/require creation of a new pod that could be attached to pre-existing
> storage.
> 
> The latter is somewhat attractive, but would obstruct local restarts,
> which is desirable for high availability and bootstrapping, and wouldn't
> interact well with replicationController, due to the need to create/manage
> an additional object and also to match individual pods and volumes, which
> would reduce the fungibility of the pods.
> 
> So, I'm going to suggest we go with pod durability.
> 
> Rather than a single pin/persistence bit, I suggest we go with
> _forgiveness_: a list of (event type, optional duration, optional rate)
> of disruption events (e.g., host unreachability) the pod will tolerate. We
> could support an _any_ event type and infinite duration for pods that
> want to be pinned regardless of what happens.
> 
> This approach would generalize nicely for cases where, for example,
> applications wanted to endure reboots but give up in the case of extended
> outages or in the case that the disk goes bad. We're also going to want to
> use a similar spec for availability requirements / failure tolerances of
> sets of pods.
> 
> Ideally, the pod could be restarted/recreated by Kubelet directly. This
> would likely require checkpointing #489
> https://github.com/GoogleCloudPlatform/kubernetes/issues/489 , but
> initially we'd at least need to be able to:
> 
>    -
> 
>    not transition the pod to a stopped state and/or delete it, or at
>    least be able to recreate it with the same identity
>    -
> 
>    recover pre-existing storage from a well known place
> 
> Regarding the former, we probably need to introduce some indication of
> outages into the pod status -- probably not the primary state enum, but in
> a separate readiness field.
> 
> Regarding the latter, there are cases where it is convenient to place the
> storage in the host in a user-specified location, to facilitate debugging,
> data recovery, etc. without needing to look up long host-specific
> system-generated identifiers, though that's probably not a requirement for
> v0.
> 
> It might be nice to have a way for a durable pod to have a way to request
> to delete itself without making an API call. Some people have suggested
> that run-until-success (i.e., exit 0) is not a sufficiently reliable way to
> convey this. Perhaps we could use an empty volume on exit as the signal.
> Certainly that would mean there wasn't any valuable data to worry about,
> and it would be easy for an application to drop an empty file there if it
> just wanted to stay put.
> 
> Support for raw SSD should be filed as a separate issue, if desired.
> 
> @thockin https://github.com/thockin @johnwilkes
> https://github.com/johnwilkes
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/598#issuecomment-50096742
> .
"
1983	598	smarterclayton	2014-07-25 04:23:03	CONTRIBUTOR	"The durable pod described above matches our experience with a broad range of real world use cases - many organizations are willing to support reasonable durability of containers in bulk, as long as the operational characteristics are understood ahead of time. They eventually want to move applications to more stateless models, but accepting outages and focusing on mean-time-to-recovery is a model they already tolerate.  Furthermore, this allows operators to focus on durability in bulk (at a host level), with a corresponding reduction in effort over their previous single use systems.

We'd be willing to describe a clear requirement for a way to indicate that certain pods should tolerate disruption, with a best effort attempt to preserve local volumes and the container until such a time as the operator describes a host ""lost"".  

The suggestion to indicate a pod is done by clearing its storage is elegant, although in practice it's either user intervention or the container idling out of use.
"
1984	598	smarterclayton	2014-07-25 04:24:54	CONTRIBUTOR	"User specified data locations is also not significant for us in the near term.
"
1985	146	thockin	2014-07-25 04:26:09	MEMBER	"I feel like there are decisions here I am not aware of and do not
understand.

Pod DNS for non-replicated pods IS ""IP per service"".  Maybe we don't want
people to run non-replicated pods?  Then we should not offer it.

pods ""won't have stable addresses or even stable names"" ?  In identifiers.md
we said:

```
4) Each pod instance on an apiserver has a PodID (a UUID) that is
```

unique across space and time
       1) If not specified by the client, the apiserver will assign this
identifier
       2) This identifier will persist for the lifetime of the pod, even if
the pod is stopped and started or moved across hosts

""we don't currently plan to reschedule pods"" - under what circumstances?
 If a minion dies, we won't trigger a reschedule?  Or are we asserting that
the ""rescheduled"" pod is a different pod, despite being the only one the
user asked for?

On Thu, Jul 24, 2014 at 4:27 PM, bgrant0607 notifications@github.com
wrote:

> @smarterclayton https://github.com/smarterclayton I'd be interested in
> hearing more about your requirements.
> 
> I'm currently thinking that, for the most part, pod DNS is not so useful,
> except as a shorthand for ipv6 addresses in debugging scenarios.
> 
> Except for possible future special cases, pods are relatively disposable
> and won't have stable addresses or even stable names. In particular,
> replicationControllers treat pods as fungible, and as discussed in #557
> https://github.com/GoogleCloudPlatform/kubernetes/pull/557 we don't
> currently plan to reschedule pods, in order to leave future flexibility for
> layers on top to manage rescheduling, migration, a variety of rolling
> update schemes, etc.
> 
> We could implement dynamic DNS, but would need to hammer on DNS caching
> problems, in linux, in language platforms and libraries, in applications,
> etc.
> 
> So, I'm thinking we should support IP per service (I think I mentioned
> that in the new networking.md doc) and DNS for services.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/146#issuecomment-50091453
> .
"
1986	600	smarterclayton	2014-07-25 04:44:49	CONTRIBUTOR	"Rest Resource is certainly more of a development term - so I wouldn't expect it to conflict with the consumption form of resource for k8s end users.  In fact, if a k8s user sees us referring to rest resources we're doing something wrong... :)
"
1987	489	smarterclayton	2014-07-25 04:59:55	CONTRIBUTOR	"Also garbage collected resources like local disk storage, SELinux MCS labels, unix users IDs for proper disk quota enforcement and extra process isolation, and any consumable resource on the host that has to be released periodically needs to be tied back to some reconciliation loop.  Also, I'm not sure there won't be resources on a system that are outside the container sync loop but need to react to deletion via tombstoning - home directories, real unix users, and .ssh directories for enabling SSH access to containers are all things we support today and will likely port to k8s.  Having at least some pattern for those sorts of resources that avoids solving lots of hard problems individually _seems_ desirable.
"
1988	618	smarterclayton	2014-07-25 05:00:49	CONTRIBUTOR	"LGTM
"
1989	616	smarterclayton	2014-07-25 05:01:33	CONTRIBUTOR	"LGTM
"
1990	592	lavalamp	2014-07-25 05:07:22	MEMBER	"@bgrant0607 ah, I see. I'll think a bit about that.
"
1991	146	lavalamp	2014-07-25 05:13:26	MEMBER	"> ""we don't currently plan to reschedule pods"" - under what circumstances?

+1; replication controllers start a new pod instead of waiting for the system to reschedule a pod. This is a nifty control loop thingy, but I have not been convinced the part of the system that makes it necessary (where we don't move pods off of damaged hosts) is a feature and not a bug. In fact I consider it a bug at the moment.
"
1992	565	brendandburns	2014-07-25 05:15:12	CONTRIBUTOR	"Ok, e2e tests now pass.  (well basic.sh is flakey, but I got it to pass at least once)

I expect Travis will also go green as soon as my latests commits finish running.

Merge when ready.
"
1993	618	smarterclayton	2014-07-25 05:15:45	CONTRIBUTOR	"Unsafe mock?
"
1994	618	lavalamp	2014-07-25 05:18:05	MEMBER	"Not unsafe mock, unsafe code ;)
"
1995	620	brendandburns	2014-07-25 05:19:29	CONTRIBUTOR	"This is already basically possible.

The kubelet implements HTTP health checks, and restarts the container if it is failing.  So no task should actually be failing for very long.  This means that for M backends you only do M health checks.

Taking it a step further, we could consider adding health checks to the Service polling, but in some ways that seems redundant, since only healthy tasks should be in the service pool anyway.
"
1996	146	thockin	2014-07-25 05:21:38	MEMBER	"As a naive user I would expect the Pod Create API to return me an ID that I
can use forever and ever to address the Pod I just Created.

On Thu, Jul 24, 2014 at 10:13 PM, Daniel Smith notifications@github.com
wrote:

> ""we don't currently plan to reschedule pods"" - under what circumstances?
> 
> +1; replication controllers start a new pod instead of waiting for the
> system to reschedule a pod. This is a nifty control loop thingy, but I have
> not been convinced the part of the system that makes it necessary (where we
> don't move pods off of damaged hosts) is a feature and not a bug. In fact I
> consider it a bug at the moment.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/146#issuecomment-50108747
> .
"
1997	146	lavalamp	2014-07-25 05:28:10	MEMBER	"> As a naive user I would expect the Pod Create API to return me an ID that I can use forever and ever to address the Pod I just Created.

I believe that is the case, if you make a pod with a blank ID, apiserver should fill in the ID field and it would appear in the response. Otherwise it's bug.
"
1998	620	dbcode	2014-07-25 05:28:13	NONE	"Can you clarify for me - let's say I have a cluster of 100 frontend containers using a backend service with 200 containers, and I have an HTTP health check on the backend service polling the URL ""/healthy"" every 5 seconds.    How many requests to /healthy does each backend instance (container) see every 5 seconds?

Also, is restarting the container something that can be configured?  I may not want to restart the container; e.g. on instance migration, I might want to just remove it from the LB pool 30-60 seconds before the migration takes place, and then put it back in once the migration is complete (thus minimizing broken connections).
"
1999	598	johnwilkes	2014-07-25 05:31:03	CONTRIBUTOR	"+1 to the forgiveness model.

Let's make sure that it's possible to list the same reason (especially
""any"") multiple times - we'd like to make it possible to forgive a few long
outages, and many shorter ones.
  john

On Thu, Jul 24, 2014 at 5:52 PM, bgrant0607 notifications@github.com
wrote:

> I renamed this issue to narrow it to the specific use case.
> 
> Support for durable local storage is an issue that has been raised by
> several partners in discussions, and is evident in every example
> application we've looked at (Guestbook, Acme Air, Drupal). This is a
> requirement for running a database, other storage system (e.g., HDFS,
> Zookeeper, etcd), SSD-based cache, etc.
> 
> Support for more types of volumes ( #97
> https://github.com/GoogleCloudPlatform/kubernetes/issues/97 ) is maybe
> necessary but definitely not sufficient. We also need to represent the
> storage devices as allocatable resources ( #168
> https://github.com/GoogleCloudPlatform/kubernetes/issues/168 ).
> 
> As I mentioned in #146
> https://github.com/GoogleCloudPlatform/kubernetes/issues/146 , pods are
> currently relatively disposable and don't have durable identities. So, I
> think the main design question is this: do we conflate the identity of the
> storage with the identity of the pod and try to increase the durability of
> the pod, or do we represent durable local volumes as objects with an
> identity and lifetime independent of the pods? The latter would
> permit/require creation of a new pod that could be attached to pre-existing
> storage.
> 
> The latter is somewhat attractive, but would obstruct local restarts,
> which is desirable for high availability and bootstrapping, and wouldn't
> interact well with replicationController, due to the need to create/manage
> an additional object and also to match individual pods and volumes, which
> would reduce the fungibility of the pods.
> 
> So, I'm going to suggest we go with pod durability.
> 
> Rather than a single pin/persistence bit, I suggest we go with
> _forgiveness_: a list of (event type, optional duration, optional rate)
> of disruption events (e.g., host unreachability) the pod will tolerate. We
> could support an _any_ event type and infinite duration for pods that
> want to be pinned regardless of what happens.
> 
> This approach would generalize nicely for cases where, for example,
> applications wanted to endure reboots but give up in the case of extended
> outages or in the case that the disk goes bad. We're also going to want to
> use a similar spec for availability requirements / failure tolerances of
> sets of pods.
> 
> Ideally, the pod could be restarted/recreated by Kubelet directly. This
> would likely require checkpointing #489
> https://github.com/GoogleCloudPlatform/kubernetes/issues/489 , but
> initially we'd at least need to be able to:
> 
>    -
> 
>    not transition the pod to a stopped state and/or delete it, or at
>    least be able to recreate it with the same identity
>    -
> 
>    recover pre-existing storage from a well known place
> 
> Regarding the former, we probably need to introduce some indication of
> outages into the pod status -- probably not the primary state enum, but in
> a separate readiness field.
> 
> Regarding the latter, there are cases where it is convenient to place the
> storage in the host in a user-specified location, to facilitate debugging,
> data recovery, etc. without needing to look up long host-specific
> system-generated identifiers, though that's probably not a requirement for
> v0.
> 
> It might be nice to have a way for a durable pod to have a way to request
> to delete itself without making an API call. Some people have suggested
> that run-until-success (i.e., exit 0) is not a sufficiently reliable way to
> convey this. Perhaps we could use an empty volume on exit as the signal.
> Certainly that would mean there wasn't any valuable data to worry about,
> and it would be easy for an application to drop an empty file there if it
> just wanted to stay put.
> 
> Support for raw SSD should be filed as a separate issue, if desired.
> 
> @thockin https://github.com/thockin @johnwilkes
> https://github.com/johnwilkes
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/598#issuecomment-50096742
> .
"
