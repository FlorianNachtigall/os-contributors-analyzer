	issue	user_login	created_at	author_association	comment
0	2	jbeda	2014-06-06 23:58:07	CONTRIBUTOR	"@brendandburns original filed.
"
1	1	jbeda	2014-06-06 23:58:24	CONTRIBUTOR	"@brendandburns originally filed.
"
2	2	brendandburns	2014-06-07 06:43:24	CONTRIBUTOR	"Added a check for go being installed.
"
3	9	proppy	2014-06-07 09:25:19	CONTRIBUTOR	"Yes, currently it's always:

```
            ""currentState"": {
                ""manifest"": {
                    ""version"": """",
                    ""volumes"": null,
                    ""containers"": null
                },
                ""host"": ""kubernetes-minion-...""
            }
```
"
4	17	brendandburns	2014-06-08 05:04:22	CONTRIBUTOR	"build/test/integration-test/e2e-test all pass.
"
5	19	proppy	2014-06-08 06:04:40	CONTRIBUTOR	"On Sat, Jun 7, 2014 at 10:35 PM, Joe Beda notifications@github.com wrote:

> Right now we use salt to distribute and start most of the server
> components for the master. As we support build and deployment from a local
> Mac, we don't pre-compile the go scripts but instead ship the source code
> to the nodes (with salt) and compile at install time.
> 
> Instead, we should do the following:
> - Only support building on a linux machine with docker installed.
>   Perhaps support local development on a mac with a local linux VM
> 
> We could link to docker instruction about boot2docker:
> http://docs.docker.io/installation/mac/
> - Package each server component up as a Docker image, built with a
>   Dockerfile
>   - We should support uploading these Docker images to either the
>     public index or a GCS backed index with google/docker-registry
>     https://index.docker.io/u/google/docker-registry/.
>     - Use the kubelet to run/health check the components. This means the
>       kubelet will manage a set of static tasks on each machine (including the
>       master) and a set of dynamic tasks.
> - The only task that shouldn't run under the docker should be the
>   kubelet itself. We may have to hack in something for (network mode = host)
>   for the proxy.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes-new/issues/19.

## 

Johan Euphrosine (proppy)
Developer Programs Engineer
Google Developer Relations
"
6	8	jbeda	2014-06-08 15:05:39	CONTRIBUTOR	"@brendandburns put in a 2 min wait here and that may be good enough for now.  See: #18.  But we should really do something more robust so leaving this bug open.
"
7	26	jbeda	2014-06-09 04:23:16	CONTRIBUTOR	"Looks reasonable to me. My guess is that unit test coverage here gets lower :/
"
8	30	brendandburns	2014-06-09 06:01:13	CONTRIBUTOR	"build/unit test/integration test/e2e tests pass.
"
9	43	lavalamp	2014-06-10 19:03:25	MEMBER	"Thank you for this fix! Can you please sign a CLA so we can accept it? Instructions are in CONTRIB.md. Thanks!
"
10	45	brendandburns	2014-06-10 19:54:22	CONTRIBUTOR	"Thank you for this fix! Can you please sign a CLA so we can accept it? Instructions are in CONTRIB.md. Thanks!
"
11	45	whiteinge	2014-06-10 19:56:32	CONTRIBUTOR	"Ah, I missed that. Done and done.
"
12	45	lavalamp	2014-06-10 20:01:01	MEMBER	"Thanks! I see your CLA in the list now.
"
13	48	lavalamp	2014-06-10 21:09:02	MEMBER	"Thank you for this fix! Can you please sign a CLA so we can accept it? Instructions are in CONTRIB.md. Thanks!
"
14	50	brendandburns	2014-06-10 22:04:32	CONTRIBUTOR	"The integration test binary is pretty close.  It'd be pretty simple to glue
in the kubelet to that binary.

Brendan
On Jun 10, 2014 2:53 PM, ""Daniel Smith"" notifications@github.com wrote:

> It would be great to have an easy way to run a kubernetes master/minion
> locally. Currently, it's not that easy to set up. We should have a
> convenient script to make it work.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/50.
"
15	49	lavalamp	2014-06-10 22:07:18	MEMBER	"Thanks for the change!
"
16	48	hmrm	2014-06-10 23:28:42	CONTRIBUTOR	"Done!
"
17	53	asm89	2014-06-11 01:53:40	CONTRIBUTOR	"I signed the CLA with the same mail address used in the commit. I guess that works?
"
18	52	proppy	2014-06-11 01:55:28	CONTRIBUTOR	"Thanks for the fix, can you sign the CLA as described in https://github.com/GoogleCloudPlatform/kubernetes/blob/master/CONTRIB.md ?
"
19	44	proppy	2014-06-11 01:56:03	CONTRIBUTOR	"Thanks for the fix, can you sign the CLA as described in https://github.com/GoogleCloudPlatform/kubernetes/blob/master/CONTRIB.md ?
"
20	53	lavalamp	2014-06-11 03:09:27	MEMBER	"Thanks for the fix!
"
21	48	lavalamp	2014-06-11 03:10:46	MEMBER	"Thanks!
"
22	54	lavalamp	2014-06-11 03:13:47	MEMBER	"Thank you for this fix! Can you please sign a CLA so we can accept it? Instructions are in CONTRIB.md. Thanks!
"
23	55	jkassemi	2014-06-11 05:08:01	NONE	"Woot - n/m already corrected elsewhere. 
"
24	56	proppy	2014-06-11 05:39:01	CONTRIBUTOR	"Looks like a duplicate of #52 
"
25	56	inthecloud247	2014-06-11 05:41:59	NONE	"lol gotta sign the CLA to merge the change? 
"
26	52	inthecloud247	2014-06-11 05:42:46	NONE	"sign and fax it in? :-)
"
27	56	proppy	2014-06-11 05:44:26	CONTRIBUTOR	"Yes, whoever sign first: gets merged :)
"
28	57	proppy	2014-06-11 05:46:42	CONTRIBUTOR	"/cc @bgrant0607 
"
29	56	inthecloud247	2014-06-11 05:50:56	NONE	"Will this suffice?

June 10, 2014
I hereby grant Google Inc. a perpetual worldwide license for my proprietary, patented implementation of the word update. 

Sincerely,
-John Albietz
"
30	56	proppy	2014-06-11 05:52:56	CONTRIBUTOR	"@inthecloud247 unfortunately no, our lawyers insist on every contributions (even small documentation update) to go through:
https://github.com/GoogleCloudPlatform/kubernetes/blob/master/CONTRIB.md
"
31	52	adnanh	2014-06-11 05:56:43	CONTRIBUTOR	"Signed it electronically, says it will be processed shortly?
"
32	56	inthecloud247	2014-06-11 06:01:39	NONE	"so that probably also means that you can't go and fix it yourself now, since that would possibly mean that you're incorporating proprietary code into your project. a pickle.
"
33	56	adnanh	2014-06-11 06:04:28	CONTRIBUTOR	"Lol, what a mess we've made.
"
34	52	proppy	2014-06-11 06:04:49	CONTRIBUTOR	"@adnanh yes, it's already processed, now we just need a maintainer to press the merge button :)
"
35	52	adnanh	2014-06-11 06:05:12	CONTRIBUTOR	"Excellent. :-)
"
36	56	inthecloud247	2014-06-11 06:05:17	NONE	"how about if I branch out upstream, re-implement (in a clean room), then patch back in?
"
37	56	proppy	2014-06-11 06:06:20	CONTRIBUTOR	"@inthecloud247 you'd have to reverse engineer the typo first.
"
38	19	monnand	2014-06-11 06:07:14	CONTRIBUTOR	"I have some questions just out of curious:
- Does it mean that salt will not be used in the future?
- About kubenetes docker containers, does it only contain compiled binary files, or contain source code? More precisely, are you going to put build process into the Dockerfile?

Thank you!
"
39	57	usarid	2014-06-11 06:21:52	CONTRIBUTOR	"I also accepted (electronically) the CLA. ""Thank you.  Your CLA submission will be processed shortly.""
"
40	54	klrkdekira	2014-06-11 06:31:16	NONE	"Hi, I've signed it electronically :)
"
41	60	travisby	2014-06-11 12:23:22	CONTRIBUTOR	"Hello,

I took a look at CONTRIB.MD, and have signed the CLA.  I noticed that normally an issue should be filed before a pull request, but I had a feeling something like this would be just as easy to commit :P
"
42	60	brendandburns	2014-06-11 14:03:03	CONTRIBUTOR	"Thanks for the fix!
"
43	52	jbeda	2014-06-11 16:19:38	CONTRIBUTOR	"Thanks for the fix.  Merging.
"
44	61	proppy	2014-06-11 16:21:57	CONTRIBUTOR	"Looks like a duplicate of #52
"
45	54	proppy	2014-06-11 16:22:56	CONTRIBUTOR	"Looks like a duplicate of #52
"
46	61	stvnrhodes	2014-06-11 16:23:28	NONE	"Okay, discarding
"
47	54	jbeda	2014-06-11 16:23:41	CONTRIBUTOR	"Thanks for the fix.  Already merged another patch with to fix my typo.  Please keep playing with Kubernetes and let us know what you think.
"
48	56	jbeda	2014-06-11 16:25:06	CONTRIBUTOR	"Moot point now as another PR has been merged to fix my stupid typo.  Thanks for helping out though.

Keep playing with Kubernetes and let us know what you think.
"
49	61	jbeda	2014-06-11 16:25:41	CONTRIBUTOR	"Thanks for helping out though!  Please keep playing with Kubernetes and let us know what you think.
"
50	19	jbeda	2014-06-11 17:24:17	CONTRIBUTOR	"@monnand I imagine that we will continue to use salt to bootstrap stuff.  But we'll be able to reduce some of the more complex salt config.

For example, we currently ship and compile the source everywhere where it is run.  If we start building docker images, we can precompile the binaries before they are run.

I'm thinking that we'll follow the example of Docker itself and do the build process in docker containers.  This, with boot2docker, could lead to a good dev flow for Mac OS X.
"
51	62	lavalamp	2014-06-11 18:42:35	MEMBER	"@brendandburns, any reason not to add source for those directly to the guestbook directory?
"
52	50	lavalamp	2014-06-11 18:55:26	MEMBER	"I'll take a look.
"
53	57	bgrant0607	2014-06-11 19:07:03	MEMBER	"LGTM. Thanks
"
54	62	brendandburns	2014-06-11 19:37:41	CONTRIBUTOR	"See:

https://github.com/GoogleCloudPlatform/kubernetes/pull/63
"
55	19	monnand	2014-06-11 20:48:12	CONTRIBUTOR	"@jbeda Thank you!

You also mentioned that kubelet should not run under the docker. Is it for technical reason, or other? I do not see any technical difficulties to run kubelet in docker. Or did I miss something?
"
56	19	jbeda	2014-06-11 22:01:13	CONTRIBUTOR	"We may be able to run the kubelet under docker, but most likely we'll want it to have a whole machine view and expanded privs.  Running it under a cgroup container is totally doable.  namespaces?  I'm not so sure if we can make that happen.

Another way of looking at this is that I think of the kubelet as operating at the same level as Docker itself (and perhaps merging with Docker at some point?) and so it should run outside of Docker.
"
57	65	jbeda	2014-06-11 22:18:54	CONTRIBUTOR	"Thanks for reading the fine print @inthecloud247.  Let me clear this with our lawyers and figure out the what they think the right thing to do here is.
"
58	65	inthecloud247	2014-06-11 22:37:09	NONE	"@bcantrill from @Joyent just wrote a great post discussing node.js removing
their CLA requirement:
http://dtrace.org/blogs/bmc/2014/06/11/broadening-nodejs/

http://www.infoworld.com/t/javascript/joyent-makes-it-easier-contribute-code-nodejs-244152

I'm heavily involved in both Saltstack and Docker, and I definitely want to get involved in your project here, but the CLA definitely has a chilling effect.

If you need to stick with the CLA route, why not use the standard Apache CLA: http://www.apache.org/licenses/icla.txt ?
"
59	50	drewcsillag	2014-06-11 22:50:36	CONTRIBUTOR	"+1 for this -- especially in script form rather than a go executable.
"
60	19	monnand	2014-06-11 23:03:35	CONTRIBUTOR	"@jbeda Correct me if I'm wrong. (I'm not saying that kubelet should run inside a docker container. I'm just trying to see what are the technical difficulties here.)

As far as I know, kubelet only needs to communicate with docker through docker remote api, which is either trhough a unix socket or a remote IP/port pair. Does it need to read/write cgroup's filesystem? In either case, it seems that we could mount the /var/run onto the container and run kubelet inside that containers.

We are currently doing this in [cadvisor](http://github.com/google/cadvisor), which runs inside a docker container but can communicate with docker daemon and read information from the cgroup filesystem. The container could still run inside its own namespace, but communicate with docker daemon through the mounted volume. We use the following command to run cadvisor inside a docker container:

```
sudo docker run \
  --volume=/var/run:/var/run:rw \
  --volume=/sys/fs/cgroup/:/sys/fs/cgroup:ro \
  --volume=/var/lib/docker/:/var/lib/docker:ro \
  --publish=8080:8080 \
  --detach=true \
  google/cadvisor
```
"
61	64	jbeda	2014-06-11 23:12:40	CONTRIBUTOR	"Yeah -- I'd love to see this happen.  Metadata is from API->VM, not the other way around though.  GCS would be good but we'd have to give GCS write permissions to the master node -- something we don't do right now.

Another option would be to generate the cert on the client as the cluster is launched but that'd mean you'd have to have openssl installed.  Looks like it is pre-installed on OSX so perhaps this is doable.  We'd probably put the private cert in the metadata for the master.  Or we could ssh in and push it after the cluster is booted.  Not as big a fan of that.
"
62	68	jbeda	2014-06-11 23:26:08	CONTRIBUTOR	"Good catch!

Can we perhaps move this into the e2e-test script instead?  I don't want to open up more ports than necessary for the regular non-test bring up.

Also, we'll need you to sign our ""Contributor Licensing Agreement"" to accept this.  Lawyers.  See CONTRIB.md.
"
63	19	jbeda	2014-06-11 23:33:06	CONTRIBUTOR	"@monnand Nice!  We should try and make that work.  

One thing I worry about is things like driving iptables rules.  To solve #15 we'll have to be able to either muck with iptable rules or get a new networking mode into Docker proper.
"
64	65	jbeda	2014-06-11 23:40:53	CONTRIBUTOR	"@inthecloud247 I'll point this stuff out to the Google Open Source team.  I totally hear what you are saying.

As for modifying the LICENSE file, our lawyer got back to us on this:

> The apache license is just giving an example.  The license itself
> should be left unmodified. Modifying the example text in it will simply cause our LICENSE file to
> not be recognized as Apache by a large number of license file identifiers out there. There is no benefit or reason to do this.

Based on advice of counsel I'm going to pass on this PR.  Thanks for the interest though.  I'll follow up on the CLA.
"
65	19	brendanburns	2014-06-11 23:49:19	CONTRIBUTOR	"There was just a discussion of this in the plumbers meeting.  The union bay
networks folks want the ability to muck with the network from a container
too.

Brendan
On Jun 11, 2014 4:33 PM, ""Joe Beda"" notifications@github.com wrote:

> @monnand https://github.com/monnand Nice! We should try and make that
> work.
> 
> One thing I worry about is things like driving iptables rules. To solve
> #15 https://github.com/GoogleCloudPlatform/kubernetes/issues/15 we'll
> have to be able to either muck with iptable rules or get a new networking
> mode into Docker proper.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/19#issuecomment-45813777
> .
"
66	64	brendanburns	2014-06-11 23:51:11	CONTRIBUTOR	"We already generate the htpasswd file locally, so we could just follow that
pattern.

Brendan
On Jun 11, 2014 4:12 PM, ""Joe Beda"" notifications@github.com wrote:

> Yeah -- I'd love to see this happen. Metadata is from API->VM, not the
> other way around though. GCS would be good but we'd have to give GCS write
> permissions to the master node -- something we don't do right now.
> 
> Another option would be to generate the cert on the client as the cluster
> is launched but that'd mean you'd have to have openssl installed. Looks
> like it is pre-installed on OSX so perhaps this is doable. We'd probably
> put the private cert in the metadata for the master. Or we could ssh in and
> push it after the cluster is booted. Not as big a fan of that.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/64#issuecomment-45812294
> .
"
67	64	jbeda	2014-06-11 23:57:54	CONTRIBUTOR	"@brendanburns Generating the cert locally and putting it in metadata would work, but we'd start putting sensitive key material in metadata.  The htpasswd stuff is hashed and so it isn't sensitive.  The metadata for the instance is readable by anyone with API read access after launch.  Perhaps this is okay...
"
68	64	mmdriley	2014-06-12 00:04:11	CONTRIBUTOR	"As Joe says, htpasswd is less sensitive than the server's SSL private key. Having it available to all project readers still isn't great, but at least it's a salted hash of a strong password.

Metadata would be secure enough to ship around the certificate fingerprint or public key, but not the private key.
"
69	64	jbeda	2014-06-12 00:06:00	CONTRIBUTOR	"That being said, it is strictly better than just trusting any ol key.
"
70	68	danielnorberg	2014-06-12 00:10:01	CONTRIBUTOR	"Sure, i'll move it into e2e-test.sh.

I've submitted a CLA electronically. 
"
71	68	brendandburns	2014-06-12 00:47:44	CONTRIBUTOR	"Merged, thanks!

--brendan
"
72	69	brendandburns	2014-06-12 01:01:36	CONTRIBUTOR	"build/unit test/integration test/e2e test/ all pass.
"
73	65	inthecloud247	2014-06-12 01:13:03	NONE	"uh... so from the apache license docs, that really seems incorrect. :-)
check out the way the docker team did it:
https://github.com/dotcloud/docker/blob/master/LICENSE

_http://httpd.apache.org/docs/2.0/license.html
http://httpd.apache.org/docs/2.0/license.html_

To apply the Apache License to your work, attach the following boilerplate
notice, with the fields enclosed by brackets ""[]"" replaced with your own
identifying information. _(Don't include the brackets!)_

John Albietz
m: 516-592-2372
e: inthecloud247@gmail.com
l'in: linkedin.com/in/ydavid

On Wed, Jun 11, 2014 at 4:55 PM, Joe Beda notifications@github.com wrote:

> Closed #65 https://github.com/GoogleCloudPlatform/kubernetes/pull/65.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/65#event-130507323
> .
"
74	65	brendandburns	2014-06-12 01:40:50	CONTRIBUTOR	"If you look at the individual source files that's what we did.  As I
understand the advice from counsel, they believe the license file should be
included verbatim.

Brendan
On Jun 11, 2014 6:13 PM, ""John Albietz"" notifications@github.com wrote:

> uh... so from the apache license docs, that really seems incorrect. :-)
> check out the way the docker team did it:
> https://github.com/dotcloud/docker/blob/master/LICENSE
> 
> _http://httpd.apache.org/docs/2.0/license.html
> http://httpd.apache.org/docs/2.0/license.html_
> 
> To apply the Apache License to your work, attach the following boilerplate
> notice, with the fields enclosed by brackets ""[]"" replaced with your own
> identifying information. _(Don't include the brackets!)_
> 
> John Albietz
> m: 516-592-2372
> e: inthecloud247@gmail.com
> l'in: linkedin.com/in/ydavid
> 
> On Wed, Jun 11, 2014 at 4:55 PM, Joe Beda notifications@github.com
> wrote:
> 
> > Closed #65 https://github.com/GoogleCloudPlatform/kubernetes/pull/65.
> > 
> > —
> > Reply to this email directly or view it on GitHub
> > <
> > https://github.com/GoogleCloudPlatform/kubernetes/pull/65#event-130507323>
> > 
> > .
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/65#issuecomment-45819700
> .
"
75	65	dberlin	2014-06-12 02:52:56	NONE	"As Brendan says, what the license says to do is actually quite clear (and you are welcome to ask the ASF, they will tell you the same :P):

""To apply the Apache License to your work, attach the following boilerplate notice, with the fields enclosed by brackets ""[]"" replaced with your own identifying information. (Don't include the brackets!) The text should be enclosed in the appropriate comment syntax for the file format. ""

IE The license says to attach the notice to the source file, replacing the [] with your own info,  _not_ modify the Apache LICENSE.

You can see what the ASF has done for years, which is exactly to leave the _LICENSE_ alone, and modify the source files, here:

http://svn.apache.org/repos/asf/httpd/httpd/trunk/LICENSE (note the LICENSE is included verbatim).
http://svn.apache.org/repos/asf/hive/trunk/LICENSE
http://svn.apache.org/repos/asf/cassandra/trunk/LICENSE.txt
etc
"
76	65	dberlin	2014-06-12 02:57:17	NONE	"@inthecloud247 Google's CLA is identical to Apache's CLA already. There used to be differences in the corporate versions, but even those don't exist any longer.

The only wording differences between the two should be the textual replacement of ""The Foundation"" with ""Google"".
"
77	72	lavalamp	2014-06-12 05:28:58	MEMBER	"PTAL
"
78	72	brendandburns	2014-06-12 05:36:21	CONTRIBUTOR	"LGTM, but looks like it needs a rebase before I can merge it cleanly.
"
79	72	lavalamp	2014-06-12 16:35:57	MEMBER	"Rebased.
"
80	76	brendandburns	2014-06-12 18:21:46	CONTRIBUTOR	"Thanks for the PR.  Have you signed our CLA?  Instructions are in CONTRIB.md.

(also, this needs to be rebased)

Thanks again!
--brendan
"
81	73	jbeda	2014-06-12 18:38:12	CONTRIBUTOR	"Thanks for this PR.  The new flags are going to land at some point, but the version isn't known yet.  I have another PR to fix up with the known good version but I'm going to switch back to our non-prerelease tool (`gcutil`) in a future PR.

I'm accepting this in spirit but will push the right fix in quickly to stop breakage.  Thanks!
"
82	73	proppy	2014-06-12 18:41:20	CONTRIBUTOR	"FYI, the PR @jbeda is referring to is #78 
"
83	62	brendandburns	2014-06-12 18:41:45	CONTRIBUTOR	"This has been submitted.
"
84	6	brendandburns	2014-06-12 18:42:16	CONTRIBUTOR	"This is now fixed.
"
85	19	vmarmol	2014-06-12 19:50:43	CONTRIBUTOR	"Today you should be able to get the host's network. +1 to @brendanburns's comment.
"
86	19	proppy	2014-06-12 19:54:14	CONTRIBUTOR	"yes `--net host` should do the trick.

Another interesting thing to do is `-v /var/run/docker.sock:/var/run/docker.sock` to access the docker daemon from the container. (or just having the docker daemon listen on localhost w/ `--net host`)
"
87	79	proppy	2014-06-12 20:02:41	CONTRIBUTOR	"Note that if you need custom marshaling for type you can implement the http://golang.org/pkg/encoding/json/#Marshaler interface
"
88	81	danielnorberg	2014-06-12 20:23:52	CONTRIBUTOR	":+1:
"
89	81	lavalamp	2014-06-12 20:24:57	MEMBER	"Whoops, thanks for catching!
"
90	82	brendandburns	2014-06-12 20:37:06	CONTRIBUTOR	"Thanks!
"
91	85	lavalamp	2014-06-12 21:14:49	MEMBER	"Thanks! But needs rebase/merge.
"
92	85	danielnorberg	2014-06-12 21:15:52	CONTRIBUTOR	"This has already been fixed: https://github.com/GoogleCloudPlatform/kubernetes/commit/75957dc5b9cea2a0fa5170851051cbf328aa21cb
"
93	85	proppy	2014-06-12 21:17:12	CONTRIBUTOR	"Yes, duplicate of #82
"
94	84	proppy	2014-06-12 21:17:48	CONTRIBUTOR	"rebased
"
95	85	lavalamp	2014-06-12 21:20:38	MEMBER	"Explains why it wasn't happening to me :)
"
96	85	proppy	2014-06-12 21:23:21	CONTRIBUTOR	"@lavalamp your go tool chain is drunk
"
97	83	lavalamp	2014-06-12 21:29:49	MEMBER	"Looks like a few additional changes are needed before travis will build?
"
98	79	lavalamp	2014-06-12 22:29:47	MEMBER	"I've fixed the json parsing. Still working on yaml parsing.
"
99	79	lavalamp	2014-06-12 23:54:46	MEMBER	"OK. This works and is ready to be merged.
"
100	76	gleamglom	2014-06-13 01:10:32	NONE	"I've signed the CLA and rebased!
"
101	67	lavalamp	2014-06-13 02:59:44	MEMBER	"I think #79 fixes this. We now support yaml input and catch syntax errors.
"
102	50	lavalamp	2014-06-13 03:02:50	MEMBER	"@drewcsillag, what exactly do you mean by ""script form""? Were you thinking of something like a version of cloudcfg that can be run without needing to have started up a daemon first? 
"
103	8	lavalamp	2014-06-13 03:04:46	MEMBER	"This seems like a good excuse to make cloudcfg list pods return more info about the status of containers. Then we could poll that instead of waiting 2 minutes.
"
104	76	lavalamp	2014-06-13 03:10:46	MEMBER	"This is a great PR, but it looks like we didn't merge fast enough and it needs another rebase.
"
105	86	lavalamp	2014-06-13 03:58:20	MEMBER	"Thanks for doing this!
"
106	76	jerome22	2014-06-13 08:33:40	NONE	"$ ./output/go/cloudcfg
usage: cloudcfg -h <host> [-c config/file.json] [-p <hostPort>:<containerPort>,..., <hostPort-n>:<containerPort-n>] <method> <methodArgs>

  Kubernetes REST API:
  cloudcfg [OPTIONS] get|list|create|delete|update <url>

  Run an image:
  cloudcfg [OPTIONS] run <image> <replicas> <name>

  Manage replication controllers:
  cloudcfg [OPTIONS] stop|rm|rollingupdate <controller>

  Options:
  -auth=""/Users/tlow/.kubernetes_auth"": Path to the auth info file.  If missing, prompt the user
  -c="""": Path to the config file.
  -h="""": The host to connect to.
  -json=false: If true, print raw JSON for responses
  -l="""": Label query to use for listing
  -p="""": The port spec, comma-separated list of <external>:<internal>,...
  -s=-1: If positive, create and run a corresponding service on this port, only used with 'run'
  -u=1m0s: Update interarrival period
  -v=false: Print the version number.
  -yaml=false: If true, print raw YAML for responses

$ ./output/go/cloudcfg rollingupdate
2014/06/12 11:13:18 usage: cloudcfg [OPTIONS] stop|rm|rollingupdate <controller>
$ ./output/go/cloudcfg get
2014/06/12 11:13:22 usage: cloudcfg [OPTIONS] get|list|create|update|delete <url>
$ ./output/go/cloudcfg run
2014/06/12 11:13:25 usage: cloudcfg [OPTIONS] run <image> <replicas> <name>
"
107	76	gleamglom	2014-06-13 17:35:49	NONE	"I find it useful that `./cloudcfg run` returns the expected number of arguments and what they mean. Otherwise, users have to go back to source to figure out that 3 arguments are required.

Updated messages:

```
$ ./output/go/cloudcfg
usage: cloudcfg -h [-c config/file.json] [-p :,..., :]

  Kubernetes REST API:
  cloudcfg [OPTIONS] get|list|create|delete|update <url>

  Manage replication controllers:
  cloudcfg [OPTIONS] stop|rm|rollingupdate <controller>
  cloudcfg [OPTIONS] run <image> <replicas> <controller>
  cloudcfg [OPTIONS] resize <controller> <replicas>

  Options:
  -auth=""~/.kubernetes_auth"": Path to the auth info file.  If missing, prompt the user.  Only used if doing https.
  -c="""": Path to the config file.
  -h="""": The host to connect to.
  -json=false: If true, print raw JSON for responses
  -l="""": Label query to use for listing
  -p="""": The port spec, comma-separated list of <external>:<internal>,...
  -s=-1: If positive, create and run a corresponding service on this port, only used with 'run'
  -u=1m0s: Update interarrival period
  -v=false: Print the version number.
  -yaml=false: If true, print raw YAML for responses
$ ./output/go/cloudcfg rollingupdate
2014/06/13 10:37:59 usage: cloudcfg [OPTIONS] stop|rm|rollingupdate <controller>
$ ./output/go/cloudcfg get
2014/06/13 10:38:02 usage: cloudcfg [OPTIONS] get|list|create|update|delete <url>
$ ./output/go/cloudcfg resize
2014/06/13 10:38:04 usage: cloudcfg resize <controller> <replicas>
$ ./output/go/cloudcfg run
2014/06/13 10:38:06 usage: cloudcfg [OPTIONS] run <image> <replicas> <controller>
```
"
108	50	drewcsillag	2014-06-13 18:31:45	CONTRIBUTOR	"@lavalamp Basically, what I had in mind, is a shell script that looks something like [this](https://gist.github.com/drewcsillag/1d7f51ec0c17f7237f86)
"
109	50	lavalamp	2014-06-13 18:54:35	MEMBER	"Ah, ok. That's very similar to local-up.sh; the only difference is that everything is packaged into a single go binary. We probably will switch to using individual binaries for this when we start running our components inside docker containers.
"
110	32	brendandburns	2014-06-13 18:55:13	CONTRIBUTOR	"This is now done in https://github.com/GoogleCloudPlatform/kubernetes/pull/80
"
111	76	brendandburns	2014-06-13 19:03:23	CONTRIBUTOR	"Sorry, what is your real name?  I need it to look it up in the CLA signers...

Thanks!
--brendan
"
112	50	drewcsillag	2014-06-13 19:05:06	CONTRIBUTOR	"The script just makes things more obvious how to run things for those who are looking to integrate them into non-GCE environments.
"
113	50	lavalamp	2014-06-13 19:09:37	MEMBER	"That's a good point. I'll look into this some more later.
"
114	98	lavalamp	2014-06-13 19:17:32	MEMBER	"LGTM
"
115	76	gleamglom	2014-06-13 20:19:11	NONE	"@brendandburns Tiffany L
"
116	4	brendandburns	2014-06-13 21:47:32	CONTRIBUTOR	"Handled by #98 for now.  We might re-open later.
"
117	76	brendandburns	2014-06-13 22:48:40	CONTRIBUTOR	"Merged.  Thanks for the contribution.
"
118	105	lavalamp	2014-06-14 00:38:48	MEMBER	"Hm, looks like the integration test flaked out in travis.
"
119	105	jbeda	2014-06-14 14:38:35	CONTRIBUTOR	"Other than the `--allowed` message this LGTM so feel free to merge.
"
120	19	jbeda	2014-06-14 15:48:42	CONTRIBUTOR	"Notes of work in progress:
- I'm starting out by moving our build process into Docker.  A snapshot of a Dockerfile and a Makefile to automate some common stuff is here: https://github.com/jbeda/kubernetes/commit/6c4a6a8fa7794874862cadaf31948bdb9235f51a
- If we say everyone has to build on Linux, this is much easier.  With the boot2docker on a Mac, you essentially have a remote machine that you are talking to through a local TCP pipe.  That means with any `-v local-path:container-path` the `local-path` is really local to the boot2docker VM and not the local workstation.

This leaves us with 2 choices:
1. Build in the boot2docker VM and copy results back out, either through stdout from the docker run or via `boot2docker ssh`
2. Build the final docker images inside the boot2docker VM inside of a docker container.  Yup, this means running docker in docker.  This is supported with `dind` but it gets complicated.  There is a repo (https://github.com/jpetazzo/dind) with support but...
   - There is no license in that repo.  Issue: https://github.com/jpetazzo/dind/issues/21
   - The Docker build process itself uses dind but has a forked script checked in to the Docker repo.  These have diverged and I worry about continued support for the separate dind repo.  Issue to harmonize these: https://github.com/jpetazzo/dind/issues/22

Right now I'm leaning toward copying stuff in and out (option 1).
"
121	108	jbeda	2014-06-14 16:00:05	CONTRIBUTOR	"Note that this relates to #19 as it would (a) make it less painful to copy stuff in and out of a boot2docker VM during build and (b) would make the resultant container images smaller as they'd all share a base with the single binary.
"
122	108	brendandburns	2014-06-14 16:25:20	CONTRIBUTOR	"SGTM.

Brendan

On Sat, Jun 14, 2014, 9:00 AM, Joe Beda notifications@github.com wrote:

> Note that this relates to #4
> https://github.com/GoogleCloudPlatform/kubernetes/issues/4 as it would
> (a) make it less painful to copy stuff in and out of a boot2docker VM
> during build and (b) would make the resultant container images smaller as
> they'd all share a base with the single binary.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/108#issuecomment-46091743
> .
"
123	103	jbeda	2014-06-14 16:32:08	CONTRIBUTOR	"I actually think that this is by design.  The contract for the replication controller is that it drives to a number of replicas.  If you remove the controller, the things that it controls shouldn't necessarily be removed.

I could see us having a mode for a clean delete.  On implication of this is the DELETE of the controller is now an async operation that returns an HTTP `202 accepted` with some way to track the process of the DELETE.
"
124	108	lavalamp	2014-06-14 16:39:21	MEMBER	"The localkube thing I made is most of the way there already.
On Jun 14, 2014 9:25 AM, ""brendandburns"" notifications@github.com wrote:

> SGTM.
> 
> Brendan
> 
> On Sat, Jun 14, 2014, 9:00 AM, Joe Beda notifications@github.com wrote:
> 
> > Note that this relates to #4
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/4 as it
> > would
> > (a) make it less painful to copy stuff in and out of a boot2docker VM
> > during build and (b) would make the resultant container images smaller
> > as
> > they'd all share a base with the single binary.
> > 
> > —
> > Reply to this email directly or view it on GitHub
> > <
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/108#issuecomment-46091743>
> > 
> > .
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/108#issuecomment-46092426
> .
"
125	105	lavalamp	2014-06-14 16:59:55	MEMBER	"Hm. I'm pretty sure I didn't change anything in this to cause the travis failure-- is this the only thing it's flaking for?
"
126	19	proppy	2014-06-14 17:00:57	CONTRIBUTOR	"You could have Dockerfile for individual binaries and have the resulting
container image launch it has its ENTRYPOINT.

That way you could leverage the fact sources are sent from your workstation
to the VM as the /build payload (context) over the remote API (no need to
copy to the host) and the docker image is your artefact.
On Jun 14, 2014 8:48 AM, ""Joe Beda"" notifications@github.com wrote:

> Notes of work in progress:
> - I'm starting out by moving our build process into Docker. A snapshot
>   of a Dockerfile and a Makefile to automate some common stuff is here:
>   jbeda@6c4a6a8
>   https://github.com/jbeda/kubernetes/commit/6c4a6a8fa7794874862cadaf31948bdb9235f51a
> - If we say everyone has to build on Linux, this is much easier. With
>   the boot2docker on a Mac, you essentially have a remote machine that you
>   are talking to through a local TCP pipe. That means with any -v
>   local-path:container-path the local-path is really local to the
>   boot2docker VM and not the local workstation.
> 
> This leaves us with 2 choices:
> 1. Build in the boot2docker VM and copy results back out, either
>    through stdout from the docker run or via boot2docker ssh
> 2. Build the final docker images inside the boot2docker VM inside of a
>    docker container. Yup, this means running docker in docker. This is
>    supported with dind but it gets complicated. There is a repo (
>    https://github.com/jpetazzo/dind) with support but...
>    - There is no license in that repo. Issue: jpetazzo/dind#21
>      https://github.com/jpetazzo/dind/issues/21
>    - The Docker build process itself uses dind but has a forked script
>      checked in to the Docker repo. These have diverged and I worry about
>      continued support for the separate dind repo. Issue to harmonize these:
>      jpetazzo/dind#22 https://github.com/jpetazzo/dind/issues/22
> 
> Right now I'm leaning toward copying stuff in and out (option 1).
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/19#issuecomment-46091452
> .
"
127	19	proppy	2014-06-14 17:11:39	CONTRIBUTOR	"Note of you just want to have a container to build the projects and get
binaries out.

You can also set the ENTRYPOINT to the build command.

docker build ; docker run # to build
docker cp # to get the file out of the container
On Jun 14, 2014 10:00 AM, ""Johan Euphrosine"" proppy@google.com wrote:

> You could have Dockerfile for individual binaries and have the resulting
> container image launch it has its ENTRYPOINT.
> 
> That way you could leverage the fact sources are sent from your
> workstation to the VM as the /build payload (context) over the remote API
> (no need to copy to the host) and the docker image is your artefact.
> On Jun 14, 2014 8:48 AM, ""Joe Beda"" notifications@github.com wrote:
> 
> > Notes of work in progress:
> > - I'm starting out by moving our build process into Docker. A
> >   snapshot of a Dockerfile and a Makefile to automate some common stuff is
> >   here: jbeda@6c4a6a8
> >   https://github.com/jbeda/kubernetes/commit/6c4a6a8fa7794874862cadaf31948bdb9235f51a
> > - If we say everyone has to build on Linux, this is much easier. With
> >   the boot2docker on a Mac, you essentially have a remote machine that you
> >   are talking to through a local TCP pipe. That means with any -v
> >   local-path:container-path the local-path is really local to the
> >   boot2docker VM and not the local workstation.
> > 
> > This leaves us with 2 choices:
> > 1. Build in the boot2docker VM and copy results back out, either
> >    through stdout from the docker run or via boot2docker ssh
> > 2. Build the final docker images inside the boot2docker VM inside of
> >    a docker container. Yup, this means running docker in docker. This is
> >    supported with dind but it gets complicated. There is a repo (
> >    https://github.com/jpetazzo/dind) with support but...
> >    - There is no license in that repo. Issue: jpetazzo/dind#21
> >      https://github.com/jpetazzo/dind/issues/21
> >    - The Docker build process itself uses dind but has a forked
> >      script checked in to the Docker repo. These have diverged and I worry about
> >      continued support for the separate dind repo. Issue to harmonize these:
> >      jpetazzo/dind#22 https://github.com/jpetazzo/dind/issues/22
> > 
> > Right now I'm leaning toward copying stuff in and out (option 1).
> > 
> > —
> > Reply to this email directly or view it on GitHub
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/19#issuecomment-46091452
> > .
"
128	108	proppy	2014-06-14 17:13:39	CONTRIBUTOR	"Consider using subcommand instead of flags for defining which component to
launch, ex:
kube agent -some-agent-flag
 On Jun 14, 2014 9:39 AM, ""Daniel Smith"" notifications@github.com wrote:

> The localkube thing I made is most of the way there already.
> On Jun 14, 2014 9:25 AM, ""brendandburns"" notifications@github.com
> wrote:
> 
> > SGTM.
> > 
> > Brendan
> > 
> > On Sat, Jun 14, 2014, 9:00 AM, Joe Beda notifications@github.com
> > wrote:
> > 
> > > Note that this relates to #4
> > > https://github.com/GoogleCloudPlatform/kubernetes/issues/4 as it
> > > would
> > > (a) make it less painful to copy stuff in and out of a boot2docker VM
> > > during build and (b) would make the resultant container images smaller
> > > as
> > > they'd all share a base with the single binary.
> > > 
> > > —
> > > Reply to this email directly or view it on GitHub
> > > <
> > 
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/108#issuecomment-46091743>
> > 
> > > .
> > 
> > —
> > Reply to this email directly or view it on GitHub
> > <
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/108#issuecomment-46092426>
> > 
> > .
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/108#issuecomment-46092804
> .
"
129	19	proppy	2014-06-14 17:25:48	CONTRIBUTOR	"You could also have a combination of the two.

Build and run kube on top of google/golang for development; for production
if the size of google/debian base bother you: rebase the binary on top of
busybox image.
On Jun 14, 2014 10:11 AM, ""Johan Euphrosine"" proppy@google.com wrote:

> Note of you just want to have a container to build the projects and get
> binaries out.
> 
> You can also set the ENTRYPOINT to the build command.
> 
> docker build ; docker run # to build
> docker cp # to get the file out of the container
> On Jun 14, 2014 10:00 AM, ""Johan Euphrosine"" proppy@google.com wrote:
> 
> > You could have Dockerfile for individual binaries and have the resulting
> > container image launch it has its ENTRYPOINT.
> > 
> > That way you could leverage the fact sources are sent from your
> > workstation to the VM as the /build payload (context) over the remote API
> > (no need to copy to the host) and the docker image is your artefact.
> > On Jun 14, 2014 8:48 AM, ""Joe Beda"" notifications@github.com wrote:
> > 
> > > Notes of work in progress:
> > > - I'm starting out by moving our build process into Docker. A
> > >   snapshot of a Dockerfile and a Makefile to automate some common stuff is
> > >   here: jbeda@6c4a6a8
> > >   https://github.com/jbeda/kubernetes/commit/6c4a6a8fa7794874862cadaf31948bdb9235f51a
> > > - If we say everyone has to build on Linux, this is much easier.
> > >   With the boot2docker on a Mac, you essentially have a remote machine that
> > >   you are talking to through a local TCP pipe. That means with any -v
> > >   local-path:container-path the local-path is really local to the
> > >   boot2docker VM and not the local workstation.
> > > 
> > > This leaves us with 2 choices:
> > > 1. Build in the boot2docker VM and copy results back out, either
> > >    through stdout from the docker run or via boot2docker ssh
> > > 2. Build the final docker images inside the boot2docker VM inside of
> > >    a docker container. Yup, this means running docker in docker. This is
> > >    supported with dind but it gets complicated. There is a repo (
> > >    https://github.com/jpetazzo/dind) with support but...
> > >    - There is no license in that repo. Issue: jpetazzo/dind#21
> > >      https://github.com/jpetazzo/dind/issues/21
> > >    - The Docker build process itself uses dind but has a forked
> > >      script checked in to the Docker repo. These have diverged and I worry about
> > >      continued support for the separate dind repo. Issue to harmonize these:
> > >      jpetazzo/dind#22 https://github.com/jpetazzo/dind/issues/22
> > > 
> > > Right now I'm leaning toward copying stuff in and out (option 1).
> > > 
> > > —
> > > Reply to this email directly or view it on GitHub
> > > https://github.com/GoogleCloudPlatform/kubernetes/issues/19#issuecomment-46091452
> > > .
"
130	108	delroth	2014-06-14 19:10:59	NONE	"Maybe also support busybox-style command selection via argv[0]? That way you can symlink/hardlink the main binary under different names.

http://www.busybox.net/FAQ.html#source
"
131	108	jbeda	2014-06-14 21:20:12	CONTRIBUTOR	"@delroth I love the busybox model but I'm not sure that we want to rely on it completely.  We will probably want to have the client tools run on Windows at some point and I'm not sure how that'd work.

In any case, everyone seems on board so I'll mark this as a ""feature request"" instead of a question.
"
132	105	jbeda	2014-06-14 21:22:55	CONTRIBUTOR	"@lavalamp I've seen this too.  Filed #109 to track it.
"
133	19	jbeda	2014-06-14 21:52:00	CONTRIBUTOR	"Thanks for the comments @proppy.

I want the resultant container image to be minimal.  I like the idea of layering it on the busybox image.

That means that the image used to build should be different than the image used at runtime.  Doing `docker cp` to copy things around is one thing I'm looking to avoid.  `dind` is one solution there.  Rebasing will require either `dind` or `docker cp`.  If we don't do this carefully we end up packaging up 60+MB every time we build the image.  That takes too long :)
"
134	103	lavalamp	2014-06-15 00:02:49	MEMBER	"I can see why this is desirable, but it's pretty hard to sell the current behavior where your deletion _timing_ determines the survival of the pods as a feature :)

Let me suggest that we say that after modifying (stopping) a replication controller, the replication controller is guaranteed one pass to make the requested changes. If we then also allowed deleting replication controllers w/out stopping them first, we'd get the semantics of:
- deleting a running replication controller leaves all of its pods running.
- deleting a stopped replication controller leaves none of its pods running.

Which seems more sensible than the race we have now.

We should just implement http async code for all http operations that take longer than some fixed amount of time. Go should make that pretty easy. We should also do api admission, user authorization, etc. Not sure that this stuff should be at the top of our list right now, though.
"
135	19	proppy	2014-06-15 01:47:44	CONTRIBUTOR	"FYI, I have a pending patch to docker that could provide an hacky
alternative.
https://github.com/dotcloud/docker/pull/5715

This would allow something like

```
docker build -t builder ; (docker run builder | docker build -t runner -)
```

On Jun 14, 2014 2:52 PM, ""Joe Beda"" notifications@github.com wrote:

> Thanks for the comments @proppy https://github.com/proppy.
> 
> I want the resultant container image to be minimal. I like the idea of
> layering it on the busybox image.
> 
> That means that the image used to build should be different than the image
> used at runtime. Doing docker cp to copy things around is one thing I'm
> looking to avoid. dind is one solution there. Rebasing will require
> either dind or docker cp. If we don't do this carefully we end up
> packaging up 60+MB every time we build the image. That takes too long :)
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/19#issuecomment-46100695
> .
"
136	113	brendandburns	2014-06-15 03:24:19	CONTRIBUTOR	"Just don't call it config...  ;)
"
137	115	brendandburns	2014-06-16 03:28:01	CONTRIBUTOR	"Thanks!
"
138	2	brendandburns	2014-06-16 03:36:33	CONTRIBUTOR	"Fixed in #111 and #112 

Closing.
"
139	9	brendandburns	2014-06-16 03:37:18	CONTRIBUTOR	"Closed by #98 
"
140	33	brendandburns	2014-06-16 03:40:16	CONTRIBUTOR	"Closed by #102 
"
141	116	brendanburns	2014-06-16 03:52:35	CONTRIBUTOR	"Addresses #114 
"
142	116	lavalamp	2014-06-16 04:11:19	MEMBER	"You may want to take a look at your .git/config; your change appears as though it's authored by ""real-cost""...
"
143	116	brendanburns	2014-06-16 04:15:58	CONTRIBUTOR	"yep.  Already fixed...  (took me a while to figure it out :P)

That's what I get for using my personal account.

On Sun, Jun 15, 2014 at 9:11 PM, Daniel Smith notifications@github.com
wrote:

> You may want to take a look at your .git/config; your change appears as
> though it's authored by ""real-cost""...
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/116#issuecomment-46139736
> .
"
144	117	brendanburns	2014-06-16 04:43:38	CONTRIBUTOR	"closes #96 
"
145	118	brendanburns	2014-06-16 05:11:31	CONTRIBUTOR	"LGTM, but I don't like the name.  Perhaps ""local-up-cluster.sh""?

THanks!
--brendan
"
146	118	lavalamp	2014-06-16 05:16:08	MEMBER	"Renamed.
"
147	123	jbeda	2014-06-16 16:13:18	CONTRIBUTOR	"Thanks for the PR, Ryan.  Unfortunately, I can't merge this unless you can sign our CLA as described in CONTRIB.md.  Even for a small change like this the lawyers are pretty strict.

Thanks!
"
148	122	brendandburns	2014-06-16 16:33:02	CONTRIBUTOR	"Hey Daz,
Thanks for the question.

To find the IP address, figure out what machine the pod is running on, using

```
cloudcfg.sh list /pods
```

Then either use gcutil or the [google cloud console](http://cloud.google.com/console) to determine the external IP address of that machine.

```
gcutil getinstance <instance-name>
```

Let me know if you have more questions, and I'll update our docs to include this.

Thanks!
--brendan
"
149	122	jbeda	2014-06-16 16:34:54	CONTRIBUTOR	"@DazWilkin -- thanks for trying it out.  

Right now mapping a service on a port to an externally addressable IP isn't super smooth.  Right now you need to figure out which minion the container is running on and then find the IP address for that minion -- probably through `gcutil listinstances`.  This is obviously not good enough.

Some things to make this smoother:
- An easier way to decode the true external IP from the API/tools.  It should be easy to figure out what you point your web browser at to get to a specific container.  I've just filed #124 to capture this.
- Easily map a set of containers to the GCP load balancer feature.  This would give a stable IP no matter how the containers move around. This is already captured in #87 
- Build/use a reverse proxy image (nginx? HAProxy?) that is configured via etcd with an API for programming that reverse proxy. That way you could have a single IP:80 for your cluster and direct traffic to the appropriate container/app based on the HTTP host or the request path. 
"
150	123	ryanwalters	2014-06-16 16:39:37	CONTRIBUTOR	"Done! Submitted the form electronically, not sure how long it takes for you guys to get confirmation or what other steps are required. Hopefully that satiates the small army of lawyers lurking around.
"
151	123	jbeda	2014-06-16 16:44:33	CONTRIBUTOR	"Awesome!  Thanks.
"
152	8	brendandburns	2014-06-16 17:56:12	CONTRIBUTOR	"There is now a 'Pending' state, we can add a 'sync' flag to cloudcfg to wait for process startup.
"
153	122	DazWilkin	2014-06-16 18:00:35	NONE	"Thanks for the help. Yes, this worked. I had to create a new firewall rule for the 2 minions for tcp:8080
"
154	125	lavalamp	2014-06-16 18:02:34	MEMBER	"I think you need a rebase...
"
155	125	brendandburns	2014-06-16 18:16:43	CONTRIBUTOR	"Done.
"
156	34	brendandburns	2014-06-16 18:24:12	CONTRIBUTOR	"This is now fixed everywhere.
"
157	103	bgrant0607	2014-06-16 22:37:16	MEMBER	"We definitely want to allow the replicationController to be deleted without deleting the pods it created. It is designed to be loosely coupled to the pods.

replicationController is eventually consistent, so this sounds like a bug in the CLI more than anything. 

We should provide an easy way (ideally an API call) to wait until currentState matches desiredState, and then use that in the command-line tool to wait for completion of all async operations.

We could also provide a bulk delete operation of the kind jbeda suggested, but using general label queries rather than being specific to replicationController.

We should be wary of leaving resources in weird states such that they are in use but can no longer be queried or updated or deleted.
"
158	113	bgrant0607	2014-06-16 22:42:30	MEMBER	"For now, we should ensure that it's straightforward to write robust, idempotent ""up"" and ""down"" scripts with our tools and APIs. I'd lump a more general declarative approach with update support.
"
159	66	bgrant0607	2014-06-17 01:01:10	MEMBER	"I view per-container liveness probes as having 4 main parts:
1. Probe control parameters: At minimum, there needs to be a probe interval (in seconds is probably fine) and timeout period (in same units as probe interval), with reasonable defaults for both. An initial (post-(re)start) delay is also typically needed, to allow for non-trivial application startup times. We could also support a threshold for the number of failures to allow before action is taken (called unhealthy_threshold in the load-balancing context). This would cover retry in the case of spurious failure. If we do, we may also want to support a number of successes before we reset this failure count (healthy_threshold).
2. Probe mechanism. 
   - HTTP GET includes at least port, path, and perhaps URL parameters. 200==success is easy to implement and to understand, but would mean that it could not share the same handler as load-balancer health (i.e., readiness) checks.  Consequently, we may want to treat 404, 500, and 503 as success, also. Intentional failure would be indicated by not responding. Non-standard success/failure criteria and/or more complex logic could be implemented using commands (e.g., wget or curl).
   - Command. Exit 0 would imply success. Agree that ""run in"" would be lighter-weight than a separate container.
3. Action control parameters: The main one is the grace period -- how long to wait before using SIGKILL. We could support configuration of a default grace period for all stop operations on the container, but it is also useful to use different grace periods for different kinds of stop reasons.
4. Action mechanism.
   - SIGTERM. Convenient in many languages but hard to pass other information, such as termination reason and grace period.
   - HTTP POST / web hook.
   - Command, again using ""run in"".

We also want it to be easy to disable/reenable these checks, such as for attaching a debugger and stopping at a breakpoint.
"
160	128	jbeda	2014-06-17 01:47:47	CONTRIBUTOR	"@kseifriedredhat Totally agree -- can you point to stuff as you see it and we'll fix it up.  Or send a PR.

Thanks!
"
161	128	ghost	2014-06-17 01:51:07	NONE	"Just grep the source, so for example:

./kubernetes-master/cluster/saltbase/salt/kube-proxy/default:DAEMON_ARGS=""$DAEMON_ARGS
--etcd_servers=http://{{ ips[0][0] }}:4001""
./kubernetes-master/cluster/saltbase/salt/kubelet/default:DAEMON_ARGS=""$DAEMON_ARGS
-etcd_servers=http://{{ ips[0][0] }}:4001 -address=$HOSTNAME""
./kubernetes-master/cluster/saltbase/salt/apiserver/default:DAEMON_ARGS=""$DAEMON_ARGS
-etcd_servers=http://{{ ips[0][0] }}:4001""
./kubernetes-master/cluster/saltbase/salt/controller-manager/default:DAEMON_ARGS=""$DAEMON_ARGS
-etcd_servers=http://{{ ips[0][0] }}:4001""
"
162	128	jbeda	2014-06-17 01:56:52	CONTRIBUTOR	"Those are all intra-cluster communication.  In the typical deployment none of those would be going over a WAN link.  While ideally that stuff would be over TLS also, distributing key material in a secure way becomes difficult in an automated way.

I'd prioritize places where we grab resources over the internet over securing intra-cluster communication.
"
163	128	jbeda	2014-06-17 02:01:11	CONTRIBUTOR	"I filed #129 to track intra-cluster communication.
"
164	130	brendandburns	2014-06-17 03:42:03	CONTRIBUTOR	"Basically LGTM, some minor stuff.
"
165	130	lavalamp	2014-06-17 05:13:41	MEMBER	"All fixed, PTAL
"
166	132	brendandburns	2014-06-17 15:18:07	CONTRIBUTOR	"Hello,
Thanks for taking a look at Kubernetes. (and thanks for volunteering to write docs!)

There are a couple of things that I think are wrong with your set up:

1) You need to point the kubelet at etcd also (`-etcd_servers=http://10.0.1.115:4001`)

2) The apiserver comes up on port 8080 by default, so you need to use that as your host for cloudcfg

``` sh
./cmd/cloudcfg/cloudcfg -h http://localhost:8080 ...
```

Port 10250 is used for communication between the apiserver and the kubelet for container details (basically 'docker inspect ...'), but that should just work.

Let me know if that helps, or if you run into more problems.  We hang out on irc on #google-containers, so feel free to drop by there for some more interactive help. 

Best (and thanks again for volunteering to help with docs ;)

--brendan
"
167	132	lavalamp	2014-06-17 17:14:56	MEMBER	"One other thing to check: kublet uses the `hostname` command to figure out what to call itself. That needs to match the thing you pass to apiserver's `-machines=` flag. You can use kubelet's `-hostname_override=` flag to force it to use another identifier if necessary.
"
168	135	lavalamp	2014-06-17 18:12:34	MEMBER	"Hm, I see some binary files and .hg directories from dependencies. Is it a problem for the next person to re-get if we add them to .gitignore?
"
169	136	proppy	2014-06-17 18:17:33	CONTRIBUTOR	"LGTM
"
170	136	lavalamp	2014-06-17 18:34:41	MEMBER	"Merging anyway, looks like travis's network blipped.
"
171	135	brendandburns	2014-06-17 19:28:45	CONTRIBUTOR	"Ok, addressed comments, removed the .hg files, cleaned things up.  I think its all set now.

PTAL
Thanks!
--brendan
"
172	135	brendandburns	2014-06-17 19:29:04	CONTRIBUTOR	"(note I still need to add unit tests, so don't merge.  Once there's LGTM, I'll add tests.)
"
173	135	lavalamp	2014-06-17 19:30:33	MEMBER	"LGTM now. One nit, cloudprovider.CloudInterface could be named just cloudprovider.Interface.
"
174	66	bgrant0607	2014-06-17 19:48:36	MEMBER	"It's worth noting that docker stop sends SIGTERM, waits for a parameterized grace period, and then sends SIGKILL, which is basically the behavior we want:
POST /containers/(id)/stop?t=(seconds)
http://docs.docker.com/reference/api/docker_remote_api_v1.12/

FWIW, some do not like SIGKILL:
https://github.com/dotcloud/docker/issues/6446
It was pointed out that kill takes a signal parameter, which maybe they also want to support in stop, but I think a longer grace period is mostly what they need.
"
175	142	lavalamp	2014-06-17 22:13:10	MEMBER	"Thanks for the change! Can you please sign our CLA (instructions in CONTRIB.md) so that we can accept it?
"
176	142	jjhuff	2014-06-17 22:18:15	CONTRIBUTOR	"Oops, forgot about that. Signed.

On Tue, Jun 17, 2014 at 3:13 PM, Daniel Smith notifications@github.com
wrote:

> Thanks for the change! Can you please sign our CLA (instructions in
> CONTRIB.md) so that we can accept it?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/142#issuecomment-46372647
> .
"
177	143	jjhuff	2014-06-17 22:53:14	CONTRIBUTOR	"Worth mentioning that this allows https://github.com/GoogleCloudPlatform/kubernetes#running-a-container-simple-version to work now:)
"
178	135	brendandburns	2014-06-18 00:27:38	CONTRIBUTOR	"Comments addressed.  Tests added.  Ready to merge when you're ready.
"
179	143	lavalamp	2014-06-18 01:08:56	MEMBER	"Thanks for finding this. It seems the unit tests don't exercise it, I'll add a test.
"
180	143	jjhuff	2014-06-18 04:48:42	CONTRIBUTOR	"Sounds good!

On Tue, Jun 17, 2014 at 6:09 PM, Daniel Smith notifications@github.com
wrote:

> Thanks for finding this. It seems the unit tests don't exercise it, I'll
> add a test.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/143#issuecomment-46385551
> .
"
181	132	discordianfish	2014-06-18 10:32:51	CONTRIBUTOR	"Thanks, I've fixed 1) and 2) now still having the hostname issue: Is there a reason not to use the IP address as identifier? I suppose if someone needs to change the IP of a given system, you need to take it down anyways and kubernetes would reschedule the instances, right?
"
182	153	brendandburns	2014-06-18 15:03:54	CONTRIBUTOR	"Thanks for trying Kubernetes, sorry it didn't work for you.

Do you have an old version of go installed?  We currently are building with go 1.2, you can run `go -version` to find the version.

I tried to repro. this on a clean client, and I can't seem to reproduce the problem, also our Travis build is green: https://travis-ci.org/GoogleCloudPlatform/kubernetes, so I think that the problem must be somewhere in your set up.

Please let me know some more details and we can help you debug.

Best!
--brendan
"
183	132	brendandburns	2014-06-18 15:08:46	CONTRIBUTOR	"machine name tends to be more robust since it survives dhcp re-negotiation, VM restarts, etc.

that said, I would think that IP addresses should work, as Daniel mentioned, I think you need to set `--hostname_override=10.0.1.115` on the kubelet binary.  That should override the hostname look up.

Please let us know if you have further problems, or if that doesn't work.

Best!
--brendan
"
184	147	vmarmol	2014-06-18 17:32:13	CONTRIBUTOR	"Since we use Docker to start containers, we'll need support there as well. @rjnagal @vishh and myself have been working towards that. There should already be some support for this and more will drop in the near future.
"
185	134	jjhuff	2014-06-18 17:35:03	CONTRIBUTOR	"It seems like a new ManifestSet/ManifestList type might make the most sense. Changing the existing manifest type will have more tendrils.

I also like the idea of reading a dir in addition to a file. That starts to make bootstrapping easier...
"
186	153	brendandburns	2014-06-18 17:42:46	CONTRIBUTOR	"I validated that this is due to trying to compile with Go version 1.0 rather than 1.2.

I'll update the scripts to validate that the appropriate version of Go is installed.

So please upgrade your installed version of golang, and try again.

Thanks!
--brendan
"
187	122	jbeda	2014-06-18 17:48:01	CONTRIBUTOR	"Closing this -- let us know if you have any more snags.
"
188	156	brendandburns	2014-06-18 18:20:41	CONTRIBUTOR	"I think we'd rather not have the Kubelet calling back into the master, since it can result in massive fan-in storms of messages though.

However, I can definitely see the value in caching the information inside the apiserver.  So what do you think about having the apiserver periodically poll all Kubelets for information, and caching that information locally.  I think that would satisfy all of the needs you enumerated, while still enabling the master to control the flow of information.

What do you think?
"
189	156	jbeda	2014-06-18 18:30:44	CONTRIBUTOR	"In the past I've suggested a strategy where we have the master (or some fellow traveller server process) scrapes the nodes regularly and writes back to the master/etcd.  We'd then return how stale results are in our API.

Further, there'd be an API option to ask for ""up to the second"" results that would result in a sync call out to the node.
"
190	156	jjhuff	2014-06-18 18:33:19	CONTRIBUTOR	"I think that any apiserver->kublet polling will have problems in some
deployments. For example;
- Using a mix of internal machines and GCE. Without a fair amount of work,
  neither has full access to the other's network -- each has to talk via NAT.
- Running the master on, say, AppEngine. That'd be super handy for
  reducing common-mode failures.

I hear ya on the fan-in problem. That problem already exists to some extent
with the HTTP polling option. Since apiserver is stateless (yay!), it
should scale-out reasonably well as long as the backing store can handle
it...but we already have that problem.

Perhaps only push updates on state changes?

On Wed, Jun 18, 2014 at 11:20 AM, brendandburns notifications@github.com
wrote:

> I think we'd rather not have the Kubelet calling back into the master,
> since it can result in massive fan-in storms of messages though.
> 
> However, I can definitely see the value in caching the information inside
> the apiserver. So what do you think about having the apiserver periodically
> poll all Kubelets for information, and caching that information locally. I
> think that would satisfy all of the needs you enumerated, while still
> enabling the master to control the flow of information.
> 
> What do you think?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/156#issuecomment-46473525
> .
"
191	156	brendandburns	2014-06-18 18:45:17	CONTRIBUTOR	"I hear you about the problems with mixed topologies, pushing updates on state changes actually does have the same fan-in problems, suppose all of your tasks fail at the same time with a packet of death, you'll still see a storm of messages.

Hacking in the polling from the apiserver is going to be easier in the short term, so I think we'll at least do that first.
"
192	156	bgrant0607	2014-06-18 19:52:44	MEMBER	"The main reasons for the apiserver to contact kubelets rather than the other way are:
- connection storms, which admittedly can be mitigated by fuzzing
- other communication storms, which can be mitigated by the apiserver returning callback times to the kubelets
- facilitating use of kubelets without apiserver, which admittedly could be controlled via configuration
- support multiple independent state scrapers
- shard state scraping amongst multiple servers, which admittedly could be handled with redirects
- natural for heartbeating, starting new containers, and remote management of kubelet itself

Regardless which components initiate the connection, we maywant to implement a number of optimizations, especially once we start to collect resource stats:
- state caching
- change notification rather than polling
- change significance filtering
"
193	155	lavalamp	2014-06-18 19:55:51	MEMBER	"Hm, good luck parsing travis's dev go version output  :(
"
194	144	lavalamp	2014-06-18 20:12:59	MEMBER	"Comments addressed, controller is now over 80% test coverage.
"
195	150	lavalamp	2014-06-18 20:16:45	MEMBER	"LGTM other than missing boilerplate.
"
196	127	bgrant0607	2014-06-18 20:20:15	MEMBER	"Thoughts on multi-container pods:

First of all, I think restart behavior should be specified at the pod level rather than the container level. It wouldn't make sense for one container to terminate and another to restart forever, for example.

Run forever is obviously easy -- we're doing it now.

Run once is fairly easy, too, I think. As soon as one container terminates, probably all should be terminated (call this policy ""any"").

For run until success, we could restart individual containers until each succeeds (call this policy ""all"").

We should make all vs. any a separate policy from forever vs. success vs. once. Another variant people would probably want is a ""leader"" container, to which the other containers' lifetimes would be tied. Since we start containers in order, the leader would need to be the first one in the list.  To play devil's advocate, if we had event hooks (#140), the user could probably implement ""any"" and ""leader"" policies if we only provided ""all"" semantics.

Now, set-level behavior:

replicationController at least needs to know the conditions under which it should replace terminated/lost instances. It's hard to provide precise success semantics since containers can be lost with indeterminate exit status, but that's technically true even for a single pod. replicationController should be able to see the restart policies and termination reasons of the pods it controls. If a pod terminates and should not be restarted, I think replicationController should just automatically reduce its desired replica count by one.

I could also imagine users wanting any/all/leader behavior at the set level. However, I don't think we should do that, which leads me to believe we shouldn't do it at the pod level for now, either. If we were to provide the functionality at the set level, it shouldn't be tied to repllicationController. Instead, it would be a separate policy resource associated with the pods via its own label selector. This would allow it to work at either the service level or replicationController level or over any other grouping the user desired. We should ensure that it's not too hard to implement these types of policies using event hooks.
"
197	150	brendandburns	2014-06-18 20:25:33	CONTRIBUTOR	"Comment addressed.  ptal

Thanks!
--brendan
"
198	150	lavalamp	2014-06-18 20:33:47	MEMBER	"Unsure why the integration test failed. This shouldn't have broken it.
"
199	150	brendandburns	2014-06-18 20:35:25	CONTRIBUTOR	"Ugh, fixed.  I'll look at the tests...

--brendan

On Wed, Jun 18, 2014 at 1:33 PM, Daniel Smith notifications@github.com
wrote:

> Unsure why the integration test failed. This shouldn't have broken it.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/150#issuecomment-46490191
> .
"
200	155	brendandburns	2014-06-18 20:46:30	CONTRIBUTOR	"Used a Travis env var as an escape hatch.  This should be good now.

--brendan
"
201	156	jjhuff	2014-06-18 20:48:30	CONTRIBUTOR	"My concern has mainly been figuring out the blockers to actually
deploying kubernetes outside of a strictly GCE environment given security
and network topology constraints. That makes me want to take scissors to
these all-all communication patterns:)

Perhaps a hybrid approach is going to be the best:
- Cache state on the master -- either in memory or backed by etcd/whatever.
  In memory should be reasonable for a single apiserver instance, but we'd
  need persistence for anything more.
- Optional state push (interval or change based) from the kubelets. Much
  like all of it's existing options.  The state push would just populate the
  cache

This adds caching to the baseline config, gives the option to reverse the
apiserver-kubelet communication, and preserves the ability for other tools
to scrape the kubelets.

On Wed, Jun 18, 2014 at 12:52 PM, bgrant0607 notifications@github.com
wrote:

> The main reasons for the apiserver to contact kubelets rather than the
> other way are:
> - connection storms, which admittedly can be mitigated by fuzzing
> - other communication storms, which can be mitigated by the apiserver
>   returning callback times to the kubelets
> - facilitating use of kubelets without apiserver, which admittedly
>   could be controlled via configuration
> - support multiple independent state scrapers
> - shard state scraping amongst multiple servers, which admittedly
>   could be handled with redirects
> - natural for heartbeating, starting new containers, and remote
>   management of kubelet itself
> 
> Regardless which components initiate the connection, we maywant to
> implement a number of optimizations, especially once we start to collect
> resource stats:
> - state caching
> - change notification rather than polling
> - change significance filtering
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/156#issuecomment-46485272
> .
"
202	150	brendandburns	2014-06-18 20:55:00	CONTRIBUTOR	"Looks like etcd flaked in the integration test.  Let's submit this, we're tracking that flake elsewhere.
"
203	156	brendandburns	2014-06-18 21:03:05	CONTRIBUTOR	"SGTM.

I will send a PR for the poll and cache support today, and then work on the
optional Kubelet -> apiserver code path.

Thanks for bearing with us as we sort through this stuff ;)

--brendan

On Wed, Jun 18, 2014 at 1:48 PM, Justin Huff notifications@github.com
wrote:

> My concern has mainly been figuring out the blockers to actually
> deploying kubernetes outside of a strictly GCE environment given security
> and network topology constraints. That makes me want to take scissors to
> these all-all communication patterns:)
> 
> Perhaps a hybrid approach is going to be the best:
> - Cache state on the master -- either in memory or backed by
>   etcd/whatever.
>   In memory should be reasonable for a single apiserver instance, but we'd
>   need persistence for anything more.
> - Optional state push (interval or change based) from the kubelets. Much
>   like all of it's existing options. The state push would just populate the
>   cache
> 
> This adds caching to the baseline config, gives the option to reverse the
> apiserver-kubelet communication, and preserves the ability for other tools
> to scrape the kubelets.
> 
> On Wed, Jun 18, 2014 at 12:52 PM, bgrant0607 notifications@github.com
> wrote:
> 
> > The main reasons for the apiserver to contact kubelets rather than the
> > other way are:
> > - connection storms, which admittedly can be mitigated by fuzzing
> > - other communication storms, which can be mitigated by the apiserver
> >   returning callback times to the kubelets
> > - facilitating use of kubelets without apiserver, which admittedly
> >   could be controlled via configuration
> > - support multiple independent state scrapers
> > - shard state scraping amongst multiple servers, which admittedly
> >   could be handled with redirects
> > - natural for heartbeating, starting new containers, and remote
> >   management of kubelet itself
> > 
> > Regardless which components initiate the connection, we maywant to
> > implement a number of optimizations, especially once we start to collect
> > resource stats:
> > - state caching
> > - change notification rather than polling
> > - change significance filtering
> > 
> > —
> > Reply to this email directly or view it on GitHub
> > <
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/156#issuecomment-46485272>
> > 
> > .
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/156#issuecomment-46491857
> .
"
204	156	jjhuff	2014-06-18 21:13:30	CONTRIBUTOR	"No problem! Thanks for even doing the work -- I was happy to do that.  I was thinking of tackling #134 as well, but I can wait if you want to avoid conflicts.
"
205	156	brendandburns	2014-06-18 21:22:04	CONTRIBUTOR	"We're happy to take the work!  Feel free to take on #134, and I might
delegate the push stuff to you too.  We'll see how the pull cache goes....

Best
--brendan

On Wed, Jun 18, 2014 at 2:13 PM, Justin Huff notifications@github.com
wrote:

> No problem! Thanks for even doing the work -- I was happy to do that. I
> was thinking of tackling #134
> https://github.com/GoogleCloudPlatform/kubernetes/issues/134 as well,
> but I can wait if you want to avoid conflicts.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/156#issuecomment-46495019
> .
"
206	160	brendandburns	2014-06-18 21:29:27	CONTRIBUTOR	"I think I prefer option 1.  There is already a communication channel from master to kublet (see kublet_server.go) and I think we can merge the info in there.

Let me know if you want/need more details.

Let's not store this in etcd for now, I don't see too much utility in storing this kind of transient info in etcd.
"
207	161	lavalamp	2014-06-18 21:30:36	MEMBER	"LGTM, but integration doesn't build.
"
208	161	brendandburns	2014-06-18 21:31:38	CONTRIBUTOR	"Yeah, it was dependent on #150.  I'm updating it now.

--brendan

On Wed, Jun 18, 2014 at 2:30 PM, Daniel Smith notifications@github.com
wrote:

> LGTM, but integration doesn't build.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/161#issuecomment-46496974
> .
"
209	160	monnand	2014-06-18 21:36:58	CONTRIBUTOR	"@brendanburns Oh, I didn't notice kubelet has already exposed a REST API. OK. I could work on it this week. Thank you @brendandburns .
"
210	161	brendandburns	2014-06-18 21:39:49	CONTRIBUTOR	"Comments addressed.  PTAL

Thanks!
--brendan
"
211	161	brendandburns	2014-06-18 21:58:34	CONTRIBUTOR	"comments addressed.  ptal.

Thanks!
--brendan
"
212	162	brendandburns	2014-06-19 00:00:38	CONTRIBUTOR	"Comments addressed, please re-check.
"
213	140	bgrant0607	2014-06-19 00:25:10	MEMBER	"Docker issues requesting postcreate hooks:
https://github.com/dotcloud/docker/issues/3317
https://github.com/dotcloud/docker/issues/252

One of these mentioned that Docker planned to support hooks for every event, but that was quite a while ago, so I'm not sure whether that's still in plan.
"
214	164	lavalamp	2014-06-19 00:26:04	MEMBER	"This isn't ready to merge yet because I need to do the raml doc generation.
"
215	164	lavalamp	2014-06-19 01:24:29	MEMBER	"Hm, the doc generation isn't working on my system. Will have to debug later.
"
216	164	brendandburns	2014-06-19 03:11:49	CONTRIBUTOR	"This basically LGTM, once you do the RAML gen, and look at the gofmt part, I'm ready to merge.

Thanks!
"
217	87	brendandburns	2014-06-19 03:13:23	CONTRIBUTOR	"Closed by #149 and #135 
"
218	124	brendandburns	2014-06-19 03:14:01	CONTRIBUTOR	"Closed by #163 and #161 
"
219	164	brendandburns	2014-06-19 04:13:19	CONTRIBUTOR	"Ok, LGTM.  It looks like the HTML was generated?  Is this ready to merge?
"
220	165	lavalamp	2014-06-19 04:32:28	MEMBER	"LGTM
"
221	165	brendandburns	2014-06-19 04:33:39	CONTRIBUTOR	"Updated to add the comment.
"
222	166	lavalamp	2014-06-19 05:23:42	MEMBER	"I had some other ideas on how to do this... let's chat tomorrow, maybe what I'm thinking won't work.
"
223	166	proppy	2014-06-19 05:35:07	CONTRIBUTOR	"Something you can consider is keeping the synchronous operation and just wrapping the call in a goroutine.

```
outc := make(chan interface{})
errc := make(chan error)
go func() {
   obj, err := storage.Create(object)
   if err != nil {
      errc <- obj
      return
   }
   objc <- obj
}()
select {
  case obj := <-objc:
        // ...
  case err := <-errc:
        // ...
  case <- time.After(1 * time.Second):
        // ...
}
```
"
224	97	thockin	2014-06-19 06:30:28	MEMBER	"I endorse this message.
"
225	13	proppy	2014-06-19 08:37:43	CONTRIBUTOR	"PTAL, refactored all client tests and add more coverage with service + makeRequests
"
226	44	drewcsillag	2014-06-19 13:46:19	CONTRIBUTOR	"Looks like this PR is no longer applicable.  Closing
"
227	43	drewcsillag	2014-06-19 13:52:14	CONTRIBUTOR	"CLA should now be signed by my employer.
"
228	167	brendanburns	2014-06-19 15:48:14	CONTRIBUTOR	"Awesome! Thanks for doing this. I'll take a detailed look today. In the
meantime, can you sign our CLA?

Instructions are in CONTRIB.md. It's basically identical to the Apache CLA

Thanks again.
Brendan

On Thu, Jun 19, 2014, 5:53 AM, discordianfish notifications@github.com
wrote:

> Hi,
> 
> I've put all this in one PR, hope that's okay. It's basically what I had
> to change to get it working in my everything-is-dockerized environment.
> I've tried to come up with ""good commits"". Let me know if you prefer
> single PRs for each of them.
> 
> So the idea here is to run kubernetes itself in Docker containers,
> therefor I created three Dockerfiles. The documentation assumes that this
> will be added as automated builds in the Google account. Guess that's up
> for discussion so let me know if I should change that.
> The Dockerfile in / will create a kubernetes 'base image' which just
> includes the code/binaries and is used by kubernetes-node and
> kubernetes-server. This might not fit all all deployments but it's a good
> start and easy to understand.
> To make this possible I added a -docker flag to point to the docker
> address (since with the bind-mount we can't use /var/run/docker.sock in the
> container). Beside that I made kubelet use the client lib for pulling the
> 
> ## image instead of the binary (which isn't installed in the container).
> 
> You can merge this Pull Request by running
> 
>   git pull https://github.com/discordianfish/kubernetes various-fixes
> 
> Or view, comment on, or merge it at:
> 
>   https://github.com/GoogleCloudPlatform/kubernetes/pull/167
> Commit Summary
> - Add docker addr flag and check for etcd_machines
> - Print errors as string
> - Use docker client lib instead of binary for pulls
> - Add Dockerfiles and deployment instructions
> 
> File Changes
> - _A_ Dockerfile
>   https://github.com/GoogleCloudPlatform/kubernetes/pull/167/files#diff-0
>   (14)
> - _M_ README.md
>   https://github.com/GoogleCloudPlatform/kubernetes/pull/167/files#diff-1
>   (3)
> - _M_ cmd/kubelet/kubelet.go
>   https://github.com/GoogleCloudPlatform/kubernetes/pull/167/files#diff-2
>   (9)
> - _A_ kubernetes-node/Dockerfile
>   https://github.com/GoogleCloudPlatform/kubernetes/pull/167/files#diff-3
>   (12)
> - _A_ kubernetes-node/README.md
>   https://github.com/GoogleCloudPlatform/kubernetes/pull/167/files#diff-4
>   (13)
> - _A_ kubernetes-node/run.sh
>   https://github.com/GoogleCloudPlatform/kubernetes/pull/167/files#diff-5
>   (14)
> - _A_ kubernetes-server/Dockerfile
>   https://github.com/GoogleCloudPlatform/kubernetes/pull/167/files#diff-6
>   (6)
> - _A_ kubernetes-server/README.md
>   https://github.com/GoogleCloudPlatform/kubernetes/pull/167/files#diff-7
>   (9)
> - _A_ kubernetes-server/run.sh
>   https://github.com/GoogleCloudPlatform/kubernetes/pull/167/files#diff-8
>   (12)
> - _M_ pkg/kubelet/kubelet.go
>   https://github.com/GoogleCloudPlatform/kubernetes/pull/167/files#diff-9
>   (40)
> 
> Patch Links:
> - https://github.com/GoogleCloudPlatform/kubernetes/pull/167.patch
> - https://github.com/GoogleCloudPlatform/kubernetes/pull/167.diff
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/167.
"
229	167	proppy	2014-06-19 15:54:29	CONTRIBUTOR	"Related: #19 and #141 
"
230	113	bgrant0607	2014-06-19 16:00:21	MEMBER	"I filed another issue for idempotent creation (#148). We can use this issue for multi-resource declarative configuration, starting with up/down.
"
231	134	jjhuff	2014-06-19 16:09:38	CONTRIBUTOR	"Are you guys concerned about breaking compatibility with the kubelet's http GET/PUT interface?
"
232	167	jbeda	2014-06-19 16:49:03	CONTRIBUTOR	"Wow!  Thanks for the PR.

Unfortunately, this overlaps with some work that I have in progress now too.  So we'll have to figure out how to merge it.

First off -- if you could break out the addr flag commit, the error commit and the client lib commit into separate PRs (or a single one) -- that'd be great -- we can land those right away once we clear the CLA.

My plan is a little bit more ambitious than what you have here -- but that is maybe why it is taking longer.  I'm looking to pre-compile the binaries before packaging the runtime Dockerfiles.  This'll help to minimize download/upload size and will help cluster start time.  I also want to rework the GCE cluster scripts to use these.  That is obviously separable though.

Things that overlap and are different:
- I lean toward running 1 process per container instead of bundling multiple servers into a single container.  This is the model we use internally at Google and it has worked out well.  In fact, the pod concept was introduced to represent a bundle of containers that work together like this. Advantages here:
  - As we add extend container services like log collection and monitoring, being able to pinpoint these to a binary instead of a container is super useful.  If memory usage on a container starts to run away, you want to know the process/binary that is doing it.
  - The kubelet will monitor for crashes and restart containers in that case.  Over time, these resources are another production metric that you'll want to track and alert on.  If we bundle multiple processes in a container, we hide that.
  - In long running production systems, being able to upgrade/push different components on different schedules is useful.  We may want to push a new etcd server without taking down the rest of the binaries on a machine.
- I'd love to start using the kubelet in a more ""static"" mode to handle the master containers.  That would consist of getting the kubelet running on the master (with out looking an etcd) and then have it run a pod for the master components by reading out of a manifest file on disk.
- While there are some that run etcd on every node, it just doesn't scale in a clustered way that well.  Once the coreos guys introduce a 'read replica' of etcd, this may make more sense.  Details on etcd cluster size [here](https://coreos.com/docs/cluster-management/scaling/etcd-optimal-cluster-size/).

That being said, having a single Docker image for the master and node does simplify start up!  Hopefully we can get the ""manual set up"" steps simple enough that it is easy to port Kubernetes to new environments.

Hopefully I'll have a PR out today or tomorrow (I need to as I'm going on vacation after that) that should get some of this stuff more concrete.  You can see the start of what I'm doing in the `build/` directory.
"
233	134	jbeda	2014-06-19 16:50:44	CONTRIBUTOR	"Hopefully we can work this out using the version string.  If we had to break compat at this early stage it wouldn't be the end of the world but we should get in the habit of not doing that.
"
234	167	discordianfish	2014-06-19 17:12:20	CONTRIBUTOR	"First of all, I've split out #169 

Regarding using pre-built binaries: I would also prefer separating build- from runtime so we would just have the built binaries in the images. But since Docker unfortunately doesn't support that (yet) and using a Dockerfile is the canonical way to build Docker images I went this way. Since Docker caches the images layer by layer the actual size it needs to fetch on update, even if we keep all build stuff around, shouldn't be a big problem.

Totally agree with ""running one app per container"" in general. But I'm being pragmatic here: Instead of writing tons of bash scripts to build and run the various components on a vanilla docker cluster I decided to wrap up the required components in two easily deployable images. It's more meant as a quick start than to be used as it is for production deployments. It will help people trying kubernetes on their docker hosts. It took me quite some time without that until I got it running. 

But I love the idea of using kubernetes to host kubernetes, so to me it depends on how fast we can get there and whether you want to provide something in the meanwhile so people can play with it.
"
235	164	lavalamp	2014-06-19 17:15:56	MEMBER	"Still having problems getting boot2docker working... The HTML is only half changed, it's missing the change that involved more than just find-and-replace.
"
236	167	discordianfish	2014-06-19 17:24:53	CONTRIBUTOR	"(I've just moved cdd6b3c back in here so this branch can be built/used)
"
237	140	bgrant0607	2014-06-19 18:35:49	MEMBER	"Just got support from tianon for runin/exec/enter support: https://github.com/dotcloud/docker/issues/1228
So we can use that to execute hooks inside containers
"
238	164	lavalamp	2014-06-19 20:29:43	MEMBER	"OK, I'm pretty sure it's not a problem with my docker setup. Let's merge this and I'll work with @bgrant0607 to get the help gen working.
"
239	156	brendandburns	2014-06-19 20:50:49	CONTRIBUTOR	"Polling from the master was added in #171 

I'll work on optional push next.
"
240	174	vmarmol	2014-06-19 20:51:55	CONTRIBUTOR	"I think maybe a more manageable approach would be for Kubernetes to depend on the released version of cAdvisor (e.g.: the Docker image), rather than to build it and use it from source. WDYT? @brendanburns 
"
241	166	brendandburns	2014-06-19 20:59:40	CONTRIBUTOR	"Ok, I discussed this offline w/ @lavalamp and we came up with the following approach, I added a MakeAsync method that is used by the registries now, this means that individual storage's don't need deal in containers.

Take another look and let me know what you think.
"
242	174	vmarmol	2014-06-19 21:08:49	CONTRIBUTOR	"Spoke offline, we need the client library and structs for the data. What do you all think about just importing those two paths from cadvisor? If not, the whole source it is.
"
243	166	proppy	2014-06-19 21:14:53	CONTRIBUTOR	"I would have thought the MakeAsync logic would be around the `ControllerRegistryStorage` methods calls ot in their impl.
"
244	173	jjhuff	2014-06-19 22:05:03	CONTRIBUTOR	"Looks like the dir test is flaky. Fixing.
"
245	174	monnand	2014-06-19 22:13:33	CONTRIBUTOR	"@lavalamp  Addressed the comment. PTAL.
"
246	166	brendandburns	2014-06-19 22:18:04	CONTRIBUTOR	"The problem with the go routine wrapping the whole thing is that you don't get immediate feedback for validation errors.  In this approach, you get an immediate error if the request isn't accepted, and then you get a channel that you can optionally wait on for the result.
"
247	13	brendandburns	2014-06-19 22:21:46	CONTRIBUTOR	"Rebase, and then LGTM.
"
248	173	jjhuff	2014-06-19 22:22:45	CONTRIBUTOR	"That's better!
"
249	174	monnand	2014-06-19 22:29:33	CONTRIBUTOR	"@lavalamp PTAL.
"
250	174	monnand	2014-06-19 22:49:27	CONTRIBUTOR	"I forgot to mention: the current code is not wired with kubelet now. I will add it in another PR.
"
251	166	brendandburns	2014-06-19 22:50:08	CONTRIBUTOR	"Comments addressed, ptal.

Thanks
--brendan
"
252	173	lavalamp	2014-06-19 22:58:01	MEMBER	"LGTM. Thanks for the change!
"
253	174	monnand	2014-06-20 00:29:50	CONTRIBUTOR	"I think the license part is cleared now.
"
254	178	thockin	2014-06-20 00:30:59	MEMBER	"Caveat: Still very much learning go.  I'm worried about the direction this goes.  I think we will see other ""kinds"" of volumes, and I don't think we want to represent them all as flags on the volume struct.

I was thinking something more like (pardon the nonsense syntax):

volume {
  name: ""host_foo_bar""
  source {
    kind: ""HOST_FS""
    host_path: ""/foo/bar""
  }
}

and we could formalize on-disk volumes as

volume {
  name: ""pod_local_disk""
  source {
    kind: ""LOCAL""
  }
}

With the explicit default for source to be kind: ""LOCAL""

Something like that.
"
255	178	vmarmol	2014-06-20 00:32:21	CONTRIBUTOR	"I agree that we want to go somewhere along those lines, my understanding was that there was already work toward doing this ""the right way"" but that it would take longer. This is the minimal change needed to get this feature in.
"
256	178	vmarmol	2014-06-20 00:33:55	CONTRIBUTOR	"I didn't want to make anything beyond that since it'd make assumptions of what that final API would be. If people want to do that route now we can, I just don't want us to get sidetracked by it :)
"
257	178	lavalamp	2014-06-20 00:52:04	MEMBER	"I would vote for using an enumeration instead of a boolean, even in the interim.
"
258	181	jkaplowitz	2014-06-20 03:25:02	CONTRIBUTOR	"LGTM. Nice, straightforward enough for me to understand even as a Go novice.
"
259	178	brendandburns	2014-06-20 03:59:40	CONTRIBUTOR	"+1, can we convert this into an enumeration?  That should still keep it pretty simple.

Thanks!
--brendan
"
260	178	vmarmol	2014-06-20 04:03:39	CONTRIBUTOR	"SGTM, any prefered way to do an enum in JSON? I'm guessing a string with pre-defined values is the best way to go? Its easier to write by hand than an int value.
"
261	174	monnand	2014-06-20 04:06:58	CONTRIBUTOR	"All comments addressed. PTAL.
"
262	174	brendandburns	2014-06-20 04:09:15	CONTRIBUTOR	"Generally LGTM, some minor comments.
"
263	178	monnand	2014-06-20 04:11:12	CONTRIBUTOR	"@vmarmol FYI, here is how go's protobuf library deals with enum:

```
type SomeEnumType int

func (self SomeEnumType) MarshalJson() ([]byte, error){
// ...
}

func (self *SomeEnumeType) UnmarshalJson([]byte) error {

// ...
}
```

I think in old cAdvisor code, I wrote some helpers would could automate such step. Let me try to find it and feel free to use it or not.
"
264	178	brendandburns	2014-06-20 04:11:40	CONTRIBUTOR	"Yeah, that's generally the way Apiary has done it.

On Thu, Jun 19, 2014 at 9:03 PM, Victor Marmol notifications@github.com
wrote:

> SGTM, any prefered way to do an enum in JSON? I'm guessing a string with
> pre-defined values is the best way to go? Its easier to write by hand than
> an int value.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/178#issuecomment-46643057
> .
"
265	178	vmarmol	2014-06-20 04:12:01	CONTRIBUTOR	"Done. PTAL @brendanburns @thockin @lavalamp 
"
266	178	vmarmol	2014-06-20 04:12:53	CONTRIBUTOR	"@monnand ah yes I've seen those, they are nifty and we could use them as a string over the wire. May be okay as is for now though
"
267	178	brendandburns	2014-06-20 04:12:56	CONTRIBUTOR	"Sorry to be nit-picky, can we go all-caps for the values, to demonstrate that their enum, not string?

thx!
"
268	178	vmarmol	2014-06-20 04:15:00	CONTRIBUTOR	"@brendanburns haha, np. done. PTAL
"
269	174	monnand	2014-06-20 04:22:59	CONTRIBUTOR	"Changed the code accordingly. Use `w.Write(data)` instead of `fmt.FprintX()`. PTAL.
"
270	178	brendandburns	2014-06-20 04:28:43	CONTRIBUTOR	"thanks!
"
271	178	thockin	2014-06-20 04:43:35	MEMBER	"I'm OK with this, but I will try to make the case to deprecate this field
ASAP :)

On Thu, Jun 19, 2014 at 9:28 PM, brendandburns notifications@github.com
wrote:

> thanks!
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/178#issuecomment-46643902
> .
"
272	178	vmarmol	2014-06-20 04:46:32	CONTRIBUTOR	"I happily welcome that effort :)
"
273	178	brendandburns	2014-06-20 04:48:27	CONTRIBUTOR	"Hrm, @thockin Sorry didn't mean to over rule you.  I thought this was implementing your suggestion.

I'm imagining extending this by adding a ""Source"" field

MountType: PERSISTENT_DISK
MountPath: /foo/bar
Source: http://some/pd/path

or

MountType: GIT
MountPath: /my/git/path
Source: git://some/git/repo@commit

I suppose elevating it into its own type is ok, but I'm not sure it buys much....
"
274	178	brendandburns	2014-06-20 04:49:06	CONTRIBUTOR	"fwiw, if @thockin if you feel strongly, I'll do the deprecate PR tomorrow.

--brendan
"
275	178	thockin	2014-06-20 05:09:57	MEMBER	"No, this is fine for now, until we have a full solution and demonstration
of why it is better.  You didn't overrule me in any sense :)

My main point here is that we have two structures - Volume and VolumeMount.
 Volume is supposed to represent that ""what"" and VolumeMount is supposed to
represent the ""Where"" and ""How"".

Up until this change, ""what"" was always ""a pod-private local directory"".
 With this change we have added a new ""what"" but we've stuck it in the
""where"" structure.  You'll notice that this can ONLY identity-mount things
- there is no opportunity to remap host /foo onto container /bar (very
  similar to Docker's --volumes-from, which is broken in the same way).

A ""source"" field is the right direction, but it belongs in Volume, not
VolumeMount.  Consider that a volume definition is pod-scope, but mounts
are container-scope.  You want to spell out the source once, at pod-scope,
and let each container mount it where they want.

This also lays groundwork to get fancier with permissions, storage medium,
quota, etc.

I'm happy to go into my ideas here more, if you like.  We spent a lot of
time on this area internally, and I think (hope!) we can apply some of
those lessons here.  But I also want to hear from others if you all feel
we're screwing it up.

I would strike out on this now, but for this fabled intern who wants to
tackle some of this ;)

Tim

On Thu, Jun 19, 2014 at 9:49 PM, brendandburns notifications@github.com
wrote:

> fwiw, if @thockin https://github.com/thockin if you feel strongly, I'll
> do the deprecate PR tomorrow.
> 
> --brendan
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/178#issuecomment-46644628
> .
"
276	178	brendandburns	2014-06-20 05:18:22	CONTRIBUTOR	"Ah, you are 100% right.  I should have caught that.

Sorry!

(and yes, Jimmy claims that his intern will be starting on this on Monday,
I'll double check tomorrow)

That said, there's no reason that we can't lay in the right data structures
before the intern does the implementation.

--brendan

On Thu, Jun 19, 2014 at 10:10 PM, Tim Hockin notifications@github.com
wrote:

> No, this is fine for now, until we have a full solution and demonstration
> of why it is better. You didn't overrule me in any sense :)
> 
> My main point here is that we have two structures - Volume and
> VolumeMount.
> Volume is supposed to represent that ""what"" and VolumeMount is supposed to
> represent the ""Where"" and ""How"".
> 
> Up until this change, ""what"" was always ""a pod-private local directory"".
> With this change we have added a new ""what"" but we've stuck it in the
> ""where"" structure. You'll notice that this can ONLY identity-mount things
> - there is no opportunity to remap host /foo onto container /bar (very
>   similar to Docker's --volumes-from, which is broken in the same way).
> 
> A ""source"" field is the right direction, but it belongs in Volume, not
> VolumeMount. Consider that a volume definition is pod-scope, but mounts
> are container-scope. You want to spell out the source once, at pod-scope,
> and let each container mount it where they want.
> 
> This also lays groundwork to get fancier with permissions, storage medium,
> quota, etc.
> 
> I'm happy to go into my ideas here more, if you like. We spent a lot of
> time on this area internally, and I think (hope!) we can apply some of
> those lessons here. But I also want to hear from others if you all feel
> we're screwing it up.
> 
> I would strike out on this now, but for this fabled intern who wants to
> tackle some of this ;)
> 
> Tim
> 
> On Thu, Jun 19, 2014 at 9:49 PM, brendandburns notifications@github.com
> wrote:
> 
> > fwiw, if @thockin https://github.com/thockin if you feel strongly,
> > I'll
> > do the deprecate PR tomorrow.
> > 
> > --brendan
> > 
> > —
> > Reply to this email directly or view it on GitHub
> > <
> > https://github.com/GoogleCloudPlatform/kubernetes/pull/178#issuecomment-46644628>
> > 
> > .
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/178#issuecomment-46645281
> .
"
277	178	thockin	2014-06-20 05:22:13	MEMBER	"I think there's room for interpretation on what ""the right structures"" are
and I don't want to prescribe exactly what it should be without
reconsidering it anew.  I'm looking forward to some design discussion with
intern Danny.

Tim

On Thu, Jun 19, 2014 at 10:18 PM, brendandburns notifications@github.com
wrote:

> Ah, you are 100% right. I should have caught that.
> 
> Sorry!
> 
> (and yes, Jimmy claims that his intern will be starting on this on Monday,
> I'll double check tomorrow)
> 
> That said, there's no reason that we can't lay in the right data
> structures
> before the intern does the implementation.
> 
> --brendan
> 
> On Thu, Jun 19, 2014 at 10:10 PM, Tim Hockin notifications@github.com
> wrote:
> 
> > No, this is fine for now, until we have a full solution and
> > demonstration
> > of why it is better. You didn't overrule me in any sense :)
> > 
> > My main point here is that we have two structures - Volume and
> > VolumeMount.
> > Volume is supposed to represent that ""what"" and VolumeMount is supposed
> > to
> > represent the ""Where"" and ""How"".
> > 
> > Up until this change, ""what"" was always ""a pod-private local directory"".
> > With this change we have added a new ""what"" but we've stuck it in the
> > ""where"" structure. You'll notice that this can ONLY identity-mount
> > things
> > - there is no opportunity to remap host /foo onto container /bar (very
> >   similar to Docker's --volumes-from, which is broken in the same way).
> > 
> > A ""source"" field is the right direction, but it belongs in Volume, not
> > VolumeMount. Consider that a volume definition is pod-scope, but mounts
> > are container-scope. You want to spell out the source once, at
> > pod-scope,
> > and let each container mount it where they want.
> > 
> > This also lays groundwork to get fancier with permissions, storage
> > medium,
> > quota, etc.
> > 
> > I'm happy to go into my ideas here more, if you like. We spent a lot of
> > time on this area internally, and I think (hope!) we can apply some of
> > those lessons here. But I also want to hear from others if you all feel
> > we're screwing it up.
> > 
> > I would strike out on this now, but for this fabled intern who wants to
> > tackle some of this ;)
> > 
> > Tim
> > 
> > On Thu, Jun 19, 2014 at 9:49 PM, brendandburns notifications@github.com
> > 
> > wrote:
> > 
> > > fwiw, if @thockin https://github.com/thockin if you feel strongly,
> > > I'll
> > > do the deprecate PR tomorrow.
> > > 
> > > --brendan
> > > 
> > > —
> > > Reply to this email directly or view it on GitHub
> > > <
> > 
> > https://github.com/GoogleCloudPlatform/kubernetes/pull/178#issuecomment-46644628>
> > 
> > > .
> > 
> > —
> > Reply to this email directly or view it on GitHub
> > <
> > https://github.com/GoogleCloudPlatform/kubernetes/pull/178#issuecomment-46645281>
> > 
> > .
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/178#issuecomment-46645551
> .
"
278	178	jkaplowitz	2014-06-20 05:41:54	CONTRIBUTOR	"Just emailed him urging him to switch primary focus and establish contact
with you ASAP, with a bit of procedural guidance (nothing about the
substance) and a link to this discussion. His other work is at the point
where this switch makes good sense.
On Jun 19, 2014 10:22 PM, ""Tim Hockin"" notifications@github.com wrote:

> I think there's room for interpretation on what ""the right structures"" are
> and I don't want to prescribe exactly what it should be without
> reconsidering it anew. I'm looking forward to some design discussion with
> intern Danny.
> 
> Tim
> 
> On Thu, Jun 19, 2014 at 10:18 PM, brendandburns notifications@github.com
> 
> wrote:
> 
> > Ah, you are 100% right. I should have caught that.
> > 
> > Sorry!
> > 
> > (and yes, Jimmy claims that his intern will be starting on this on
> > Monday,
> > I'll double check tomorrow)
> > 
> > That said, there's no reason that we can't lay in the right data
> > structures
> > before the intern does the implementation.
> > 
> > --brendan
> > 
> > On Thu, Jun 19, 2014 at 10:10 PM, Tim Hockin notifications@github.com
> > wrote:
> > 
> > > No, this is fine for now, until we have a full solution and
> > > demonstration
> > > of why it is better. You didn't overrule me in any sense :)
> > > 
> > > My main point here is that we have two structures - Volume and
> > > VolumeMount.
> > > Volume is supposed to represent that ""what"" and VolumeMount is
> > > supposed
> > > to
> > > represent the ""Where"" and ""How"".
> > > 
> > > Up until this change, ""what"" was always ""a pod-private local
> > > directory"".
> > > With this change we have added a new ""what"" but we've stuck it in the
> > > ""where"" structure. You'll notice that this can ONLY identity-mount
> > > things
> > > - there is no opportunity to remap host /foo onto container /bar (very
> > >   similar to Docker's --volumes-from, which is broken in the same way).
> > > 
> > > A ""source"" field is the right direction, but it belongs in Volume, not
> > > VolumeMount. Consider that a volume definition is pod-scope, but
> > > mounts
> > > are container-scope. You want to spell out the source once, at
> > > pod-scope,
> > > and let each container mount it where they want.
> > > 
> > > This also lays groundwork to get fancier with permissions, storage
> > > medium,
> > > quota, etc.
> > > 
> > > I'm happy to go into my ideas here more, if you like. We spent a lot
> > > of
> > > time on this area internally, and I think (hope!) we can apply some of
> > > those lessons here. But I also want to hear from others if you all
> > > feel
> > > we're screwing it up.
> > > 
> > > I would strike out on this now, but for this fabled intern who wants
> > > to
> > > tackle some of this ;)
> > > 
> > > Tim
> > > 
> > > On Thu, Jun 19, 2014 at 9:49 PM, brendandburns <
> > > notifications@github.com>
> > > 
> > > wrote:
> > > 
> > > > fwiw, if @thockin https://github.com/thockin if you feel
> > > > strongly,
> > > > I'll
> > > > do the deprecate PR tomorrow.
> > > > 
> > > > --brendan
> > > > 
> > > > —
> > > > Reply to this email directly or view it on GitHub
> > > > <
> > 
> > https://github.com/GoogleCloudPlatform/kubernetes/pull/178#issuecomment-46644628>
> > 
> > > > .
> > > 
> > > —
> > > Reply to this email directly or view it on GitHub
> > > <
> > 
> > https://github.com/GoogleCloudPlatform/kubernetes/pull/178#issuecomment-46645281>
> > 
> > > .
> > 
> > —
> > Reply to this email directly or view it on GitHub
> > <
> > https://github.com/GoogleCloudPlatform/kubernetes/pull/178#issuecomment-46645551>
> > 
> > .
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/178#issuecomment-46645702
> .
"
279	13	proppy	2014-06-20 06:20:42	CONTRIBUTOR	"abandonning this PR in favor of #183 since I can't push to my private fork anymore.
"
280	169	discordianfish	2014-06-20 11:01:20	CONTRIBUTOR	"Ok, I've removed the check. Working on CLA, will sent it later hopefully.
"
281	147	smarterclayton	2014-06-20 14:50:32	CONTRIBUTOR	"Would QoS tiers specified via labeling or orthogonal to labeling (assuming orthogonal given presentations, just want to make sure)?   Do you see the need for feedback between a minion about current resource load and the scheduler, and if so, is this a backchannel (a formal event bus) concept that is part of Kubernetes or encapsulated behind the scheduler interface?
"
282	184	brendandburns	2014-06-20 15:33:09	CONTRIBUTOR	"Thanks for the fix.  I hate to ask, but its required: can you sign our CLA, details are in:

https://github.com/GoogleCloudPlatform/kubernetes/blob/master/CONTRIB.md

Thanks again!
--brendan
"
283	174	brendandburns	2014-06-20 15:36:19	CONTRIBUTOR	"LGTM, thanks!
"
284	147	thockin	2014-06-20 15:55:02	MEMBER	"Caveat, still coming up to speed on everything, but that won't stop me from
having an opinion.

QoS and administrative or config information should never be labels.
 Labels are the domain of the user, not the system.

On Fri, Jun 20, 2014 at 7:50 AM, Clayton Coleman notifications@github.com
wrote:

> Would QoS tiers specified via labeling or orthogonal to labeling (assuming
> orthogonal given presentations, just want to make sure)? Do you see the
> need for feedback between a minion about current resource load and the
> scheduler, and if so, is this a backchannel (a formal event bus) concept
> that is part of Kubernetes or encapsulated behind the scheduler interface?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/147#issuecomment-46686825
> .
"
285	182	thockin	2014-06-20 16:07:34	MEMBER	"LGTM modulo not knowing go that well :)
"
286	147	bgrant0607	2014-06-20 16:12:22	MEMBER	"@smarterclayton : I agree with @thockin . QoS specifications need to be first-class fields in our API, together with resource specifications (#168).

For one possible example of what this could look like, see lmctfy:
https://github.com/google/lmctfy/blob/master/include/lmctfy.proto
"
287	184	lavalamp	2014-06-20 16:16:28	MEMBER	"Argh, I missed the comma here, too. Thanks for finding! Will merge as soon as you sign the CLA. :)
"
288	169	lavalamp	2014-06-20 16:20:04	MEMBER	"LGTM, thanks for the change.
"
289	173	lavalamp	2014-06-20 16:22:14	MEMBER	"Can you rebase this so we can merge? Thanks!
"
290	173	jjhuff	2014-06-20 16:35:21	CONTRIBUTOR	"Done. Only a few conflicts, but I think they got sorted out. Just waiting on travis.
"
291	188	smarterclayton	2014-06-20 16:45:09	CONTRIBUTOR	"The VM per pod model would negate the container efficiency gains - I'm not sure how many people would be interested in deploying like that vs. simply using a VM.  

Practically speaking, if routable IP per pod is out of the technical capabilities of existing clouds at reasonable densities (is the 60 IP per amazon m1.xlarge too low for practical use cases?) or out of the administrative / organizational capabilities of non-cloud shops (which deserves further investigation), and if IPv6 is still 2-3 years out from reasonable deployment, then the kubernetes model is only deployable on GCE in practice.  Would be good to list out the practical limits in other clouds (openstack neutron, aws, soft layer) as well as a recommended IP per pod configuration that would work without a ton of admin headaches on metal.

It's possible that dynamic port allocation could be an alternate mode, supported with a subset of features and known limitations.  What would that abstraction have to look like for Kubernetes to continue to work on the ideal path?  A few things I can think of - the scheduler has to be aware of port exhaustion and record allocated ports OR the exposed ports have to be reported back via some backchannel to the master.  If there is a global record of allocated ports, the mechanism to efficiently distribute that port information to the appropriate proxies is required.  You _must_ implement at least one level of abstraction between container communication (either in a local or shared proxy or an iptables NAT translation a la geard).  You also must implement a more complex migration path for things like CRIU - with more steps before and afterwards to ensure that network abstraction is ready to accept the moved container.
"
292	174	monnand	2014-06-20 16:59:13	CONTRIBUTOR	"Thank you @brendanburns !
"
293	180	brendandburns	2014-06-20 17:48:36	CONTRIBUTOR	"Small stuff, basically LGTM
"
294	180	lavalamp	2014-06-20 18:00:03	MEMBER	"I'm still debugging this a bit. Will address comments later and repush later today.

What do you think about having this talk to the cloud provider, such that creating/deleting a minion creates/removes a new instance?
"
295	180	jbeda	2014-06-20 18:31:58	CONTRIBUTOR	"@lavalamp Right now the salt code will automatically reconfigure and restart the apiserver when the set of minions change.  I think you might have to do a highstate on the master but we could fix that.
"
296	184	gottwald	2014-06-20 18:41:05	CONTRIBUTOR	"signed
"
297	184	lavalamp	2014-06-20 18:43:39	MEMBER	"Thanks again!
"
298	108	gottwald	2014-06-20 19:08:32	CONTRIBUTOR	"I like the subcommand idea as seen in git, go command, gcloud and so on....
"
299	140	bgrant0607	2014-06-21 00:25:33	MEMBER	"I investigated Docker's event stream. It provides container id and event type (e.g., start, stop). It doesn't provide further details about the events. Also, it's obviously asynchronous with respect to the events. At least pre-start and post-termination event hook commands would be most useful if executed synchronously, inline with container execution.

Docker restart allows restarting the same command in the same container with the same container id and filesystem, even after the death of the previous process. There's no way to change the command executed AFAICT, however. If the forthcoming ""runin""/""exec"" allowed execution in dead containers, we could maybe use it for post-termination hooks. Pre-start hook commands look ugly without Docker support. No response yet to my docker-dev question about whether Docker is actually planning to add hook support.

We could override the container entrypoint with the pre-start hook command and then use ""runin"" to execute the real entrypoint, but the container's status, wait, restart, etc. would be broken. Creating a new container image that included the pre-start command, actual entrypoint, and post-termination hook might work, but we'd need to carefully propagate arguments, signals, exit status, etc.

The main reason for a post-start hook would be consistency, but it also might be convenient for start actions that don't block the start of the container's entrypoint, such as registration in a third-party discovery service, pushing events to pubsub, etc. ""runin"" should just work, though there may be a race if the application immediately terminated. Probably it would be useful to serialize execution with respect to later hooks on the same entity.

If we wrapped the application we could intercept SIGTERM in order to execute the pre-termination hook, and just pass the signal on to the application if no hook were specified. Again, we probably want to serialize with respect to the post-termination hook. It might not be super-useful to execute the pre-termination hook in the case that it weren't a planned container stop, but I'm not sure whether it's more natural to execute it prior to the post-termination hook regardless or whether it would be annoying for the pre-termination hook to execute when the application was already dead. OTOH, that case would probably need to be handled since the application could die at any time, including concurrently with the start of the pre-termination hook.

Asynchronous webhooks would be comparatively easy to support. The most difficult issue is what to do about auth. If generated by a command in the container, presumably they could authenticate as the container, so it looks appealing to only support command hooks. However, there are situations where we wouldn't have an obvious container to execute commands in, such as for pod lifecycle hooks or, even more obviously, replicationController and service lifecycle event hooks.
"
300	180	lavalamp	2014-06-21 01:04:44	MEMBER	"I have this mostly ready to go, but I'd like to get #196 merged first.
"
301	197	lavalamp	2014-06-21 15:33:08	MEMBER	"Thank you very much for the test!
"
302	198	lavalamp	2014-06-21 19:40:59	MEMBER	"Thanks. I think this is a general issue, I'll file a bug.
"
303	166	lavalamp	2014-06-21 20:50:11	MEMBER	"btw, @brendandburns @brendanburns-- I'm working on some changes to cloudcfg, so you don't need to update that.
"
304	200	lavalamp	2014-06-21 20:56:05	MEMBER	"I agree that the -- logic is a terrible hack. To solve it for real, though, I was thinking we should just make unique names and keep track of what names we've used, rather than parsing the names for our pattern. I'm OK with this patch in the interim, though.
"
305	200	jjhuff	2014-06-21 20:59:57	CONTRIBUTOR	"Yeah.
Does docker allow us to attach arbitrary data to a container's info? Or maybe go with nested containers? Although, that'd probably mean that any managed containers would die if the kubelet died, I'm not sure how I feel about that...
"
306	200	lavalamp	2014-06-21 21:05:04	MEMBER	"Yeah, the kubelet would have to store the mapping persistently, or possibly apiserver can pre-generate globally unique ids for this purpose (a la #199). It's a requirement that containers keep running even if kubelet restarts, so any subcontainer design would have to take that into account.
"
307	200	jjhuff	2014-06-21 21:10:33	CONTRIBUTOR	"Cool. #199 looks like the ticket.

On Sat, Jun 21, 2014 at 2:05 PM, Daniel Smith notifications@github.com
wrote:

> Yeah, the kubelet would have to store the mapping persistently, or
> possibly apiserver can pre-generate globally unique ids for this purpose (a
> la #199 https://github.com/GoogleCloudPlatform/kubernetes/issues/199).
> It's a requirement that containers keep running even if kubelet restarts,
> so any subcontainer design would have to take that into account.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/200#issuecomment-46764749
> .
"
308	201	jjhuff	2014-06-22 16:34:50	CONTRIBUTOR	"Yup, that's lots cleaner!
"
309	201	lavalamp	2014-06-22 18:03:00	MEMBER	"Thanks for the change!
"
310	180	lavalamp	2014-06-22 19:18:19	MEMBER	"Will fix this build once #196 is merged.
"
311	196	lavalamp	2014-06-23 16:55:24	MEMBER	"Comments addressed.
"
312	180	jjhuff	2014-06-23 16:59:25	CONTRIBUTOR	"""What do you think about having this talk to the cloud provider, such that creating/deleting a minion creates/removes a new instance?""
If I'm understanding what you mean, this feels 'backwards' to me. Creating a new instance often includes a number of infrastructure/environment specific things (network, images, packages, etc). I doubt we want to tackle wrapping all of that complexity.
It might be cleaner to have a minions register with the master, which would also remove the need for salt to restart it.
"
313	180	lavalamp	2014-06-23 17:08:18	MEMBER	"Sounds good. As this is written now, minions could use this API to add themselves. But we need to figure out some security stuff (How do minions trust the master? How does master trust the minions?) first before enabling that.

I do at least intend to use the cloud provider package to look up a minion's IP address upon a GET request. That seems pretty useful.
"
314	196	lavalamp	2014-06-23 17:37:51	MEMBER	"(That build failure looks like the etcd flake we get once in a while)
"
315	207	lavalamp	2014-06-23 17:51:50	MEMBER	"It shouldn't:

https://github.com/GoogleCloudPlatform/kubernetes/blob/master/hack/e2e-test.sh#L30

But my shell-fu is weak, maybe there's something wrong with that?
"
316	207	lavalamp	2014-06-23 17:55:46	MEMBER	"I think it's missing some quotes. I'll make a PR.
"
317	208	lavalamp	2014-06-23 19:04:22	MEMBER	"Will merge when build finishes.

Also possible (and more go-ish) with:

$ source hack/config-go.sh
$ cd pkg/kublet
$ go test
"
318	208	lavalamp	2014-06-23 19:18:10	MEMBER	"Looks like travis is down or something. Merging.
"
319	209	thockin	2014-06-23 20:16:12	MEMBER	"Don't you want to update README.md to add this to the hooks setup?
"
320	209	lavalamp	2014-06-23 20:17:22	MEMBER	"If you linked (`ln -s ...`) as written in the readme, it's already taken care of for you.
"
321	209	lavalamp	2014-06-23 20:18:47	MEMBER	"I'll see about adding a check for this to the travis build.
"
322	180	lavalamp	2014-06-23 20:38:20	MEMBER	"I think this is safe to merge now.
"
323	209	thockin	2014-06-23 20:49:48	MEMBER	"Ahh, I missed it being called from an existing hook.  Sorry.
"
324	209	jkaplowitz	2014-06-23 20:55:35	CONTRIBUTOR	"We should be careful that our automation doesn't remove external copyright notices from contributed code, nor add Google copyright notices to files we haven't modified. At the same time, we should make sure that Google-authored files do have our copyright notice, and that the Apache license notice is present wherever it should be. Our internal open source legal folks might have good advice on how to strike the right balance here.
"
325	209	lavalamp	2014-06-23 20:59:27	MEMBER	"The script doesn't modify any files, it only flags files, and it can be overridden. We're excluding third party dependencies already.
"
326	209	jkaplowitz	2014-06-23 21:03:33	CONTRIBUTOR	"SGTM
"
327	212	jkaplowitz	2014-06-23 22:10:58	CONTRIBUTOR	"The concept expressed in the pull request title LGTM as a useful fix. Leaving a proper code review to someone who knows Go (not me).
"
328	139	vishh	2014-06-23 23:48:33	MEMBER	"Is /run a tmpfs? There is an [outstanding PR](https://github.com/docker/libcontainer/pull/16) for this in libcontainer.
"
329	139	bgrant0607	2014-06-24 01:31:20	MEMBER	"Good call. Looks like not.

Here's df from a google/nodejs container:
Filesystem                                             1K-blocks    Used Available Use% Mounted on
rootfs                                                  10188088 1639712   8007808  17% /
none                                                    10188088 1639712   8007808  17% /
tmpfs                                                     304556       0    304556   0% /dev
shm                                                        65536       0     65536   0% /dev/shm
/dev/disk/by-uuid/485b0b37-5e5f-4878-85a4-2d8653315786  10188088 1639712   8007808  17% /.dockerinit
/dev/disk/by-uuid/485b0b37-5e5f-4878-85a4-2d8653315786  10188088 1639712   8007808  17% /etc/resolv.conf
/dev/disk/by-uuid/485b0b37-5e5f-4878-85a4-2d8653315786  10188088 1639712   8007808  17% /etc/hostname
/dev/disk/by-uuid/485b0b37-5e5f-4878-85a4-2d8653315786  10188088 1639712   8007808  17% /etc/hosts
/dev/disk/by-uuid/485b0b37-5e5f-4878-85a4-2d8653315786  10188088 1639712   8007808  17% /data
tmpfs                                                     304556       0    304556   0% /proc/kcore
"
330	213	brendandburns	2014-06-24 02:47:37	CONTRIBUTOR	"Generally LGTM, but integration tests are failing...

I think pulling it is ok, but if we see lots of flakes, we may need to change out policy.
"
331	205	brendandburns	2014-06-24 02:51:30	CONTRIBUTOR	"basically LGTM, two small nits.
"
332	214	lavalamp	2014-06-24 04:02:50	MEMBER	"Thanks for the change!
"
333	127	bgrant0607	2014-06-24 04:59:41	MEMBER	"FWIW, two people have recommended the Erlang supervisor model/spec:
http://www.erlang.org/doc/design_principles/sup_princ.html
http://www.erlang.org/doc/man/supervisor.html

IIUC, Erlang calls my ""any"" policy ""one for all"" and my ""all"" policy ""one for one"".
"
334	199	thockin	2014-06-24 05:07:17	MEMBER	"Internally we use RFC4122 UUIDs for identifying pods.  Any objections to making this part of the pod setup?  I guess it would really be a string (like ""id"") but with the strong suggestion that it be an encoded UUID.

Or we could use docker-style 256 bit randoms, but that might get confusing.

If we further lock down container names to RFC1035 labels, we can use <uuid>.<name> as the docker container name, which seems much nicer than the current dashes and underscores :)

What think?
"
335	213	lavalamp	2014-06-24 05:07:44	MEMBER	"Yeah, I think this is going to be too flaky. Will fix tomorrow.
"
336	216	thockin	2014-06-24 05:13:15	MEMBER	"updated
"
337	195	thockin	2014-06-24 05:15:01	MEMBER	"+1 - I'd advocate dropping the .sh so we have freedom to change it up when we need to grow beyond shell.
"
338	188	thockin	2014-06-24 05:16:02	MEMBER	"I've started a doc on this topic, but will be out of office about half of this week.
"
339	175	thockin	2014-06-24 05:17:34	MEMBER	"Interesting.  Let's consider how to expose this carefully.  I had an idea around exposing the existing --net=container through the API such that we could move to private IP per container and still have a clean APi for saying which mode people wanted per pod.  This ... does not fit my model :)
"
340	195	proppy	2014-06-24 05:19:10	CONTRIBUTOR	"did you consider having a `k8s` or `kube` singleton command with actions?
"
341	139	thockin	2014-06-24 05:20:36	MEMBER	"Is /run LSB compliant?
"
342	134	thockin	2014-06-24 05:23:11	MEMBER	"Is it possible to detect the top-most JSON object and swictch modes based on whether it is an array (of pods) or an object (single pod)?  I know this is getting sketchy...
"
343	134	lavalamp	2014-06-24 05:25:48	MEMBER	"See my pending pr which does this.
On Jun 23, 2014 10:23 PM, ""Tim Hockin"" notifications@github.com wrote:

> Is it possible to detect the top-most JSON object and swictch modes based
> on whether it is an array (of pods) or an object (single pod)? I know this
> is getting sketchy...
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/134#issuecomment-46933276
> .
"
344	134	proppy	2014-06-24 05:26:50	CONTRIBUTOR	"#213
"
345	97	thockin	2014-06-24 05:31:52	MEMBER	"From other discussion:

We have two structures - Volume and VolumeMount.  Volume is supposed to represent that ""what"" and VolumeMount is supposed to represent the ""Where"" and ""How"".

The ""what"" is, so far, ""a pod-private local directory"".  We want to add more kinds of ""what"" but we've stuck it (the host FS flag) in the ""where"" structure.  You'll notice that this can ONLY identity-mount things - there is no opportunity to remap host /foo onto container /bar (very similar to Docker's --volumes-from, which is broken in the same way).

A ""source"" field is the right direction, but it belongs in Volume, not VolumeMount.  Consider that a volume definition is pod-scope, but mounts are container-scope.  You want to spell out the source once, at pod-scope, and let each container mount it where they want.

This also lays groundwork to get fancier with permissions, storage medium, quota, etc.

I'm happy to go into my ideas here more, if you like.  We spent a lot of time on this area internally, and I think (hope!) we can apply some of those lessons here.  But I also want to hear from others if you all feel we're screwing it up.
"
346	15	thockin	2014-06-24 05:32:32	MEMBER	"Why did we disable IP tables?
"
347	139	bgrant0607	2014-06-24 05:36:05	MEMBER	"It will be once LSB is updated to FHS 3.0:
http://www.linuxbase.org/betaspecs/fhs/fhs.html#runRuntimeVariableData

http://askubuntu.com/questions/57297/why-has-var-run-been-migrated-to-run
"
348	217	brendandburns	2014-06-24 05:36:33	CONTRIBUTOR	"fwiw, I fixed the tests...
"
349	97	discordianfish	2014-06-24 10:43:54	CONTRIBUTOR	"@brendandburns What about extending volumes in Docker itself? There is current an issue for collecting general issues with volumes to discuss changes: https://github.com/dotcloud/docker/issues/6496
"
350	15	jbeda	2014-06-24 13:21:33	CONTRIBUTOR	"We needed to disable the blanket NAT for all traffic out of the bridge and only nat for `! 10.0.0.0/8`.  But the Docker flag for disable that also disabled the NAT for setting up iptables for incoming port maps.  Docker then falls back on user mode proxying of that traffic.

We either need to make Docker itself more flexible or start setting up the port forwarding in the kubelet.  If we do it in the kubelet we could do something like:
1. Set up the netns container for the pod
2. Set up the port forwards
3. Launch other containers for the pod

If we want to adapt Docker, it might be enough to split the iptables flag into a flag for configuring the iptables for the bridge in general vs. configuring iptables for the incoming port mapping.
"
351	195	jbeda	2014-06-24 13:24:20	CONTRIBUTOR	"@thockin To get rid of the .sh, see #194 

@proppy Single binary w/ subcommands, see #108.  I'd be okay with one server command and one client command.  But at least one single server binary would help speed things along and simplify deployments.
"
352	175	jbeda	2014-06-24 13:26:21	CONTRIBUTOR	"The proxy is special.  We could run it outside of docker or outside the kubelet, but ideally we'd manage it from the kubelet.

Or we could find a way to port map a range to it, but ideally we'd avoid NAT for all traffic through the proxy.
"
353	97	thockin	2014-06-24 14:33:07	MEMBER	"I am in favor of extending docker, in theory, but I am wary of how flexible
they want to make it.  I can imagine a lot of cool stuff modeled as
volumes, and I don't know how to handle it generically.  It seems way
simpler to say that docker can put a volume around anything you can mount
in the host.
On Jun 24, 2014 3:44 AM, ""discordianfish"" notifications@github.com wrote:

> @brendandburns https://github.com/brendandburns What about extending
> volumes in Docker itself? There is current an issue for collecting general
> issues with volumes to discuss changes: dotcloud/docker#6496
> https://github.com/dotcloud/docker/issues/6496
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/97#issuecomment-46956476
> .
"
354	15	thockin	2014-06-24 15:26:35	MEMBER	"I feel like a moron for not understanding the problem after that
explanation.  Why do we need to disable NAT?  I'm still digging into the
impl of Kubelet, so maybe it will emerge.

Aren't you on vacation?

On Tue, Jun 24, 2014 at 6:21 AM, Joe Beda notifications@github.com wrote:

> We needed to disable the blanket NAT for all traffic out of the bridge and
> only nat for ! 10.0.0.0/8. But the Docker flag for disable that also
> disabled the NAT for setting up iptables for incoming port maps. Docker
> then falls back on user mode proxying of that traffic.
> 
> We either need to make Docker itself more flexible or start setting up the
> port forwarding in the kubelet. If we do it in the kubelet we could do
> something like:
> 1. Set up the netns container for the pod
> 2. Set up the port forwards
> 3. Launch other containers for the pod
> 
> If we want to adapt Docker, it might be enough to split the iptables flag
> into a flag for configuring the iptables for the bridge in general vs.
> configuring iptables for the incoming port mapping.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/15#issuecomment-46969487
> .
"
355	15	brendandburns	2014-06-24 15:39:33	CONTRIBUTOR	"Joe is the expert on our config, but I think we needed to disable NAT to be
able to assign our own IP addresses.

Brendan
On Jun 24, 2014 8:26 AM, ""Tim Hockin"" notifications@github.com wrote:

> I feel like a moron for not understanding the problem after that
> explanation. Why do we need to disable NAT? I'm still digging into the
> impl of Kubelet, so maybe it will emerge.
> 
> Aren't you on vacation?
> 
> On Tue, Jun 24, 2014 at 6:21 AM, Joe Beda notifications@github.com
> wrote:
> 
> > We needed to disable the blanket NAT for all traffic out of the bridge
> > and
> > only nat for ! 10.0.0.0/8. But the Docker flag for disable that also
> > disabled the NAT for setting up iptables for incoming port maps. Docker
> > then falls back on user mode proxying of that traffic.
> > 
> > We either need to make Docker itself more flexible or start setting up
> > the
> > port forwarding in the kubelet. If we do it in the kubelet we could do
> > something like:
> > 1. Set up the netns container for the pod
> > 2. Set up the port forwards
> > 3. Launch other containers for the pod
> > 
> > If we want to adapt Docker, it might be enough to split the iptables flag
> > into a flag for configuring the iptables for the bridge in general vs.
> > configuring iptables for the incoming port mapping.
> > 
> > ## 
> > 
> > Reply to this email directly or view it on GitHub
> > <
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/15#issuecomment-46969487
> > 
> > .
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/15#issuecomment-46986391
> .
"
356	15	jbeda	2014-06-24 15:43:25	CONTRIBUTOR	"Quick clarification and then back to vacation :)

We want traffic between containers to use the container API across nodes.  Say we have `Node A` with a container IP space of `10.244.1.0/24` and `Node B` with a container IP space of `10.244.2.0/24`.  And we have `Container A1` at `10.244.1.1` and `Container B1` at `10.244.2.1`.  We want `Container A1` to talk to `Container B1` directly with no NAT.  `B1` should see the ""source"" in the IP packets of `10.244.1.1` -- not the ""primary"" IP for `Node A`.  That means that we want to turn off NAT for traffic between containers (and also between VMs and containers).

But we don't support (yet?) the extra container IPs that we've provisioned talking to the internet directly.  We don't map external IPs to the container IPs.  So we solve that problem by having traffic that isn't to the internal network (`! 10.0.0.0/8`) get NATed through the primary host IP address so that it can get 1:1 NATed by the GCE networking when talking to the internet.  Similarly, incoming traffic from the internet has to get NATed/proxied through the host IP.

So we end up with 3 cases:
1. `Container -> Container` or `Container <-> VM`.  These should use `10.` addresses directly and there should be no NAT.
2. `Container -> Internet`.  These have to get mapped to the primary host IP so that GCE knows how to egress that traffic.  There is actually 2 layers of NAT here: `Container IP -> Internal Host IP -> External Host IP`.  The first level happens in the guest with IP tables and the second happens as part of GCE networking.  The first one (`Container IP -> internal host IP`) does dynamic port allocation while the second maps ports 1:1.
3. `Internet -> Container`.  This also has to go through the primary host IP and also has 2 levels of NAT, ideally.  However, the path currently is a proxy with `(External Host IP -> Internal Host IP -> Docker) -> (Docker -> Container IP)`.  Once this bug is closed, it should be `External Host IP -> Internal Host IP -> Container IP`.  But to get that second arrow we have to set up the port forwarding iptables rules per mapped port.

If this is still confusing let me know and I can find some time for a hangout to dive into the details.
"
357	97	discordianfish	2014-06-24 16:23:51	CONTRIBUTOR	"@thockin That's up for discussion but I would be a vocal proponent of extending it and right now is the best time to discuss that. I see both at least using physical disks as well as network storage for volumes as very useful.
"
358	134	jjhuff	2014-06-24 16:54:37	CONTRIBUTOR	"Besides the backward compatibility side of things, is support for a single pod likely to be useful?  That'd essentially limit the kubelet to running only a single pod via http.
"
359	219	lavalamp	2014-06-24 17:00:45	MEMBER	"Nice catch.
"
360	205	lavalamp	2014-06-24 17:09:48	MEMBER	"Should be ready to merge now.
"
361	169	lavalamp	2014-06-24 17:38:56	MEMBER	"Let us know when the CLA happens. I'd love to merge this change :)
"
362	213	lavalamp	2014-06-24 17:44:02	MEMBER	"Hm, I just ran it half a dozen times locally with no flakes. Let's see if travis passes it this time.
"
363	220	jkaplowitz	2014-06-24 17:54:21	CONTRIBUTOR	"This is something I was planning to mention after the pace of this week
subsides. It's definitely a wise idea, both for our users and for whoever
takes on ongoing maintenance of the container-optimized image builds.

On Tue, Jun 24, 2014 at 10:50 AM, Daniel Smith notifications@github.com
wrote:

> How often do we make breaking changes, and what does it look like to
> upgrade a running kubernetes cluster? Should we start cutting releases,
> with tags and things?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/220.
"
364	213	lavalamp	2014-06-24 17:56:53	MEMBER	"OK, I'm clearly going to have to make a local copy to get this to work in Travis.
"
365	223	thockin	2014-06-24 18:03:48	MEMBER	"Dup of #175 
"
366	221	lavalamp	2014-06-24 18:04:42	MEMBER	"Our startup scripts redirect stderr/out to (e.g.) /var/log/kublet.log. I haven't actually investigated glog; will that still work as expected?
"
367	221	thockin	2014-06-24 18:12:15	MEMBER	"Nope - great point.  What do we want the behavior to be?  We can log
everything to stderr and redirect to /var/log/kubelet.log, or we can do it
glog-style and write logfiles to /var/log/kubelet/\* (kubelet.INFO, and so
on).

stderr+redirect will capture etcd and anyone else who uses log.\* so I think
I vote for that.

On Tue, Jun 24, 2014 at 11:04 AM, Daniel Smith notifications@github.com
wrote:

> Our startup scripts redirect stderr/out to (e.g.) /var/log/kublet.log. I
> haven't actually investigated glog; will that still work as expected?
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/221#issuecomment-47007596
> .
"
368	218	thockin	2014-06-24 18:17:04	MEMBER	"I stripped the Debugf change for now.  PTAL
"
369	221	lavalamp	2014-06-24 18:17:10	MEMBER	"Can we make a new log.Logger for etcd, and have it write to a /var/log/kublet/kubelet.ETCD?

I think that's nicer, but we may need to provide a flag so users can control the output directory. If that's difficult, then continuing the redirection is fine with me, too.
"
370	169	discordianfish	2014-06-24 18:25:20	CONTRIBUTOR	"@lavalamp We've signed and mailed the CLA last Friday
"
371	217	brendandburns	2014-06-24 18:26:27	CONTRIBUTOR	"Updated to use the new Client api.  PTAL.

Thanks!
--brendan
"
372	169	lavalamp	2014-06-24 18:28:58	MEMBER	"Ah, OK, great. I'll merge as soon as it shows up on our list.
"
373	199	jjhuff	2014-06-24 18:41:19	CONTRIBUTOR	"Both points sound great to me.  These encoded names are just plain ugly:)

Id is also a required field (network containers break otherwise), so it seems weird to have them marked as 'omitempty'. This is probably more an issue for config files than anything else.
"
374	217	lavalamp	2014-06-24 19:01:00	MEMBER	"I'm merging this to unblock Brendan. Nits can be fixed with another PR.
"
375	221	lavalamp	2014-06-24 19:03:01	MEMBER	"Brendan and I both merged a commit, so it looks like you'll have to rebase and do the find/replace again. Sorry... 
"
376	221	thockin	2014-06-24 19:30:11	MEMBER	"Do we really want to split the logs across files?  It's etcd for now, but
who know what else it will be in the future.  For better or worse, glog is
NOT the go logging standard.

On Tue, Jun 24, 2014 at 12:03 PM, Daniel Smith notifications@github.com
wrote:

> Brendan and I both merged a commit, so it looks like you'll have to rebase
> and do the find/replace again. Sorry...
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/221#issuecomment-47016063
> .
"
377	226	lavalamp	2014-06-24 20:16:48	MEMBER	"Thanks for the test!
"
378	227	lavalamp	2014-06-24 20:27:11	MEMBER	"Definitely an improvement. Thanks.
"
379	232	jjhuff	2014-06-24 22:18:59	CONTRIBUTOR	"Handy, I was just about to do this... Thanks!
"
380	213	lavalamp	2014-06-25 00:02:57	MEMBER	"YES! I finally got this to pass travis.

Before this change, integration wasn't testing kubelet. Now it at least verifies that it tries to start containers!
"
381	234	lavalamp	2014-06-25 00:21:57	MEMBER	"The GET command tacks on the hostname currently. I'm not sure if we want to promise that scheduling will be so instant that you always get a hostname immediately when you create a pod...
"
382	236	lavalamp	2014-06-25 00:30:59	MEMBER	"This is configurable in cluster/config-default.sh, but perhaps it makes sense to have them confirm the first time they run.
"
383	221	thockin	2014-06-25 00:32:43	MEMBER	"OK.  I have made it so that all logs go to glog now, even etcd.  I meant to rebase and then add that as an extra diff, but I messed it up, sorry.

This still needs to define log dirs per command.

PTAL @lavalamp 
"
384	239	brendandburns	2014-06-25 02:32:13	CONTRIBUTOR	"I don't think there is anything in flight that I know of.  Someone can always look at the history and merge them back in.
"
385	221	thockin	2014-06-25 03:29:44	MEMBER	"Fixed Forever() comment and rebased.
"
386	221	lavalamp	2014-06-25 03:44:26	MEMBER	"Just one more nit, also looks like a merge problem in integration.go.
"
387	239	lavalamp	2014-06-25 03:46:22	MEMBER	"Thanks for noticing this, I meant to have this in my previous PR but it totally slipped my mind.
"
388	221	thockin	2014-06-25 03:52:55	MEMBER	"OK, actually builds this time.
"
389	221	lavalamp	2014-06-25 04:01:15	MEMBER	"Thanks for the change!
"
390	221	thockin	2014-06-25 04:09:22	MEMBER	"Whoah, uhh, shouldn't we decide where logs should actually be going before
we commit this?  Right now they get written to /tmp, which seems like a
less than ideal answer.

I suggest we back this out post haste :)

On Tue, Jun 24, 2014 at 9:01 PM, Daniel Smith notifications@github.com
wrote:

> Thanks for the change!
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/221#issuecomment-47058366
> .
"
391	193	rafael	2014-06-25 04:35:11	CONTRIBUTOR	"+1
"
392	243	lavalamp	2014-06-25 05:13:57	MEMBER	"Thanks for the fix!
"
393	244	brendandburns	2014-06-25 05:31:49	CONTRIBUTOR	"Oops, this isn't ready for merge, I need to add a filter.  I'll do that tomorrow, and ping the PR when ready.
"
394	222	smarterclayton	2014-06-25 15:12:13	CONTRIBUTOR	"Should it be the interface, or an abstract network interface type?  Are interfaces better represented by ""internal, external, fast, private"" or by ""eth3"" which might be different on every minion?  Is it reasonable to expect all minions to have the same named interface?
"
395	222	thockin	2014-06-25 16:28:05	MEMBER	"There's a natural tension between people who want to schedule with some
understanding of the underlying hardware and people who want to schedule
abstractly.

IMO we should encourage the latter and discourage the former, but provide
escape hatches to do more ""advanced"" stuff.  I don't think it will be the
normal case for people to want to bind to specific interfaces of the
machine.  It should be possible, but I don't think it is the path with the
smoothest surface.

I acknowledge that this note was not helpful in deciding how to do it. :)

On Wed, Jun 25, 2014 at 8:12 AM, Clayton Coleman notifications@github.com
wrote:

> Should it be the interface, or an abstract network interface type? Are
> interfaces better represented by ""internal, external, fast, private"" or by
> ""eth3"" which might be different on every minion? Is it reasonable to expect
> all minions to have the same named interface?
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/222#issuecomment-47114507
> .
"
396	222	smarterclayton	2014-06-25 16:56:36	CONTRIBUTOR	":)  I strongly agree on the ""encourage abstraction, allow escape hatch"" duality here.

Some concrete use cases we've discussed in relation to selecting interfaces
- Deploy a replicated service that wants to use a separate, ""internal/private"" network interface to route cluster traffic.  I.e. mongodb in replica set mode, where the containers in the replica set want to communicate over a private network configured on a specific interface
- As an app deployer, allow choosing to expose ports on an interface that is internal to the network/security group that minions are on, or alternatively expose it on an interface that is reachable by a larger network (other Kube clusters, intranet, internet)
- As a Kube cluster admin, configure a loadbalancer/proxy (Kube proxy, HAProxy functioning in the same role) that is on a specific, reserved device on a set of machines.  If the interface isn't available, that's likely to be an error condition (an input to scheduling)

It's also worth distinguishing between the cluster-owner and tenant-user roles - the former has access to make cluster wide decisions and configure minions in very specific ways, the latter only has access to options exposed by the Kubernetes API
"
397	246	jjhuff	2014-06-25 17:46:49	CONTRIBUTOR	"The kubelet already supports reading config from a file, does that work?
"
398	246	lavalamp	2014-06-25 17:50:19	MEMBER	"Yeah, I think the work to do here is:

1) Finish up jbeda's work to dockerize all our components.
2) Adjust our startup scripts to use kubelet to launch apiserver/replication-controller via the config file.
"
399	127	smarterclayton	2014-06-25 19:12:20	CONTRIBUTOR	"How would an API client know that the individual container ""succeeded""? (for a definition of success)
"
400	247	lavalamp	2014-06-25 19:27:05	MEMBER	"Thanks for the change!

It may be time to remove localkube. We can always find it in the history if we decide to move to a single binary system-- localkube would be a good starting point for that.
"
401	247	jjhuff	2014-06-25 19:42:50	CONTRIBUTOR	"Cool. I can do the removal PR later today.
On Jun 25, 2014 12:27 PM, ""Daniel Smith"" notifications@github.com wrote:

> Thanks for the change!
> 
> It may be time to remove localkube. We can always find it in the history
> if we decide to move to a single binary system-- localkube would be a good
> starting point for that.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/247#issuecomment-47146901
> .
"
402	127	bgrant0607	2014-06-25 19:46:39	MEMBER	"@smarterclayton If you're asking about how Kubernetes will detect successful termination, we need machine-friendly, actionable status codes from Docker and from libcontainer (https://github.com/GoogleCloudPlatform/kubernetes/issues/137). Every process management and workflow orchestration system in the universe is going to need that. Normal termination with exit code 0 should indicate success.

If you're asking how Kubernetes's clients would detect termination, they could poll the currentState of the pod. We don't really have per-container status information there yet -- we'd need to add that. A Watch API would be better than polling -- that's worth an issue of its own. We could also provide a special status communication mechanism to containers and/or to event hooks (e.g., tmpfs file or environment variable). On top of these primitives, we could build a library and command-line operation to wait for termination and return the status.
"
403	127	aspyker	2014-06-25 19:47:51	NONE	"ran into this for Acme Air for case # 3 (run-once) for initial database loader processes
"
404	127	smarterclayton	2014-06-25 20:13:34	CONTRIBUTOR	"@bgrant0607 Probably fair to restate my question as whether you have a model in mind (based on previous experience at Google) that defines what you feel is a scalable and reliable mechanism for communicating fault and exit information to an API consumer - for instance, to implement run-once or run-at-least-once containers that are expected to exit and not restart as aspyker mentioned.

For instance, Mesos defines a protocol between master and slave that attempts to provide some level of guarantees for communicating the exit status, subject to the same limitations you noted above about not being truly deterministic.  That model assumes bidirectional communication between master and slave, which Kubernetes does not.

Agree that watch from client->slave or client->master->slave is better than polling, although it seems more difficult to scale the master when the number of watchers+tasks grows.  Do you see the master recording exit status for run-once containers in a central store, or that being a separate subsystem that could scale orthogonally to the api server / replication server and aggregate events generated by the minions?  I could imagine that transient failures of containers with a ""restart-always"" policy would be useful to know to an api consumer - to be able to see that container X restarted at time T1, T2, and T3.
"
405	127	bgrant0607	2014-06-25 23:43:09	MEMBER	"@smarterclayton First, I think the master should delegate the basic restart policy to the slave: always restart, restart on failure, never restart. The master should only handle cross-node restarts directly. And, yes, the master should pull status from the slaves and store it (#156), as well as periodically check their health (#193). As scaling demands grow, that responsibility could indeed be split out to a separate component or set of components.

Reason for last termination (#137), termination message from the application (#139), time of last termination, and number of terminations should be among the information collected. State of terminated containers/pods should be kept around on the slaves long enough for the master to observe it the vast majority of the time (e.g., 10x the normal polling interval, or 2x the period after which an unresponsive node would be considered failed, anyway; explicit decoupling of stop vs. delete would also enable the master to control deletion of observed terminated containers/pods), but the master would record unobserved terminations as having failed, ideally with as much specificity as possible about what happened (node was unresponsive, node failed, etc.). A monotonically increasing count of restarts could be converted to approximate recency, sliding-window counts, rates, and other useful information by continuous observers. A means of setting or resetting the count is sometimes useful, but non-essential. Termination events could also be buffered (in a bounded-sized ring buffer with an event sequence number) and streamed off the slave and logged for debugging, but shouldn't be necessary for correctness, since events could always be lost.

Reasons for system-initiated container stoppages (e.g., due to liveness probe failures - #66) should be explicitly recorded (#137), but can be treated as failures with respect to restart policy. User-initiated cancellation should override the restart policy, as should user-initiated restarts (#159).

With more comprehensive failure information from libcontainer and Docker we could distinguish container setup errors from later-stage execution failures, but if in doubt, the slave (and master) should be conservative about not restarting ""run once"" containers that may have started execution.

Containers should have unique identifiers so the system doesn't confuse different instances or incarnations (#199).

Overall system architecture for availability, scalability, fault tolerance, etc. should be discussed elsewhere.
"
406	251	lavalamp	2014-06-26 00:20:49	MEMBER	"I'm not on a mac at the moment so can't test, but does http://brew.sh/ allow you to install `htpasswd`?
"
407	251	dgageot	2014-06-26 00:22:47	NONE	"No. I'm investigating...
"
408	251	dgageot	2014-06-26 00:34:34	NONE	"Definitely not in homebrew
"
409	251	lavalamp	2014-06-26 00:38:43	MEMBER	"Was mentioned in IRC a while ago, but no resolution yet. Thanks for filing the issue.

https://botbot.me/freenode/google-containers/msg/16261657/
"
410	251	dgageot	2014-06-26 01:08:52	NONE	"I'm trying to use a [docker container](https://registry.hub.docker.com/u/dgageot/htpasswd/) as a workaround. Will tell you if it works. 
"
411	199	smarterclayton	2014-06-26 01:35:30	CONTRIBUTOR	"Was going to get familiar and try to fix this - sounds like the suggestion is to add ""ID string"" to Container, and fail PodRegistryStorage.Create() if Container.ID is empty?  Or should PodRegistryStorage.Create() populate unset DesiredState.Manifest.Containers[].ID that are empty?  Latter seems more flexible (server controls default UUID generation for clients)
"
412	251	dgageot	2014-06-26 01:43:24	NONE	"OK. It works. Here's the commit that uses docker to run htpasswd inside a docker https://github.com/dgageot/kubernetes/commit/231048477876623a02f4a5ffddda908612e8d281 

I'm not sure this is something that could be used by other than me. The user needs to have docker installed and my docker image has to be trusted.

Anyway it helped me!
"
413	199	thockin	2014-06-26 02:08:52	MEMBER	"I started in on some validation logic this morning.

It's not clear whether unique ID is something users should have to spec or
not.  I lean towards not.  That means the master (api server?) Has to
generate a uuid upon acceptance.  That uuid would have to flow down to
Kubelet.

Is that in the same vein as you were thinking?
On Jun 25, 2014 6:35 PM, ""Clayton Coleman"" notifications@github.com wrote:

> Was going to get familiar and try to fix this - sounds like the suggestion
> is to add ""ID string"" to Container, and fail PodRegistryStorage.Create() if
> Container.ID is empty? Or should PodRegistryStorage.Create() populate unset
> DesiredState.Manifest.Containers[].ID that are empty?
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/199#issuecomment-47179178
> .
"
414	199	smarterclayton	2014-06-26 02:31:17	CONTRIBUTOR	"As an API consumer I like not having to specify things that the server can do for me - having to generate a UUID on the command line to curl a new pod into existence feels wrong.  I started [here](https://github.com/smarterclayton/kubernetes/commit/984334f42dfcb470a6add005613330477108a146#diff-89731e89e31105da32e30576a5939fb6R149) but didn't pass down to kubelet yet.
"
415	199	vmarmol	2014-06-26 02:38:14	CONTRIBUTOR	"One thing to note with global unique identifier is static containers. Today
we setup cAdvisor as a static container and have no way to assign it a
unique ID globally.

On Wed, Jun 25, 2014 at 7:31 PM, Clayton Coleman notifications@github.com
wrote:

> As an API consumer I like not having to specify things that the server can
> do for me - having to generate a UUID on the command line to curl a new pod
> into existence feels wrong. I started here
> https://github.com/smarterclayton/kubernetes/commit/984334f42dfcb470a6add005613330477108a146#diff-89731e89e31105da32e30576a5939fb6R149
> but didn't pass down to kubelet yet.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/199#issuecomment-47181803
> .
"
416	199	smarterclayton	2014-06-26 02:43:09	CONTRIBUTOR	"Static as in ""defined on each host via a config file""?  Would it make sense for the Kubelet to auto assign a UUID for containers pulled from files based on the host MAC and the position in the file (or a SHA1 of the contents of the manifest plus the host MAC)?
"
417	199	thockin	2014-06-26 02:46:35	MEMBER	"Are you setting it up by config file?

Can we generate a new uuid when we write the config file, or do we want it
to be identical across machines (google prod style)?

We cod do something like: if Kubelet finds a config without a uuid, it will
assign it a uuid and log it.

Internally we go one step further and define ""master space"" where each
configuration originator can choose an ID and then manage ids within that
space, meaning the master doesn't have to generate a UUID at all just a
unique masterspace + pod name.

Do we need to go that far?  Uuids are notoriously human-hostile.
On Jun 25, 2014 7:38 PM, ""Victor Marmol"" notifications@github.com wrote:

> One thing to note with global unique identifier is static containers.
> Today
> we setup cAdvisor as a static container and have no way to assign it a
> unique ID globally.
> 
> On Wed, Jun 25, 2014 at 7:31 PM, Clayton Coleman notifications@github.com
> 
> wrote:
> 
> > As an API consumer I like not having to specify things that the server
> > can
> > do for me - having to generate a UUID on the command line to curl a new
> > pod
> > into existence feels wrong. I started here
> > <
> > https://github.com/smarterclayton/kubernetes/commit/984334f42dfcb470a6add005613330477108a146#diff-89731e89e31105da32e30576a5939fb6R149>
> > 
> > but didn't pass down to kubelet yet.
> > 
> > ## 
> > 
> > Reply to this email directly or view it on GitHub
> > <
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/199#issuecomment-47181803>
> > 
> > .
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/199#issuecomment-47182140
> .
"
418	199	vmarmol	2014-06-26 02:48:20	CONTRIBUTOR	"Yes, it is a static file we leave on the machine. I don't think we have the
ability to assign it a unique ID (outside of running some custom init
script). Having the Kubelet assign it a UUID seems reasonable to me.

On Wed, Jun 25, 2014 at 7:43 PM, Clayton Coleman notifications@github.com
wrote:

> Static as in ""defined on each host via a config file""? Would it make sense
> for the Kubelet to auto assign a UUID for containers pulled from files
> based on the host MAC and the position in the file (or a SHA1 of the
> contents of the manifest plus the host MAC)?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/199#issuecomment-47182360
> .
"
419	199	thockin	2014-06-26 02:50:45	MEMBER	"To be clearer on masterspace.  Each new pod gets fields such as:

Masterspace: kubernetes.google.com
Name: ""a name unique within this masterspace"".  I am sultaneously trying to
argue that new should be DNS compatible, which lits them to 64 lowercase
alphanums, and gives names semantic meaning.  Making then bad as unique
IDs.  Need to think more about this...
On Jun 25, 2014 7:46 PM, ""Tim Hockin"" thockin@google.com wrote:

> Are you setting it up by config file?
> 
> Can we generate a new uuid when we write the config file, or do we want it
> to be identical across machines (google prod style)?
> 
> We cod do something like: if Kubelet finds a config without a uuid, it
> will assign it a uuid and log it.
> 
> Internally we go one step further and define ""master space"" where each
> configuration originator can choose an ID and then manage ids within that
> space, meaning the master doesn't have to generate a UUID at all just a
> unique masterspace + pod name.
> 
> Do we need to go that far?  Uuids are notoriously human-hostile.
> On Jun 25, 2014 7:38 PM, ""Victor Marmol"" notifications@github.com wrote:
> 
> > One thing to note with global unique identifier is static containers.
> > Today
> > we setup cAdvisor as a static container and have no way to assign it a
> > unique ID globally.
> > 
> > On Wed, Jun 25, 2014 at 7:31 PM, Clayton Coleman <
> > notifications@github.com>
> > wrote:
> > 
> > > As an API consumer I like not having to specify things that the server
> > > can
> > > do for me - having to generate a UUID on the command line to curl a new
> > > pod
> > > into existence feels wrong. I started here
> > > <
> > > https://github.com/smarterclayton/kubernetes/commit/984334f42dfcb470a6add005613330477108a146#diff-89731e89e31105da32e30576a5939fb6R149>
> > > 
> > > but didn't pass down to kubelet yet.
> > > 
> > > ## 
> > > 
> > > Reply to this email directly or view it on GitHub
> > > <
> > > https://github.com/GoogleCloudPlatform/kubernetes/issues/199#issuecomment-47181803>
> > > 
> > > .
> > 
> > ## 
> > 
> > Reply to this email directly or view it on GitHub
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/199#issuecomment-47182140
> > .
"
420	199	lavalamp	2014-06-26 03:09:05	MEMBER	"I want the container names from the different sources to have different namespaces, so there won't be any collisions. E.g., kubelet prepends/appends "".etcd"" (or something) to etcd-sourced containers, "".cfg"" to containers from the config file, "".http"" to containers from the manifest url, etc. Then it is up to each source to stay unique.

Api server can stay unique via uuid or counting up. Config files & manifest url stay unique by humans not screwing up; kubelet rejects them otherwise.

This has the nice effect that the container name produced from a config file is predictable without having to do a lookup. This would be good for our own container vm image and anything like it.
"
421	253	lavalamp	2014-06-26 03:16:06	MEMBER	"@jjhuff is working on fixing up the kubelet's container naming scheme, which is the other half of this. See: https://github.com/jjhuff/kubernetes/commit/ff131a93170e96733fc63185780bf7de18045bc2
"
422	199	thockin	2014-06-26 03:50:38	MEMBER	"This started structured and turned into a stream0-of-consiousness, sorry.

I agree that unique names are desirable, but I'm not sure ""etcd"" and
""httpd"" are sufficient.

The reason we have masterspace internally is because we also need to attach
metadata about the master that created a pod (e.g. which cluster it thought
it was in, what magic number it had (generation number) and so on).

Here's what I think I have convinced myself of, so far.

1) All pods have a required name string, which is human-friendly and DNS
friendly (probably rfc1035 subdomain rather than label).

2) All pods which are running have a uuid (suggest RFC4122 but could be
something else).  This of this as a cluster-wide PID.  It has no semantic
meaning and changes whenever a pod is started on a new host.

3) The kubelet API includes this uuid.  If the uuid field is not specified,
the kubelet will assign a new UUID.

This _does not_ have the property that Daniel wants - predictable container
names.  The problem is that you putting semantic meaning into the unique
ID.  There's a reason that databse best practices involve surrogate keys
and that UNIX syscalls operate on PIDs rather than command names.  Consider
what happens if we get a phantom instance on a split network - both pods
end up with the same name - not unique any more.

That said, I could maybe be convinced.  If we put the rule that the pod
name had to be unique within a masterspace, we could sort of punt the
problem a bit, for now.

E.g.

Pod {
  masterspace = ""k8s.mydomain.com""
  id = ""id8675309.tims-pod""
  containers [
    {
      name = ""apache""

...would be created with container name
apache.id8675309.tims-pod.k8s.mydomain.com

If the apiserver did not care about phantoms, it could leave off the
id8675309 noise.  for Google people, this should look very familiar.

I still have a vague foreboding about making the uniqueness be the master's
problem, but it would get rid of the need for opaque UUIDs.  Or would it?
 We need to handle static pods (e.g. cAdvisor).  If they are allowed to
collide, then some hyopthetical cluster-data aggregator can not use ID as
primary key, and we will need to disambiguate queries by hostname.  Blech.

What if the rule is that masterspace is optional.  If not specified,
kubelet will make something up derived from the source.  So cAdvsor would
say:

Pod {
  #no masterspace
  id = ""cadvisor""
  containers [
    {
      name = ""cadvisor""

...would be created with container name cadvisor.cadvisor.file.<host_fqdn>

This still sort of sucks in that you can't aggregate by masterspace, e.g.
""give me a list of all containers in the cadvisor masterspace"".  But maybe
that's better served by labels anyway.  Yes, I think so.

Thoughts?  I could go wither way (UUIDs or masterspace + unique name).  Or
does someone have other ideas?  I feel like this got complicated pretty
fast.

Tim

On Wed, Jun 25, 2014 at 8:09 PM, Daniel Smith notifications@github.com
wrote:

> I want the container names from the different sources to have different
> namespaces, so there won't be any collisions. E.g., kubelet
> prepends/appends "".etcd"" (or something) to etcd-sourced containers, "".cfg""
> to containers from the config file, "".http"" to containers from the manifest
> url, etc. Then it is up to each source to stay unique.
> 
> Api server can stay unique via uuid or counting up. Config files &
> manifest url stay unique by humans not screwing up; kubelet rejects them
> otherwise.
> 
> This has the nice effect that the container name produced from a config
> file is predictable without having to do a lookup. This would be good for
> our own container vm image and anything like it.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/199#issuecomment-47183465
> .
"
423	245	thockin	2014-06-26 04:12:07	MEMBER	"Can we detail places where we're not currently compatible?
"
424	199	lavalamp	2014-06-26 05:12:47	MEMBER	"Just to be clear, I only think that the property of predictable names is desirable for containers that come from the manifest url and maybe the container, because without that property, `docker ps` is unhelpful. But you've got me somewhat convinced that maybe I shouldn't care so much about that.

I don't know if we want to wait to solve naming in general before we accept a better solution than what we do currently.
"
425	199	thockin	2014-06-26 05:20:47	MEMBER	"I could put my weight behind either approach.  The problem with UUIDs (be
they RFC4122 or SHA or whatever) is that they suck for humans.  The problem
with !UUIDs is that we have to count on the ""master"" to get uniqueness
right.

On Wed, Jun 25, 2014 at 10:12 PM, Daniel Smith notifications@github.com
wrote:

> Just to be clear, I only think that the property of predictable names is
> desirable for containers that come from the manifest url and maybe the
> container, because without that property, docker ps is unhelpful. But
> you've got me somewhat convinced that maybe I shouldn't care so much about
> that.
> 
> I don't know if we want to wait to solve naming in general before we
> accept a better solution than what we do currently.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/199#issuecomment-47188351
> .
"
426	199	jjhuff	2014-06-26 05:32:27	CONTRIBUTOR	"""I feel like this got complicated pretty fast."" Yup:)

[I think I've read & parsed all of the comments, but I could be mistaken]

I like the idea of DNS-styled hierarchical namespaces. I'm not super worried about multiple masters scheduling over the same cluster of minions, but it does make multi-tenant masters easier (i.e. a provider is running a master, and customers run their minions). It's also something that people are used to.

I think it's also reasonable to require that users produce unique names for managing in the system. 

Machine-generated unique IDs are more useful for running containers (i.e. a global PID). Having this be globally unique is _really_ handy for things like log aggregation, etc. Why not always have the kubelet generate that? The master can learn it when it lists the running containers. 

It's worth noting that the only restriction of docker names is that they are unique to the host. We can still encode useful human data (manifest+container names) in addition to the unique ID into the name...even if we only parse out the unique ID!

FWIW, I'm just polishing my ID cleanups to remove dead (or dieing) code. My goal was to normalize on docker IDs for many/most things inside the kubelet. I should have it out for review tomorrow.
"
427	199	jjhuff	2014-06-26 06:07:19	CONTRIBUTOR	"Also, clarifying some terminology might be handy.  Here's how I think of things (could easily be wrong!):
- `ContainerManifest` is a collection of `Containers`. 
- `Pods` are an instance of a `ContainerManifest`. They can be running (or not). They are assigned to a host.

Things that confuse me:
- `ContainerManifests` have an Id. `Containers` have a name. `Pods` have an Id. Seems like the manifest should only have a name (but this gets to the discussion above about names vs ids).
- `ReplicationController` has a `PodTemplate` which really just wraps `ContainerManifest`. Seems like overkill.
"
428	199	thockin	2014-06-26 06:22:03	MEMBER	"On Wed, Jun 25, 2014 at 10:32 PM, Justin Huff notifications@github.com wrote:

> ""I feel like this got complicated pretty fast."" Yup:)
> 
> [I think I've read & parsed all of the comments, but I could be mistaken]
> 
> I like the idea of DNS-styled hierarchical namespaces. I'm not super worried about multiple masters scheduling over the same cluster of minions, but it does make multi-tenant masters easier (i.e. a provider is running a master, and customers run their minions). It's also something that people are used to.

I'm more worried about ""masters"" that are config files crafted by
humans without coordination.  Those should not accidentally collide :)

> I think it's also reasonable to require that users produce unique names for managing in the system.

Here ""users"" == apiserver?

> Machine-generated unique IDs are more useful for running containers (i.e. a global PID). Having this be globally unique is really handy for things like log aggregation, etc. Why not always have the kubelet generate that? The master can learn it when it lists the running containers.

Yeah, this is doable (and is in fact closer to what we do internally)

> It's worth noting that the only restriction of docker names is that they are unique to the host. We can still encode useful human data (manifest+container names) in addition to the unique ID into the name...even if we only parse out the unique ID!

I think it is useful, but not critical that docker-reported names be
human-friendly.

> FWIW, I'm just polishing my ID cleanups to remove dead (or dieing) code. My goal was to normalize on docker IDs for many/most things inside the kubelet. I should have it out for review tomorrow.

I'm keenly interested in this, and I'd like to stabilize it all ASAP.
Please do not merge this change until I have had time to feedback on
it.  Caveat: I am part-time the rest of this week.

> ## 
> 
> Reply to this email directly or view it on GitHub.
"
429	199	thockin	2014-06-26 06:27:09	MEMBER	"On Wed, Jun 25, 2014 at 11:07 PM, Justin Huff notifications@github.com wrote:

> Also, clarifying some terminology might be handy. Here's how I think of things (could easily be wrong!):
> 
> ContainerManifest is a collection of Containers.
> Pods are an instance of a ContainerManifest. They can be running (or not). They are assigned to a host.

The names as defined in pkg/api are somewhat confusing.  It all
depends on perspective.  We really have two APIs here (user -> master;
master -> kubelet), and we're cramming the spec into one file.  We
should consider whether we really want to do that.

From Kubelet's POV ContainerManifest == Pod.

> Things that confuse me:
> 
> ContainerManifests have an Id. Containers have a name. Pods have an Id. Seems like the manifest should only have a name (but this gets to the discussion above about names vs ids).

Yes, let's settle the discussion about UUIDs vs Names before we rename
things.  and then lets rename things.

> ReplicationController has a PodTemplate which really just wraps ContainerManifest. Seems like overkill.

I think this was to leave room for growth.  Kubelet should do the same
and define a Pod type that has an api.ContainerManifest and maybe the
UUID (if we do that).

BTW: I have a change pending (not sent yet) to do validation of a
ContainerManifest - so it will reject manifests that, for example, do
not have an ID.  The core of it is done, but it needs a test and has
some open questions.  Hope to have it out tomorrow.

> ## 
> 
> Reply to this email directly or view it on GitHub.
"
430	199	bgrant0607	2014-06-26 06:54:09	MEMBER	"Whew! This is long. This issue started with no context about what we're trying to do.

Starting with the end: Yes, we should clean up the identifiers. Currently, every object/resource includes JSONBase, which has an ID (which probably should be Id). ContainerManifest also has an Id. Container has Name. Port has Name. I'll point out that Container and Port are not standalone resources right now. ContainerManifest is the way it is due to compatibility with the container-vm release.

What are we trying to do? I saw several things mentioned in this issue:
- Human-friendly, memorable, semantically meaningful, short-ish references in operations
- Predictable references in operations and/or configuration files
- Unambiguous references in space, such as pods/containers created by multiple masters or by static configuration
- Unambiguous references in time, such as for logging
- Idempotent creation (#148) 
- Auto-generation of DNS names (#146)
- Identification of sets for aggregation

Both unique identifiers and human-friendly names have value. Human-friendly names can only be unique in space, not in time. We should use ""Id"" for the former and ""Name"" for the latter. Names could be used to ensure idempotent (at most once) creation, though a non-indexed resource value could be used for that, also. 

The replicationController would treat ""Name"" in the template as a prefix and would append relatively short random numbers for uniqueness. Static pods could be treated similarly.

Label selectors should be used for set formation / aggregation.

If users are providing Names, we could also use them for DNS (#146). I'd use ""domain"" rather than ""masterspace"".

Argh, my laptop needs to be rebooted...
"
431	199	bgrant0607	2014-06-26 08:01:27	MEMBER	"Continuing...

I agree we want a mechanism that permits unique id allocation by Kubelets and that doesn't require centralized and/or persistent state. I like UUIDs. There are the issues of ensuring unique MAC addresses in VMs and/or namespaces, and determinism for testing, but I think we've found ways around these issues. 

I've considered not having human-friendly names for pods before and just using labels instead. Labels are predictable and human-friendly, but don't require uniqueness, and don't require concatenating lots of identifying info together in order to ensure uniqueness, which users WILL do for names (and they'll want to parameterize them). They aren't short, though. Also, I guess part of the problem is that Docker doesn't support labels. We should push on that.

Idempotence could be ensured by a client-generated cookie, such as a fingerprint/hash or PR number. It wouldn't be required for static configs.

DNS names for instances aren't super-useful if they aren't predictable and DNS-like human-friendly names aren't that friendly if they are long. Short nicknames don't have to be predictable (so they could have a short uniquifying suffix) and don't even need to be semantically relevant -- just memorable (hence Docker's silly auto-generated names, I guess).

Services need predictable DNS names, OTOH, and ports need predictable names (for DNS SRV lookup or ENV vars or whatever), and pod-relative hostnames of containers should be predictable, so they can communicate with each other, though I don't know that they actually need to be FQDNs. Other types of services (e.g., master-elected services) and groups will need predictable DNS names, also.

Is the main motivation for names for pods to be consistent across all resource types? If so, I could buy into DNS-like hierarchical names for them. I'd use something like ""domain"" or ""namespace"" instead of ""masterspace"". 
"
432	199	smarterclayton	2014-06-26 14:26:39	CONTRIBUTOR	"Pod-relative hostnames and stable internal names are definitely valuable - and limiting ""name"" to rfc1035 subdomain has been extremely valuable in practice to us on OpenShift.  

> The replicationController would treat ""Name"" in the template as a prefix and would append relatively short random numbers for uniqueness. Static pods could be treated similarly.

Quasi-uniqueness I assume?

> Other types of services (e.g., master-elected services) and groups will need predictable DNS names, also.

How predictable?  As a concrete example, with something like Zookeeper you need the container to have an identifier/name that is stable across restarts / reschedules in a shared config (so not a pod ID).  I'd assumed you'd model this with a set of replication controllers (vs a shared controller) so you had 3 replication controllers with 1 item each, and you'd be able to either set an ENV per pod, or use the name in order to apply that.  If name changes over pod instances that rules out the reuse there.
"
433	199	lavalamp	2014-06-26 16:26:28	MEMBER	"One last thought from me; it occurs to me that docker already generates a long container id. Perhaps we can consider using that directly as our spatial/temporal unique identifier, and use this hypothetical dns-style solution as the friendly human name. We'd need to investigate just how unique docker's id's are, and we may not want to depend on docker for that, but it would reduce the number of IDs needed.

Also, as a footnote, if we end up with both pod.ID and Manifest.ID, IMO they should be the same identifier.
"
434	199	jjhuff	2014-06-26 16:37:47	CONTRIBUTOR	"> One last thought from me; it occurs to me that docker already generates a
> long container id. Perhaps we can consider using that directly as our
> spatial/temporal unique identifier, and use this hypothetical dns-style
> solution as the friendly human name. We'd need to investigate just how
> unique docker's id's are, and we may not want to depend on docker for that,
> but it would reduce the number of IDs needed.
> 
> Yes, I was starting to think the same thing.
> 
> Also, as a footnote, if we end up with both pod.ID and Manifest.ID, IMO
> they should be the same identifier.
> 
> My only concern here is that starts to feel weird when a Manifest is used
> as part of a ReplicationController. I think. You end up with N pods and a
> Manifest each with ID fields.
"
435	199	lavalamp	2014-06-26 16:46:14	MEMBER	"For clarity, let me suggest the convention that ""name"" means a friendly mostly human readable string, possibly in the style of dns names, and that ""id"" means an opaque, machine generated identifier, guaranteed unique at some level of resolution. Maybe everyone except for me is already using this convention. :) But I want to talk about ids and names in general without referring to a particular implementation.

> My only concern here is that starts to feel weird when a Manifest is used as part of a ReplicationController. I think. You end up with N pods and a Manifest each with ID fields.

I think, in that case, the rep. controller itself has a name, which it can use to make names for the pods it creates (prepend/append indices or something). IDs for the pods could still be generated by the apisever or wherever we decide they need to be generated. I was trying to say that since there's a 1:1 relationship between pods and manifests, we shouldn't make different IDs for each.
"
436	233	brendandburns	2014-06-26 17:26:06	CONTRIBUTOR	"ok, migrated to http.FileServer
"
437	253	jjhuff	2014-06-26 18:55:08	CONTRIBUTOR	"My internal Id cleanup is in #254 
"
438	199	thockin	2014-06-26 19:00:51	MEMBER	"Trying to collect thoughts and ideas into a proposal.  It's tricky
considering it from all angles (users, apiserver, replication controllers,
kubelet).  My background is node-centric, so I am counting on people to
smack me if I say something dumb for higher-levels :)

I have to run out right now, but I'm hoping we can distill the discussion
into a design O(soon).

NB: We spec names as RFC 1035 compatible, though we might extend that to
allow purely numeric tokens (e.g. 123.something.com) but I forget which RFC
that is.  Docker names allow underscores, which kubelet reserves for use in
infrastructure containers.

From kubelet's point of view:

1) All pods have a namespace string, which is human-friendly and DNS
friendly (rfc1035 subdomain fragment).  For example: ""k8s.mydomain.com"".
 This is used to indicate the provenance of the pod.  If the namespace is
not specified when creating a pod, kubelet will assign a namespace derived
from the source.  For example, a file ""/etc/k8s/cadvsor.pod"" on host ""
xyz123.mydomain.com"" might get masterspace ""
file-f5sxiyzpnm4hgl3dmfshm2ltn5zc44dpmqfa.xyz123.mydomain.com"" (the big
mess is a base32 encoding of the path).

2) All pods have a name string, which is human-friendly and DNS friendly
(rfc1035 subdomain fragment).  For example: ""id8675309.myjob"".  These names
are typically NOT user-provided, but are generated from infrastructure.
 For example, if the user asked for a job named ""myjob"" the apiserver must
uniquify that into a pod name.

3) The namespace + name pair is assumed to be unique.  This provides simple
idempotency, for example when re-reading a config file.

Open: Do we need UUIDs at all?  For what purpose?  The only argument I can
come up with is to protect against accidentally non-unique names at cluster
scope (e.g. in a giant database of namespace, name, uuid, stats) you could
disambiguate colliding namespace+names pairs with uuid.  Other
justifications?

From the apiserver's point of view:

1) The apiserver has a configured namespace.

2) All incoming pods have a name.  I don't know what the formatting and
uniqueness rules are here.

3) Upon acceptance, a pod is assigned a unique ID of some sort.

4) Upon assignment to a minion, the name and unique ID become the pod name.

Open: Should the unique ID persist if the pod is moved to a new minion?
 Maybe we need two IDs - one that sticks across moves and one that does not?

On Thu, Jun 26, 2014 at 9:46 AM, Daniel Smith notifications@github.com
wrote:

> For clarity, let me suggest the convention that ""name"" means a friendly
> mostly human readable string, possibly in the style of dns names, and that
> ""id"" means an opaque, machine generated identifier, guaranteed unique at
> some level of resolution. Maybe everyone except for me is already using
> this convention. :) But I want to talk about ids and names in general
> without referring to a particular implementation.
> 
> My only concern here is that starts to feel weird when a Manifest is used
> as part of a ReplicationController. I think. You end up with N pods and a
> Manifest each with ID fields.
> 
> I think, in that case, the rep. controller itself has a name, which it can
> use to make names for the pods it creates (prepend/append indices or
> something). IDs for the pods could still be generated by the apisever or
> wherever we decide they need to be generated. I was trying to say that
> since there's a 1:1 relationship between pods and manifests, we shouldn't
> make different IDs for each.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/199#issuecomment-47249802
> .
"
439	244	brendandburns	2014-06-26 19:01:33	CONTRIBUTOR	"Ok, this is ready for merge now.

PTAL
Thanks!
--brendan
"
440	244	lavalamp	2014-06-26 19:39:38	MEMBER	"Generally LGTM; one thing to consider, I'd like the GET minions/<minion> api call to be able to return the minion's external IP address. That may affect the types which a minion registry returns to the MinionStorage. Doesn't have to be addressed in this PR.

(Footnote: Our XXStorage/XXRegistry notation is unfortunate because the XXRegistry does the storage and the XXStorage is just the access layer for the apiserver. :( )
"
441	255	lavalamp	2014-06-26 19:44:27	MEMBER	"Travis is sleeping, I guess? Merging anyway, thanks for the change.
"
442	256	lavalamp	2014-06-26 19:54:09	MEMBER	"Thanks for the change! Can you sign the CLA so we can accept it? Instructions in CONTRIB.md. Thanks!
"
443	238	lavalamp	2014-06-26 19:59:06	MEMBER	"Needs a rebase due to your other PR that I merged. Also, cloudcfg is now kubecfg. Other than that, is this ready to merge?
"
444	254	lavalamp	2014-06-26 20:50:10	MEMBER	"Generally LGTM. Will wait for LGTM from Tim or Brendan before merging.
"
445	254	jjhuff	2014-06-26 21:00:46	CONTRIBUTOR	"Ok, fixes pushed. I haven't done the dockerId string->type conversion. I'm happy to tackle that in another PR, or this one if you guys want.
"
446	254	thockin	2014-06-26 21:07:16	MEMBER	"I have to run off - will take a detailed look this evening, if that is OK
"
447	256	lavalamp	2014-06-26 21:16:23	MEMBER	"I found your CLA. Thanks again!
"
448	199	bgrant0607	2014-06-26 21:17:30	MEMBER	"Numeric host names: http://tools.ietf.org/html/rfc1123#page-13
"
449	199	smarterclayton	2014-06-26 21:26:19	CONTRIBUTOR	"Regarding the open question: is a pod on a new minion the same as the old pod?  Is a move an action that is logically part of kubernetes, or is only ""remove"" and ""create new"" available?  If it's the former, it seems like the ID should be the same, if it's the later it seems like it should be different.

We have a use case for being able to move a pod from minion to minion (and any volumes that come with it) - however, since this requires the volume data on disk to be in a resting state, it's not an operation that seems to lend itself well to the replication controller (since the move is inherently stateful).  So I would expect this to be managed as an operation above the replication controller, vs part of it.

One namespace per apiserver can be limiting if the namespace is automatically bound to DNS and you're dealing with very large numbers of containers, but it doesn't sound unreasonable if you are using wildcard DNS.
"
450	148	smarterclayton	2014-06-26 21:44:57	CONTRIBUTOR	"Re #199, if this is a PUT would the caller have to specify the unique pod ID in order to preserve idempotency?
"
451	257	lavalamp	2014-06-26 21:45:59	MEMBER	"Looks like you need to increase the timeout in the integration test.
"
452	254	jjhuff	2014-06-26 21:47:04	CONTRIBUTOR	"Yup, that's fine.
"
453	238	brendandburns	2014-06-26 21:56:50	CONTRIBUTOR	"Please don't merge this yet.  Few more changes coming.

--brendan
"
454	258	smarterclayton	2014-06-26 22:06:48	CONTRIBUTOR	"Also appears https://github.com/GoogleCloudPlatform/kubernetes/blob/546600371ed26c2cd59265561517f790c03f8125/pkg/registry/endpoints.go#L55
"
455	258	lavalamp	2014-06-26 22:08:45	MEMBER	"Good catch, let me fix that one, too, before merging.
"
456	199	jjhuff	2014-06-26 22:11:48	CONTRIBUTOR	"@lavalamp BTW, it looks like docker IDs are more or less just 32 random bytes: https://github.com/dotcloud/docker/blob/master/utils/utils.go#L412

They enforce them to be unique per-machine: https://github.com/dotcloud/docker/blob/master/daemon/daemon.go#L470
"
457	199	bgrant0607	2014-06-26 22:35:06	MEMBER	"Unique ids disambiguate among multiple instances reusing the same name over
time, such as when polling termination status.

Beware of moving unique ids. Once we do that, they are no longer unique,
such as in the cases of live migration or even just preloading. If we do
move ids, we should have 2 ids, the movable one at the cluster level and a
non-movable one at the node level, and the movable ids would correspond to
zero or more of the non-movable ids.
"
458	148	bgrant0607	2014-06-26 22:50:14	MEMBER	"At the master level, I think the direction we're headed is similar to what we do internally: user-provided space-unique human-friendly names and system-generated spacetime-unique numerical ids. The caller would specify the name. Duplicate creations would be rejected with an ALREADY_EXISTS error.

Since nodes are both used directly by users and as slaves, we probably want similar user-oriented behavior, but the master would auto-generate names by combining the user-assigned name and master-generated id, as proposed by @thockin in #199. 
"
459	258	lavalamp	2014-06-26 23:22:20	MEMBER	"OK, should be good to go. IPv6 is hard enough without doing broken concatenations :)
"
460	234	lavalamp	2014-06-26 23:50:21	MEMBER	"I believe my change #249 will have fixed this as a side effect.
"
461	258	lavalamp	2014-06-26 23:58:09	MEMBER	"Hm, that was an interesting flake. Added a small change to make the test fail cleaner if that happens again.
"
462	261	lavalamp	2014-06-27 00:00:51	MEMBER	"Thanks for the change! Can you sign our CLA, as in the CONTRIB.md file, and tell me what name to look for? Thanks!
"
463	261	meirf	2014-06-27 00:06:00	CONTRIBUTOR	"Mark Fischer

Thanks!!
On Jun 26, 2014 8:01 PM, ""Daniel Smith"" notifications@github.com wrote:

> Thanks for the change! Can you sign our CLA, as in the CONTRIB.md file,
> and tell me what name to look for? Thanks!
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/261#issuecomment-47294645
> .
"
464	234	dchen1107	2014-06-27 00:20:49	MEMBER	"Yes, I noticed it and verified. The issue can be closed.

On Thu, Jun 26, 2014 at 4:50 PM, Daniel Smith notifications@github.com
wrote:

> I believe my change #249
> https://github.com/GoogleCloudPlatform/kubernetes/pull/249 will have
> fixed this as a side effect.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/234#issuecomment-47294023
> .
"
465	262	lavalamp	2014-06-27 01:40:12	MEMBER	"Just so I understand things correctly, you're fixing this by watching a smaller portion of the tree? That's good and we should definitely do it, but if we kill/restart on an unrelated change, we should fix that, too...
"
466	262	jjhuff	2014-06-27 01:55:53	CONTRIBUTOR	"Yes to watching a smaller part of the tree. Watching too much was the source of the kill/restart behavior. The master does two things when creating a new pod on a kubelet:
1. Set /register/hosts/<host>/pod/<id> to be the pod's json blob
2. Write /register/hosts/<host>/kubelet
3. Add the new manifest (extracted from the pod) to the list from 2
4. Write back to  /register/hosts/<host>/kubelet
FWIW, I don't think that 2-4 is safe.

The killing was because the kubelet would pickup the change under /pod/, parse it, and notify SyncManifests, which assumes that it's always being notified with the full set of manifests rather than just deltas.
"
467	262	brendandburns	2014-06-27 02:00:14	CONTRIBUTOR	"fwiw, I think that Justin meant for step 2 to be ""read /register/hosts/<machine>/kublet"" rather than write.

But yes, you're right, its not safe.  We need to add a lock in there to prevent read/update/write races.

I'll merge this, since it fixes the probem, but we should do more in the master as well.
"
468	238	brendandburns	2014-06-27 02:02:07	CONTRIBUTOR	"Ok, fixed, this is ready to merge.

Thanks!
--brendan
"
469	262	jjhuff	2014-06-27 02:03:46	CONTRIBUTOR	"Yup, that's what I meant.
I'll open an issue for the master side of things.
"
470	262	lavalamp	2014-06-27 02:04:41	MEMBER	"LGTM.
"
471	263	jjhuff	2014-06-27 02:10:04	CONTRIBUTOR	"Beat me to it.

I'm not a fan of B w/ lock when we have etcd available. 
"
472	257	lavalamp	2014-06-27 02:15:19	MEMBER	"LOL-- I think my test is broken if the random number at the end of the docker name is < 0x1fffffff
"
473	264	lavalamp	2014-06-27 02:37:14	MEMBER	"Thanks!
"
474	257	brendandburns	2014-06-27 02:55:58	CONTRIBUTOR	"Ok, with padding in place, Travis now appears to be a happy camper...
"
475	263	brendandburns	2014-06-27 03:00:05	CONTRIBUTOR	"Ok, I'm going to take a stab at implementing this w/ etcd semantics
"
476	265	lavalamp	2014-06-27 03:41:45	MEMBER	"How difficult is it to write a test that would expose any racy behavior?
"
477	265	brendandburns	2014-06-27 03:43:28	CONTRIBUTOR	"Yeah, I was thinking about that.  You could probably do it by adding some synchronization logic to the fake etcd client, which caused the write to sleep until you performed the other write.

I kind of feel like I should do it, because this is tricky code to get right.  At the same time, its going to be a total pain in the a*\* to write.  Maybe we can merge this with a TODO to write a test :P
"
478	265	lavalamp	2014-06-27 03:47:32	MEMBER	"I'm OK with a TODO. I wouldn't mind writing such a test tomorrow.
"
479	199	thockin	2014-06-27 04:04:47	MEMBER	"OK, collecting thoughts again.  If this is acceptable, I (or Clayton) can
write up a short .md file covering identifiers.  There are still a few
FIXME comments in here.  Please tell me if I am capturing any of this
incorrectly.

Global: We spec most names as RFC 1035/1123 compatible.  Docker allows
container names to include underscores, which we reserves for use by
infrastructure.

From kubelet's point of view:

1) All pods have a namespace string, which is human-friendly and DNS
friendly (an rfc1035/1132 subdomain).  For example: ""k8s.mydomain.com"".
 This is used to indicate the provenance of the pod.  If the namespace is
not specified when creating a pod, kubelet will assign a namespace derived
from the source of the pod.  For example, a file ""/etc/k8s/cadvisor.pod"" on
host ""xyz123.mydomain.com"" might get masterspace ""
file-f5sxiyzpnm4hgl3dmfshm2ltn5zc44dpmqfa.xyz123.mydomain.com"" (the big
mess is a base32 encoding of the path).  (FIXME: do we need this if we have
a UUID as spec'ed below?)

2) All pods have a name string, which is human-friendly and DNS friendly
(an rfc1035/1132 subdomain fragment).  For example: ""8675309.myjob"".  These
names are typically NOT user-provided, but are generated by infrastructure.
 For example, if the user asked for a job named ""myjob"" the apiserver must
uniquify that into a pod name.

3) The namespace + name pair is assumed to be unique.  This provides simple
idempotency, for example when re-reading a config file.

4) When starting an instance of a pod for the first time (i.e. not
restarting an existing pod), kubelet will assign an rfc4122 compatible UUID
to the pod.  This provides an ID that is guaranteed unique across time and
space.  If the pod is stopped and an identical pod (same namespace + name)
is started, a new UUID will be assigned.

5) Kubelet will use the aforementioned identifiers to produce unique
container names, for example
""8675309.myjob.k8s.mydomain.com_7c9fc7d1-5aac-46f5-b76f-b0d8d0effe2a"".

NB: The UUID is per-pod, not to be confused with Docker's own container IDs
which are per-container.

From the apiserver's point of view:

1) The apiserver has a configured namespace, for example ""k8s.mydomain.com"".

2) All incoming pods have a name assigned by the originator of the pod (be
they human users or other infrastructure).  Pod names must be unique in
space, but not time.

3) Upon acceptance, a pod is assigned a unique ID (FIXME: just spec it as
rfc4122 again?  could maybe be simpler here?).  This provides an easy way
to disambiguate successive pods with the same name.  This ID will persist
for the lifetime of the pod, across restarts and moves.  (FIXME: is this
really needed or is name good enough?)

4) Upon assignment to a minion, the name and unique ID become the pod name.

5) The namespace + name together must be less no more than 255 characters
long.

If DNS service is to be configured automatically (not a feature yet, but
has been discussed), the pod namespace + name will already be DNS compliant.

> On Thu, Jun 26, 2014 at 9:46 AM, Daniel Smith notifications@github.com
> wrote:
> 
> > For clarity, let me suggest the convention that ""name"" means a friendly
> > mostly human readable string, possibly in the style of dns names, and that
> > ""id"" means an opaque, machine generated identifier, guaranteed unique at
> > some level of resolution. Maybe everyone except for me is already using
> > this convention. :) But I want to talk about ids and names in general
> > without referring to a particular implementation.
> > 
> > My only concern here is that starts to feel weird when a Manifest is
> > used as part of a ReplicationController. I think. You end up with N pods
> > and a Manifest each with ID fields.
> > 
> > I think, in that case, the rep. controller itself has a name, which it
> > can use to make names for the pods it creates (prepend/append indices or
> > something). IDs for the pods could still be generated by the apisever or
> > wherever we decide they need to be generated. I was trying to say that
> > since there's a 1:1 relationship between pods and manifests, we shouldn't
> > make different IDs for each.
> > 
> > ## 
> > 
> > Reply to this email directly or view it on GitHub.

On Thu, Jun 26, 2014 at 3:35 PM, bgrant0607 notifications@github.com
wrote:

> Unique ids disambiguate among multiple instances reusing the same name over
> time, such as when polling termination status.
> 
> Beware of moving unique ids. Once we do that, they are no longer unique,
> such as in the cases of live migration or even just preloading. If we do
> move ids, we should have 2 ids, the movable one at the cluster level and a
> non-movable one at the node level, and the movable ids would correspond to
> zero or more of the non-movable ids.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/199#issuecomment-47288913
> .
"
480	199	jjhuff	2014-06-27 05:51:10	CONTRIBUTOR	"I think this seems like a good outline!
Two comments:
1. For config files, maybe use a hash of the file vs a base64 encoding of the path as the component of the namespace. Just seems more natural.
2. ""These names are typically NOT user-provided, but are generated by infrastructure."" I'd consider being more explicit about that behavior, even if it's as simple as a ""unique prefix will be added to the name"". This sorta touches on the whole id/name discussion....
"
481	199	bgrant0607	2014-06-27 13:29:46	MEMBER	"Another positive attribute of unique ids: they are typically more concise
than names, and are therefore more efficient to use in cross references in
logs.

Based on experience in Omega, I think there's no question that we need
unique ids.

Also, if you play around with Docker for a short while, you'll find a pile
of old objects that only have unique ids and can no longer be referenced by
name. How do you query them, resurrect them, or delete them once their
names have been recycled?

---

Is there a compelling reason to use a different kind of unique identifier
in the apiserver than in the kubelet? I don't think so.

We could probably shorten it (e.g., remove the locator) when we combine it
with the name to form the name for kubelet.
"
482	199	smarterclayton	2014-06-27 15:18:10	CONTRIBUTOR	"Definitely prefer hash of something in the config file (maybe hash of the JSON of the pod's manifest or something) for use in the namsepace component.
"
483	199	smarterclayton	2014-06-27 15:19:14	CONTRIBUTOR	"I'm happy to write this up as part of #253 (along with some more tests and the apiserver implications) once folks reach closure.
"
484	246	smarterclayton	2014-06-27 15:31:41	CONTRIBUTOR	"As a further item (possibly as a separate issue), being able to change code on your devenv and either one-line a command (hack/update-local) or have the image/source automatically reloaded would be valuable for reducing code-test loop time.  Probably the former though
"
485	199	smarterclayton	2014-06-27 15:39:33	CONTRIBUTOR	"One minor note regarding Docker ecosystem - ""name"" is currently being used in Docker for a lot of lightweight integrations (linking, sky dock dns, hostname in container).  A side effect of any generated name for a Docker container is that those lightweight integrations may become more difficult for end admins.  Is there a practical way to make the name appropriately unique on the minion without breaking those potential integrations (making the name a subdomain fragment by omitting '.' for instance)
"
486	199	brendanburns	2014-06-27 15:45:40	CONTRIBUTOR	"I would also like to see us drive labels all the way down into Docker. That
would enable us to abandon much of what we are encoding into the name today.

Brendan

On Fri, Jun 27, 2014, 8:39 AM, Clayton Coleman notifications@github.com
wrote:

> One minor note regarding Docker ecosystem - ""name"" is currently being used
> in Docker for a lot of lightweight integrations (linking, sky dock dns,
> hostname in container). A side effect of any generated name for a Docker
> container is that those lightweight integrations may become more difficult
> for end admins. Is there a practical way to make the name appropriately
> unique on the minion without breaking those potential integrations?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/199#issuecomment-47362231
> .
"
487	199	bgrant0607	2014-06-27 16:04:33	MEMBER	"+1 to what brendanburns@ wrote. Labels provide much cleaner solutions for
many use cases.
"
488	265	brendandburns	2014-06-27 16:13:06	CONTRIBUTOR	"Ok, updated.  I added a pretty extensive TODO with the recipe for the test in etcd_registry_test.go
"
489	266	brendandburns	2014-06-27 16:20:11	CONTRIBUTOR	"Thanks for the fix!
"
490	267	lavalamp	2014-06-27 16:52:57	MEMBER	"Check out the change @brendanburns made in #244, to figure out the list of minions via query to cloud provider. It's not wired in yet, but when it is it'll be pretty easy to completely customize this just by adding a custom cloud provider plugin.
"
491	158	brendandburns	2014-06-27 17:38:52	CONTRIBUTOR	"Closing this, I believe it is currently addressed in the readme.
"
492	1	brendandburns	2014-06-27 17:40:36	CONTRIBUTOR	"We're now at 58%, still not great, but I think we can close this issue, and track improving coverage as a general project goal.
"
493	267	smarterclayton	2014-06-27 17:53:57	CONTRIBUTOR	"Good to subdivide this into two use cases - I'm on a cloud, or I'm on a custom environment/non-cloud.   I don't think small non-cloud deployments would be able to write their own plugin effectively - they might prefer the simple API/CLI for add/remove minion.  However, that does limit the system's ability to release resources / ask for more resources (which probably is an abstraction separate from the minion API).
"
494	267	lavalamp	2014-06-27 17:59:29	MEMBER	"Yeah. Maybe we should write a generic non-cloud CloudProvider. We could make one that shelled out for various commands, for example, or just provide default useful actions.
"
495	267	smarterclayton	2014-06-27 18:43:48	CONTRIBUTOR	"Definitely - @mrunalp / @ironcladlou this may fit with the stuff you mentioned you were looking at
"
496	254	jjhuff	2014-06-27 19:11:55	CONTRIBUTOR	"Pushed.
"
497	254	thockin	2014-06-27 19:13:45	MEMBER	"I have to go pick up my kids, but I'll do a final pass during naptime. :)

Is there any way to see the delta between this and the last version I
looked at?

On Fri, Jun 27, 2014 at 12:12 PM, Justin Huff notifications@github.com
wrote:

> Pushed.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/254#issuecomment-47389092
> .
"
498	254	jjhuff	2014-06-27 19:24:41	CONTRIBUTOR	"Sounds good.
As for the last version. I don't think so. I like to keep the merged commits to a minimum, so I typically squash minor commits prior to pushing. Perhaps a better workflow would be to postpone the rebase -i until later?
"
499	188	smarterclayton	2014-06-27 20:09:45	CONTRIBUTOR	"Still gathering more feedback from customers and ops folks, but there's a lot of concern about being able to deploy the IP-per-container model outside of the big cloud providers.  Recording comments I'm hearing:
- In most non-cloud deployments this involves setting up the necessary network configuration to make IP allocation scalable
- This is expensive for operations teams
- In some organizations this may be a non-starter (maybe not many, but it's tough to get configured)
- If it's possible to hack this together slowly, it's less of a concern.  I.e. if you start with 1-2 minions and there's some simple scripts you can run to hack together a VPC for the containers, you might be ok up to 4-5 minions.  Then you need to switch to something more maintainable
- A lot of ops shops expect to be in control of things like DHCP, and are leery of deploying multiple DHCP servers just to configure a special VPC.  So they'd have to do the config to integrate the container use case into their existing DHCP which can be frustrating.
"
500	254	thockin	2014-06-27 20:45:09	MEMBER	"In hindsight, maybe so.  At least for non-trivial changes.  reviewing now.

On Fri, Jun 27, 2014 at 12:24 PM, Justin Huff notifications@github.com
wrote:

> Sounds good.
> As for the last version. I don't think so. I like to keep the merged
> commits to a minimum, so I typically squash minor commits prior to pushing.
> Perhaps a better workflow would be to postpone the rebase -i until later?
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/254#issuecomment-47390374
> .
"
501	254	thockin	2014-06-27 21:01:56	MEMBER	"I'm good with this change, let me know if you want to make the last tweak
or not.

On Fri, Jun 27, 2014 at 1:44 PM, Tim Hockin thockin@google.com wrote:

> In hindsight, maybe so.  At least for non-trivial changes.  reviewing now.
> 
> On Fri, Jun 27, 2014 at 12:24 PM, Justin Huff notifications@github.com
> wrote:
> 
> > Sounds good.
> > As for the last version. I don't think so. I like to keep the merged
> > commits to a minimum, so I typically squash minor commits prior to pushing.
> > Perhaps a better workflow would be to postpone the rebase -i until later?
> > 
> > ## 
> > 
> > Reply to this email directly or view it on GitHub
> > https://github.com/GoogleCloudPlatform/kubernetes/pull/254#issuecomment-47390374
> > .
"
502	254	jjhuff	2014-06-27 21:05:37	CONTRIBUTOR	"I just did the rename and rebased to master, so it should be good to go.

On Fri, Jun 27, 2014 at 2:02 PM, Tim Hockin notifications@github.com
wrote:

> I'm good with this change, let me know if you want to make the last tweak
> or not.
> 
> On Fri, Jun 27, 2014 at 1:44 PM, Tim Hockin thockin@google.com wrote:
> 
> > In hindsight, maybe so. At least for non-trivial changes. reviewing now.
> > 
> > On Fri, Jun 27, 2014 at 12:24 PM, Justin Huff notifications@github.com
> > wrote:
> > 
> > > Sounds good.
> > > As for the last version. I don't think so. I like to keep the merged
> > > commits to a minimum, so I typically squash minor commits prior to
> > > pushing.
> > > Perhaps a better workflow would be to postpone the rebase -i until
> > > later?
> > > 
> > > ## 
> > > 
> > > Reply to this email directly or view it on GitHub
> > > <
> > > https://github.com/GoogleCloudPlatform/kubernetes/pull/254#issuecomment-47390374
> > > 
> > > .
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/254#issuecomment-47399867
> .
"
503	254	thockin	2014-06-27 21:20:55	MEMBER	"Thanks.  Great cleanup.

On Fri, Jun 27, 2014 at 2:14 PM, Daniel Smith notifications@github.com
wrote:

> Merged #254 https://github.com/GoogleCloudPlatform/kubernetes/pull/254.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/254#event-136083841
> .
"
504	251	brendandburns	2014-06-27 21:57:54	CONTRIBUTOR	"Please see #271 for a more generic fix.
"
505	251	brendandburns	2014-06-27 22:07:53	CONTRIBUTOR	"Closed by #271 
"
506	269	lavalamp	2014-06-27 22:18:04	MEMBER	"Looks like an unstable map ordering issue from #254 caused this flake-- I'll fix.
"
507	127	bgrant0607	2014-06-27 22:28:55	MEMBER	"Two relevant Docker issues being discussed:
https://github.com/dotcloud/docker/issues/26    auto-restart processes
https://github.com/dotcloud/docker/issues/1311   production-ready process monitoring

The former is converging towards supporting restarts in Docker, with the 3 modes proposed here: always, on failure, never.

The latter has been debating the merits of ""docker exec"", which would not run the process under supervision of the Docker daemon. The motivation is to facilitate management by process managers such as systemd, supervisord, upstart, monit, runit, etc. This approach is attractive for a number of reasons.
"
508	127	smarterclayton	2014-06-27 22:38:10	CONTRIBUTOR	"While not called out in the latter issue, the blocking dependency is the ability for the daemon to continue to offer a consistent API for managing containers.  This was one of the inspirations for libswarm - allowing the daemon to connect to a process running in the container namespace in order to issue commands that affect the container as a unit (stop, stream logs, execute a new process).  The refactored Docker engine to allow that currently exists in a branch of Michael Crosby's, but libchan and swarm are not mature enough yet to provide that behavior.
"
509	269	lavalamp	2014-06-27 22:41:27	MEMBER	"Added TODO about racy set. Should be good to go now.
"
510	273	lavalamp	2014-06-27 23:11:53	MEMBER	"See #169... Just waiting on a CLA to show up
"
511	160	proppy	2014-06-27 23:12:19	CONTRIBUTOR	"I'd be great if the scheduler was also leverage this data for pod placement, should I file a separate bug?
"
512	273	brendandburns	2014-06-27 23:14:14	CONTRIBUTOR	"hrm, I like mine, it uses DOCKER_HOST too ;)
"
513	160	vmarmol	2014-06-27 23:14:34	CONTRIBUTOR	"I believe that is planned, but we can file an issue to track it
On Jun 27, 2014 4:12 PM, ""Johan Euphrosine"" notifications@github.com
wrote:

> I'd be great if the scheduler was also leverage this data for pod
> placement, should I file a separate bug?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/160#issuecomment-47409498
> .
"
514	260	bgrant0607	2014-06-27 23:24:25	MEMBER	"Note that we should probably also rename service to lbservice or somesuch to distinguish them from other types of services.
"
515	273	lavalamp	2014-06-27 23:47:22	MEMBER	"One thing-- can you modify the call to docker pull, so that it attempts to use the same endpont as the client?
"
516	273	brendandburns	2014-06-27 23:56:46	CONTRIBUTOR	"Done.
"
517	273	brendandburns	2014-06-28 00:01:52	CONTRIBUTOR	"Good catch!  Hard to catch this...

Fixed.
"
518	273	lavalamp	2014-06-28 00:10:23	MEMBER	"I only know that because I was trying to make my local kubelet talk to a docker running in a GCE instance via a forwarded port the other day. :)
"
519	169	brendandburns	2014-06-28 00:17:22	CONTRIBUTOR	"This is superceded by #273 (sorry!)  We're still waiting on the CLA, and a user ran into a similar problem.

We'd love to integrate the rest of your PR when it gets broken up some more, and the CLA comes through.

Thanks!
--brendan
"
520	279	brendandburns	2014-06-28 04:48:23	CONTRIBUTOR	"Closes #272 
"
521	263	brendandburns	2014-06-28 04:49:52	CONTRIBUTOR	"Closed by #265
"
522	199	thockin	2014-06-28 06:21:42	MEMBER	"On Thu, Jun 26, 2014 at 10:51 PM, Justin Huff notifications@github.com wrote:

> I think this seems like a good outline!
> Two comments:
> 1. For config files, maybe use a hash of the file vs a base64 encoding of the path as the component of the namespace. Just seems more natural.

This is an implementation detail - human-friendliness of docker's
names is a nice-to-have not critical, I think.

> 1. ""These names are typically NOT user-provided, but are generated by infrastructure."" I'd consider being more explicit about that behavior, even if it's as simple as a ""unique prefix will be added to the name"". This sorta touches on the whole id/name discussion....

Yeah, where is the line between ""ID"" and ""name""?  The apiserver COULD
just send a UUID for the name, but I think we want the structure and
content of a pod spec to stay largely the same throughout the entire
system, at least for now.  I don't think we've got any consensus on
diverging it, anyway.  For that reason, I think ""name"" makes sense.
It is more human readable than not.
"
523	199	thockin	2014-06-28 06:25:32	MEMBER	"+1 to labels

On Fri, Jun 27, 2014 at 8:45 AM, brendanburns notifications@github.com
wrote:

> I would also like to see us drive labels all the way down into Docker.
> That
> would enable us to abandon much of what we are encoding into the name
> today.
> 
> Brendan
> 
> On Fri, Jun 27, 2014, 8:39 AM, Clayton Coleman notifications@github.com
> wrote:
> 
> > One minor note regarding Docker ecosystem - ""name"" is currently being
> > used
> > in Docker for a lot of lightweight integrations (linking, sky dock dns,
> > hostname in container). A side effect of any generated name for a Docker
> > container is that those lightweight integrations may become more
> > difficult
> > for end admins. Is there a practical way to make the name appropriately
> > unique on the minion without breaking those potential integrations?
> > 
> > ## 
> > 
> > Reply to this email directly or view it on GitHub
> > <
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/199#issuecomment-47362231>
> > 
> > .
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/199#issuecomment-47363939
> .
"
524	199	thockin	2014-06-28 06:26:27	MEMBER	"What would break on those?  We're not proposing to do anything with name
that is not totally valid as per standalone Docker?

On Fri, Jun 27, 2014 at 8:39 AM, Clayton Coleman notifications@github.com
wrote:

> One minor note regarding Docker ecosystem - ""name"" is currently being used
> in Docker for a lot of lightweight integrations (linking, sky dock dns,
> hostname in container). A side effect of any generated name for a Docker
> container is that those lightweight integrations may become more difficult
> for end admins. Is there a practical way to make the name appropriately
> unique on the minion without breaking those potential integrations?
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/199#issuecomment-47362231
> .
"
525	282	lavalamp	2014-06-28 18:29:11	MEMBER	"Nice catch, thanks!
"
526	283	thockin	2014-06-28 20:15:40	MEMBER	"LGTM.  Sadly, I'll need an IntSet, too.
"
527	283	lavalamp	2014-06-28 21:01:45	MEMBER	"Ah. In that case, maybe we should make a set package, with set.Int and set.String types. At least that will keep the name lengths down.
"
528	283	thockin	2014-06-28 21:32:13	MEMBER	"SGTM as a followup.  Apprently you CAN make a set that will hold any type -
the downside being that it will accept any type - more like void\* than a
template.  better or worse?

On Sat, Jun 28, 2014 at 2:01 PM, Daniel Smith notifications@github.com
wrote:

> Ah. In that case, maybe we should make a set package, with set.Int and
> set.String types. At least that will keep the name lengths down.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/283#issuecomment-47438412
> .
"
529	280	lavalamp	2014-06-28 21:32:52	MEMBER	"Travis is failing you for needing a gofmt, btw. Might want to double check that your hooks are linked from .git/hooks.
"
530	283	lavalamp	2014-06-28 21:36:01	MEMBER	"IMO, worse, because if you do a ""for key := range myGenericSet"", you'll have to do type assertions on the key to use it. I can see doing this only if you have a need to put multiple different types in the set.
"
531	283	lavalamp	2014-06-28 21:38:30	MEMBER	"We should see about getting a complete such set library added to the go standard library as ""container/set"". IMO it's ridiculous that everyone has to write their own.
"
532	199	smarterclayton	2014-06-28 21:55:24	CONTRIBUTOR	"It's not that the names aren't valid for Docker, its that something consuming those names (as a dns prefix a la skydock, or as a hostname) would break due to length or format on the Kube generated name.  I don't think that is a blocker to the proposed naming patterns above, but it's a consideration when thinking about how other software that plays well with Docker might react.

Concrete ideas might be to reduce the generated names' length and avoid using '.' as a separator.
"
533	283	thockin	2014-06-28 22:52:39	MEMBER	"+1 except let's keep it minimal - no point duplicating code for each type
when we won't use a lot of it.

Without custom comparators set is a lot less useful.
On Jun 28, 2014 2:38 PM, ""Daniel Smith"" notifications@github.com wrote:

> We should see about getting a complete such set library added to the go
> standard library as ""container/set"". IMO it's ridiculous that everyone has
> to write their own.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/283#issuecomment-47439232
> .
"
534	286	lavalamp	2014-06-28 23:01:20	MEMBER	"Weird-- integration passes for me. Will fix later.
"
535	287	lavalamp	2014-06-29 01:37:50	MEMBER	"Thanks for catching this! Can we get you to sign a CLA as described in CONTRIB.md so that we can merge it? Thanks!
"
536	288	lavalamp	2014-06-29 01:59:04	MEMBER	"I believe @vmarmol is or was working on this? Adding it so we don't forget about it.
"
537	288	thockin	2014-06-29 02:43:45	MEMBER	"Caution: This is a slippery slope.  One day people will start setting
resource limits.  Launching all pulls in parallel can easily exhaust those
resource limits or put so much pressure on the disk that they all slow to
crawl while the disk head thrashes about (for those of us with physical
disks).

Moreover, this is a general scalability problem - we need to build Kubelet
in a way that ensures that one pod can not disrupt the servicing of
another.  This is just one (very noticeable) aspect of that problem.

This is something we have put a lot of thought into internally, and I'd
like to do properly...

On Sat, Jun 28, 2014 at 6:59 PM, Daniel Smith notifications@github.com
wrote:

> I believe @vmarmol https://github.com/vmarmol is or was working on
> this? Adding it so we don't forget about it.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/288#issuecomment-47443687
> .
"
538	288	brendandburns	2014-06-29 02:45:45	CONTRIBUTOR	"Also, please note that docker barfs if you try to pull the same image in parallel, that's why the lock is there in the first place.

If we really want to do this right, we should add a sharded lock, that will enable us to both have parallelism, and limit the extend of the parallelism, with a single configuration.
"
539	289	brendandburns	2014-06-29 02:51:30	CONTRIBUTOR	"LGTM
"
540	286	brendandburns	2014-06-29 02:58:57	CONTRIBUTOR	"Small stuff, generally LGTM
"
541	289	lavalamp	2014-06-29 04:10:06	MEMBER	"Cool!
"
542	285	smarterclayton	2014-06-29 04:12:29	CONTRIBUTOR	"Related to #127, #137, and #156. Are there other pieces of information besides state that are related to a container's execution that should be rolled up?  Certainly the time that a state transition occurred is relevant.  I bring up because we tend to think of this as a three part problem - 

1) specific container events which are core to the platform being reported to the core infrastructure (state, exit status)
2) generic container events triggered by other host level monitors (resource threshold exceeded), and possibly host level events (no memory available, swap warnings)
3) making decisions based on 1) or 2) that might feed into the scheduler or replication controllers

At the start, we saw this as a separate logical subsystem (events) with a separate return channel (since this information is essentially untrustworthy in a multi-tenant system), feeding back into an event bus that detached components could react to.  However, for 1) in particular I could buy the argument that this is a core mechanism of the master->kubelet system and is separate from a generic event mechanism.
"
543	278	smarterclayton	2014-06-29 04:17:40	CONTRIBUTOR	"Will fix in #253
"
544	280	thockin	2014-06-29 04:29:38	MEMBER	"I have resolved all FIXMEs, run gofmt, etc etc.

There are a number of comments on code that I don't know if I have
satisfied or not.  Could anyone who cares about this change please take
another look?

On Sat, Jun 28, 2014 at 2:32 PM, Daniel Smith notifications@github.com
wrote:

> Travis is failing you for needing a gofmt, btw. Might want to double check
> that your hooks are linked from .git/hooks.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/280#issuecomment-47439119
> .
"
545	280	thockin	2014-06-29 05:00:13	MEMBER	"OK, I finally got a green CI build - integration tests FTW.  Running this on my desktop at work means I can't run integration tests directly.
"
546	285	thockin	2014-06-29 05:06:12	MEMBER	"I think you're right that some things are so core that they have to be part
of the primary API.  As Kubelet gets more complicated, we really do need to
expose some details about whats going on.  We don't really have a state
machine yet, but we will.  We don't really have local state yet, but I
think we will.

I think we want the user experience to be centralized - some common tools
or UI or database that shows you everything that has been going on.

On Sat, Jun 28, 2014 at 9:12 PM, Clayton Coleman notifications@github.com
wrote:

> Related to #127
> https://github.com/GoogleCloudPlatform/kubernetes/issues/127 and #137
> https://github.com/GoogleCloudPlatform/kubernetes/issues/137. Are there
> other pieces of information besides state that are related to a container's
> execution that should be rolled up? Certainly the time that a state
> transition occurred is relevant. I bring up because we tend to think of
> this as a three part problem -
> 
> 1) specific container events which are core to the platform being reported
> to the core infrastructure (state, exit status)
> 2) generic container events triggered by other host level monitors
> (resource threshold exceeded), and possibly host level events (no memory
> available, swap warnings)
> 3) making decisions based on 1) or 2) that might feed into the scheduler
> or replication controllers
> 
> At the start, we saw this as a separate logical subsystem (events) with a
> separate return channel (since this information is essentially
> untrustworthy in a multi-tenant system), feeding back into an event bus
> that detached components could react to. However, for 1) in particular I
> could buy the argument that this is a core mechanism of the master->kubelet
> system and is separate from a generic event mechanism.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/285#issuecomment-47445384
> .
"
547	199	thockin	2014-06-29 05:19:51	MEMBER	"I would argue (for the sake of arguing) that anyone who was making
assumptions about container names deserves to be broken.  But yeah, let's
keep an eye on it.

On Sat, Jun 28, 2014 at 2:55 PM, Clayton Coleman notifications@github.com
wrote:

> It's not that the names aren't valid for Docker, its that something
> consuming those names (as a dns prefix a la skydock, or as a hostname)
> would break due to length or format on the Kube generated name. I don't
> think that is a blocker to the proposed naming patterns above, but it's a
> consideration when thinking about how other software that plays well with
> Docker might react.
> 
> Concrete ideas might be to reduce the generated names' length and avoid
> using '.' as a separator.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/199#issuecomment-47439586
> .
"
548	290	lavalamp	2014-06-29 05:30:34	MEMBER	"LGTM. Strongly agree that %#v is almost never better than an error's default Error() message, have been meaning to do this for a while. Leaving open in case @brendanburns requires more convincing.
"
549	280	lavalamp	2014-06-29 05:45:35	MEMBER	"Will give another look tomorrow or Monday. You _should_ be able to run the integration test on your desktop; I am doing that. For this change, you probably also want to run the e2e tests (hack/e2e-test.sh), which test starting a cluster, the guestbook example and another more basic test, and stopping the cluster.
"
550	199	smarterclayton	2014-06-29 16:53:08	CONTRIBUTOR	"Agree, naming is hard
"
551	288	vmarmol	2014-06-29 17:48:34	CONTRIBUTOR	"I started work on this, but while something simple that works is not too
bad to implement. I imagine you won't all be happy with it :) Trying to see
what I can come up with that doesn't require large re-design.

On Sat, Jun 28, 2014 at 7:45 PM, brendandburns notifications@github.com
wrote:

> Also, please note that docker barfs if you try to pull the same image in
> parallel, that's why the lock is there in the first place.
> 
> If we really want to do this right, we should add a sharded lock, that
> will enable us to both have parallelism, and limit the extend of the
> parallelism, with a single configuration.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/288#issuecomment-47444321
> .
"
552	286	lavalamp	2014-06-29 19:36:32	MEMBER	"Comments addressed.
"
553	245	thockin	2014-06-29 19:51:49	MEMBER	"One of them: VolumeMount.Path vs MountPath.
"
554	291	brendandburns	2014-06-30 02:51:19	CONTRIBUTOR	"No this is a total hack to make it easier for me to wire the Docker inspect info straight into our API.  We should really define our own node level info struct.
"
555	290	brendandburns	2014-06-30 03:34:11	CONTRIBUTOR	"I guess my concern is that we're depending on the implementor of Error to do sane things.  I've seen at least one case where the string version of an RESTful API error didn't contain the status code, and so it was annoying to get it out.  That said, I'm not 100% wedded to %#v if we really find it more spammy, but I like additional info when I'm debugging.
"
556	290	thockin	2014-06-30 04:18:40	MEMBER	"what about %v by default and %+v where needed?

On Sun, Jun 29, 2014 at 8:34 PM, brendandburns notifications@github.com
wrote:

> I guess my concern is that we're depending on the implementor of Error to
> do sane things. I've seen at least one case where the string version of an
> RESTful API error didn't contain the status code, and so it was annoying to
> get it out. That said, I'm not 100% wedded to %#v if we really find it more
> spammy, but I like additional info when I'm debugging.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/290#issuecomment-47492231
> .
"
557	288	thockin	2014-06-30 05:47:13	MEMBER	"Empirically concurrent pulls all bloxk until it is done pulling
"
558	288	brendandburns	2014-06-30 06:04:22	CONTRIBUTOR	"For the same image?  Maybe this is a recent change/fix.  I definitely saw
concurrent pulls off the same image fail in previous versions of docker
(Jan-ish, I think)

Brendan
On Jun 29, 2014 10:47 PM, ""Tim Hockin"" notifications@github.com wrote:

> Empirically concurrent pulls all bloxk until it is done pulling
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/288#issuecomment-47496676
> .
"
559	288	thockin	2014-06-30 06:10:35	MEMBER	"root@thdev:/home/thockin# docker pull ubuntu
Repository ubuntu already being pulled by another client. Waiting.
root@thdev:/home/thockin# docker -v
Docker version 0.11.1, build fb99f99

On Sun, Jun 29, 2014 at 11:04 PM, brendandburns notifications@github.com
wrote:

> For the same image? Maybe this is a recent change/fix. I definitely saw
> concurrent pulls off the same image fail in previous versions of docker
> (Jan-ish, I think)
> 
> Brendan
> On Jun 29, 2014 10:47 PM, ""Tim Hockin"" notifications@github.com wrote:
> 
> > Empirically concurrent pulls all bloxk until it is done pulling
> > 
> > ## 
> > 
> > Reply to this email directly or view it on GitHub
> > <
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/288#issuecomment-47496676>
> > 
> > .
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/288#issuecomment-47497425
> .
"
560	253	smarterclayton	2014-06-30 13:47:05	CONTRIBUTOR	"Would like to include on top of #280 
"
561	302	brendanburns	2014-06-30 15:32:15	CONTRIBUTOR	"While this is possible, we resync every 30 secs, so at worst, this adds
latency...

Brendan
On Jun 30, 2014 7:59 AM, ""Tim Hockin"" notifications@github.com wrote:

> This function gets initial state and then sets up a watch. It looks like
> (have not proven it yet) there is a race wherein the stat might change
> between those two events and not get noticed.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/302.
"
562	290	brendandburns	2014-06-30 16:11:16	CONTRIBUTOR	"I'm fine to give this a try until I have trouble debugging something ;)
"
563	292	lavalamp	2014-06-30 16:21:52	MEMBER	"You beat me to this. Perhaps should be COLLAB.md, since all our other docs are all uppercase for reasons unknown to me?
"
564	290	lavalamp	2014-06-30 16:26:54	MEMBER	"Brendan, my score with %#v vs %v for debugging error messages via their printed form is something like a 0-5 shutout at the moment. With %#v I always get something like &etcd.BlahError{Message: &(0xhexnumber)(*string)} where all the useful stuff is obscured in an internal field that dosen't get expanded. On the other hand, %#v is great for printing objects.
"
565	294	lavalamp	2014-06-30 16:29:19	MEMBER	"Also need to consider how replication controllers produce IDs for the pods they make, right now I think they just copy the possibly-blank ID field in their pod template.
"
566	296	lavalamp	2014-06-30 16:31:13	MEMBER	"It does track this, just not over reboots. (But we never reboot, because we religiously use defer util.HandleCrash() in all goroutines. ;) ;) )
"
567	277	brendandburns	2014-06-30 16:31:13	CONTRIBUTOR	"Ready to merge, I think.  PTAL.

Thanks!
--brendan
"
568	299	lavalamp	2014-06-30 16:34:00	MEMBER	"For even more delicious horribleness, use encoding/gob and encoding/base64 to produce a name literally constructed from the metadata :)
"
569	301	lavalamp	2014-06-30 16:35:20	MEMBER	"This is the generalized version of #288.
"
570	286	brendandburns	2014-06-30 16:36:15	CONTRIBUTOR	"LGTM.  New protocol says 2 LGTMs for merge, @thockin ?
"
571	299	brendandburns	2014-06-30 16:40:33	CONTRIBUTOR	"I did this in an old version of Kubernetes.

I would actually like to see us actively work to push labels down into the
Docker daemon.  Then we could use them to encode all this stuff.

--brendan

On Mon, Jun 30, 2014 at 9:34 AM, Daniel Smith notifications@github.com
wrote:

> For even more delicious horribleness, use encoding/gob and encoding/base64
> to produce a name literally constructed from the metadata :)
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/299#issuecomment-47554040
> .
"
572	293	lavalamp	2014-06-30 16:46:13	MEMBER	"LGTM
"
573	301	thockin	2014-06-30 17:00:17	MEMBER	"Sort of.  Having a ""thread"" per pod is not the same as a thread per
container.  Concurrent pulls could be serialized within a pod (so one pod
doesn't hurt others) or even serialized per disk if needed.  But yes, this
is more general.

On Mon, Jun 30, 2014 at 9:35 AM, Daniel Smith notifications@github.com
wrote:

> This is the generalized version of #288
> https://github.com/GoogleCloudPlatform/kubernetes/issues/288.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/301#issuecomment-47554187
> .
"
574	302	monnand	2014-06-30 17:46:31	CONTRIBUTOR	"Oops. I just filed an issue #304. It may be related.

FYI, go's [race detector](http://blog.golang.org/race-detector) is your friends.
"
575	302	monnand	2014-06-30 17:55:16	CONTRIBUTOR	"Not only in kubelet, `pkg/controller` has 4 data races. I will file another issue. Turning on the race detector when running `go test` might be a good choice.
"
576	302	thockin	2014-06-30 17:58:25	MEMBER	"This one in particular is not likely to be something that any tool can
catch - it's higher level.

On Mon, Jun 30, 2014 at 10:46 AM, monnand notifications@github.com wrote:

> Oops. I just filed an issue #304
> https://github.com/GoogleCloudPlatform/kubernetes/issues/304. It may be
> related.
> 
> FYI, go's race detector http://blog.golang.org/race-detector is your
> friends.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/302#issuecomment-47562526
> .
"
577	296	thockin	2014-06-30 18:01:15	MEMBER	"It only tracks it in memory.  Should kubelet ever restart, that info is lost.
"
578	302	monnand	2014-06-30 18:08:35	CONTRIBUTOR	"@thockin At least the race detector detected something. Did not read the message yet, it could be found here #304
"
579	302	thockin	2014-06-30 18:09:32	MEMBER	"Yes, we should look at that race, too :)

On Mon, Jun 30, 2014 at 11:08 AM, monnand notifications@github.com wrote:

> @thockin https://github.com/thockin At least the race detector detected
> something. Did not read the message yet, it could be found here #304
> https://github.com/GoogleCloudPlatform/kubernetes/issues/304
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/302#issuecomment-47565263
> .
"
580	306	monnand	2014-06-30 18:18:00	CONTRIBUTOR	"Changed to mutex. PTAL.
"
581	306	lavalamp	2014-06-30 18:19:17	MEMBER	"LGTM
"
582	306	monnand	2014-06-30 18:20:10	CONTRIBUTOR	"Thank you, @lavalamp.
"
583	280	proppy	2014-06-30 18:29:08	CONTRIBUTOR	"Curious if you considered using `jsonschema` for the validation?

There is a go implementation here:
https://github.com/xeipuuv/gojsonschema

And I did a schema for the old manifest here:
https://github.com/proppy/container-agent/blob/9df44d84241948a20c927e633594961904bf65b4/container_agent/manifest-schema.yaml
"
584	167	brendandburns	2014-06-30 19:10:04	CONTRIBUTOR	"We got your CLA today.

Any chance you can extract the Docker API calls for Image Pulling (rather than shelling out) into a separate PR?

Many thanks!
--brendan
"
585	306	monnand	2014-06-30 19:13:43	CONTRIBUTOR	"Good to merge?
"
586	306	lavalamp	2014-06-30 19:15:32	MEMBER	"Ah, just double checking our shiney new policy. I think this counts as a ""small"" change not needing to LGTMs. :)
"
587	306	monnand	2014-06-30 19:16:11	CONTRIBUTOR	"Thank you!
"
588	294	smarterclayton	2014-06-30 19:39:05	CONTRIBUTOR	"If Pod ID is required, and a ContainerManifest ID is also required, should the pod controller duplicate the pod id into the container manifest id if the container manifest id is not specified?  Then the replication controller should also generate a unique container manifest ID in the template on creation if one is not specified.  

I will make that change in #253 if so
"
589	293	brendandburns	2014-06-30 19:42:16	CONTRIBUTOR	"Ok, I think this qualifies for a 2 person review under the new guidelines @thockin can you hook me up?

(or @smarterclayton or @jjhuff ;)
"
590	280	thockin	2014-06-30 20:01:18	MEMBER	"We need to validate YAML, too.  And more, I am not sure we will never need
to validate other inputs, so I chose to centralize it.  I'm not super happy
with it, though.

On Mon, Jun 30, 2014 at 11:29 AM, Johan Euphrosine <notifications@github.com

> wrote:
> 
> Curious if you considered using jsonschema for the validation?
> 
> There is a go implementation here:
> https://github.com/xeipuuv/gojsonschema
> 
> And I did a schema for the old manifest here:
> 
> https://github.com/proppy/container-agent/blob/9df44d84241948a20c927e633594961904bf65b4/container_agent/manifest-schema.yaml
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/280#issuecomment-47567949
> .
"
591	280	thockin	2014-06-30 20:01:39	MEMBER	"I'm going to break this into a bunch of smaller pulls while I work out the
last few issues.

On Mon, Jun 30, 2014 at 1:00 PM, Tim Hockin thockin@google.com wrote:

> We need to validate YAML, too.  And more, I am not sure we will never need
> to validate other inputs, so I chose to centralize it.  I'm not super happy
> with it, though.
> 
> On Mon, Jun 30, 2014 at 11:29 AM, Johan Euphrosine <
> notifications@github.com> wrote:
> 
> > Curious if you considered using jsonschema for the validation?
> > 
> > There is a go implementation here:
> > https://github.com/xeipuuv/gojsonschema
> > 
> > And I did a schema for the old manifest here:
> > 
> > https://github.com/proppy/container-agent/blob/9df44d84241948a20c927e633594961904bf65b4/container_agent/manifest-schema.yaml
> > 
> > ## 
> > 
> > Reply to this email directly or view it on GitHub
> > https://github.com/GoogleCloudPlatform/kubernetes/pull/280#issuecomment-47567949
> > .
"
592	308	lavalamp	2014-06-30 21:19:18	MEMBER	"LGTM
"
593	308	lavalamp	2014-06-30 21:33:02	MEMBER	"Still LGTM
"
594	280	proppy	2014-06-30 21:42:17	CONTRIBUTOR	"@thockin I know it's a bit weird but `jsonschema` python module was validating `dict`, so it also worked with `yaml` input.

I suspect Go implementation just validate struct (or map) fields using reflection too.
"
595	199	smarterclayton	2014-06-30 21:48:26	CONTRIBUTOR	"Writing up a summary md - does this belong in api/doc, DESIGN.md, or another location?
"
596	293	jjhuff	2014-06-30 21:49:37	CONTRIBUTOR	"I'm about to head out on vacation so this is a perfect time for me to review code:)
At some point in the near future it might make sense for minions to 'register themselves' in etcd (or via http to the master) under /registry/hosts/<fqdn>/status or something.
"
597	293	lavalamp	2014-06-30 21:53:07	MEMBER	"> At some point in the near future it might make sense for minions to 'register themselves' in etcd (or via http to the master) under /registry/hosts//status or something.

Yeah, we've got a couple issues filed about this (security is hard). I think it makes sense to get them from the cloud provider if you're running with a cloud provider, though.
"
598	293	smarterclayton	2014-06-30 21:54:16	CONTRIBUTOR	"+1 - allowing an external source (a different Cloud implementation) to define the list helps for other integration cases by allowing trust delegation.
"
599	308	lavalamp	2014-06-30 22:22:51	MEMBER	"Merging, since this is now a small change :)
"
600	311	lavalamp	2014-06-30 22:24:16	MEMBER	"LGTM
"
601	312	lavalamp	2014-06-30 22:25:02	MEMBER	"LGTM
"
602	309	monnand	2014-06-30 22:34:08	CONTRIBUTOR	"@lavalamp PTAL.
"
603	313	lavalamp	2014-06-30 22:50:51	MEMBER	"Your git history is showing :)

Tip: never merge (or `git pull`). I'm joking, but only a little bit. Use `git fetch` and rebase instead. In fact, do it now to get rid of the ""Merge"" commit up there: `git rebase upstream/master`

After that, run `git rebase -i HEAD~3` and change three of your commits to be `fixup`s. In the future, you can use git commit --amend to modify your last commit.

After that, you should be left with a single commit. You can re-push with `git push -f origin fileserver`.
"
604	245	lavalamp	2014-06-30 23:09:18	MEMBER	"I think the other was Key vs Name in the Env section.
"
605	293	thockin	2014-06-30 23:11:13	MEMBER	"LGTM, modulo comments about empty fqdn.
"
606	309	monnand	2014-06-30 23:11:18	CONTRIBUTOR	"@lavalamp  PTAL, switched to sync.Cond.
"
607	245	thockin	2014-06-30 23:12:36	MEMBER	"For simple fields renames, is it sufficient to define both fields in go and
(once we have validation) canonicalize internally?

On Mon, Jun 30, 2014 at 4:09 PM, Daniel Smith notifications@github.com
wrote:

> I think the other was Key vs Name in the Env section.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/245#issuecomment-47598977
> .
"
608	294	thockin	2014-06-30 23:14:57	MEMBER	"I think so.  Re3plication controller should add a short random
prefix/suffix to each ID

On Mon, Jun 30, 2014 at 12:39 PM, Clayton Coleman notifications@github.com
wrote:

> If Pod ID is required, and a ContainerManifest ID is also required, should
> the pod controller duplicate the pod id into the container manifest id if
> the container manifest id is not specified? Then the replication controller
> should also generate a unique container manifest ID in the template on
> creation if one is not specified.
> 
> I will make that change in #253
> https://github.com/GoogleCloudPlatform/kubernetes/pull/253 if so
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/294#issuecomment-47576577
> .
"
609	286	thockin	2014-06-30 23:15:33	MEMBER	"LGTM
"
610	245	lavalamp	2014-06-30 23:16:55	MEMBER	"We could declare a ""for marshalling"" struct and write code to copy it over. I'd be unhappy to have both field names in our public types with comments saying ""don't use this"" for one of them.
"
611	313	dchen1107	2014-06-30 23:18:46	MEMBER	"I addressed your last comment. PTAL?
"
612	313	lavalamp	2014-06-30 23:22:36	MEMBER	"Awesome. LGTM. Can you squash your commit history (`git rebase -i HEAD~4`; change the last three ""pick"" to ""fixup"") and re-push before I merge?
"
613	245	thockin	2014-06-30 23:22:42	MEMBER	"Brendan and I were talking about that this morning - we both agree that we
should do this, but I want to try to do it cleanly.  In the short term, to
get validation in, I want to have compat.

On Mon, Jun 30, 2014 at 4:17 PM, Daniel Smith notifications@github.com
wrote:

> We could declare a ""for marshalling"" struct and write code to copy it
> over. I'd be unhappy to have both field names in our public types with
> comments saying ""don't use this"" for one of them.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/245#issuecomment-47599582
> .
"
614	304	dchen1107	2014-06-30 23:38:26	MEMBER	"Anyone is working on this one? If no, I plan to take it. Thanks!
"
615	304	lavalamp	2014-06-30 23:39:50	MEMBER	"I think @monnand has already got PRs in progress that fix it?
"
616	313	lavalamp	2014-06-30 23:53:29	MEMBER	"Thanks for the change!
"
617	304	monnand	2014-07-01 00:58:43	CONTRIBUTOR	"@dchen1107 @lavalamp I'm not working on this package, so feel free to change it.
"
618	318	brendandburns	2014-07-01 03:12:07	CONTRIBUTOR	"Generally LGTM, but tests are failing.
"
619	314	brendandburns	2014-07-01 03:21:31	CONTRIBUTOR	"LGTM
"
620	307	brendandburns	2014-07-01 03:24:44	CONTRIBUTOR	"Small comment, LGTM.
"
621	309	monnand	2014-07-01 03:52:16	CONTRIBUTOR	"@brendanburns All comments are address. PTAL.
"
622	274	monnand	2014-07-01 03:55:17	CONTRIBUTOR	"Right now, kubelet could pull data from cAdvisor. However, since cAdvisor currently could not read root container information without lmctfy, we could not get machine level usage percentiles.
"
623	103	brendandburns	2014-07-01 04:15:55	CONTRIBUTOR	"Closing this.  Kubecfg is sync by default, and when sync, the replication controller waits for the replicas to reach hysterisis.

So the pattern is:

kubecfg resize myController 0
kubecfg delete replicationControllers/myController

That shouldn't orphan any pods.
"
624	272	brendandburns	2014-07-01 04:36:51	CONTRIBUTOR	"Closed by #279 
"
625	302	brendandburns	2014-07-01 04:39:53	CONTRIBUTOR	"I thought about this some more.  I don't think that there is a race here.  /.../pods is supposed to contain the complete state for the kubelet.  So if we miss data, it doesn't matter, we get it on the subsequent watch.

If a container was added and then removed, and we miss it, then that is ok, since the current ""desired state"" is removed anyway.

Am I missing something?
"
626	297	brendandburns	2014-07-01 04:43:34	CONTRIBUTOR	"We actually had this a while ago (well byte-wise comparison anyway)

I think this is pretty low on the list of desired optimizations, although refactoring all of the data sources in way that would make this easy to add generically, is probably a good idea.
"
627	302	thockin	2014-07-01 04:45:31	MEMBER	"As long as we periodically force-resync, it's OK.  But there was some
discussion here about reducing the noise by making each source only submit
updates when it believes something has actually changed.  For example, set
an inotify watch, load initial state, while true handle inotify events.

I don't know etcd well enough to say whether it works this way, but I'll
bet 10 bucks it could.

This is not a very important change to make right now, so I am letting it
simmer.  I wanted to file an issue so I don't forget.

On Mon, Jun 30, 2014 at 9:40 PM, brendandburns notifications@github.com
wrote:

> I thought about this some more. I don't think that there is a race here.
> /.../pods is supposed to contain the complete state for the kubelet. So if
> we miss data, it doesn't matter, we get it on the subsequent watch.
> 
> If a container was added and then removed, and we miss it, then that is
> ok, since the current ""desired state"" is removed anyway.
> 
> Am I missing something?
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/302#issuecomment-47616296
> .
"
628	318	thockin	2014-07-01 04:59:14	MEMBER	"LGTM, fix tests.
"
629	284	bgrant0607	2014-07-01 05:44:04	MEMBER	"Both at the apiserver and in kubelet, I was thinking the easiest starting
point would be to add links to list the different types of objects, which
would literally be URLs referring to the corresponding API calls. We could
perhaps conditionalize subsequent hyperlinks based on the mediatype
requested. That would at least make it possible to do through the UI what's
already possible via the CLI.
"
630	284	dchen1107	2014-07-01 05:47:00	MEMBER	"+1 on bgrant0607@

btw, https://${apiserver}/logs does work now. I am working on kubelet one.
But looks like kubelet_server's http page doesn't work at all.

On Mon, Jun 30, 2014 at 10:44 PM, bgrant0607 notifications@github.com
wrote:

> Both at the apiserver and in kubelet, I was thinking the easiest starting
> point would be to add links to list the different types of objects, which
> would literally be URLs referring to the corresponding API calls. We could
> perhaps conditionalize subsequent hyperlinks based on the mediatype
> requested. That would at least make it possible to do through the UI what's
> already possible via the CLI.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/284#issuecomment-47619056
> .
"
631	253	smarterclayton	2014-07-01 14:50:49	CONTRIBUTOR	"Currently I'm using full UUID - in practice and for readability I could drop it down to 64 bits and then do a base64 (omitting - and _).  That would make generated ids immediately more human readable from the apiserver and kubelet, at the risk of moving to a 1 in 10 million chance of a collision with ~5 million containers.
"
632	284	brendandburns	2014-07-01 15:06:19	CONTRIBUTOR	"How are you trying to access it?

By default I don't think that firewalls are set up to show external traffic
to the Kubelet's http server.

Brendan
On Jun 30, 2014 10:47 PM, ""Dawn Chen"" notifications@github.com wrote:

> +1 on bgrant0607@
> 
> btw, https://${apiserver}/logs does work now. I am working on kubelet
> one.
> But looks like kubelet_server's http page doesn't work at all.
> 
> On Mon, Jun 30, 2014 at 10:44 PM, bgrant0607 notifications@github.com
> wrote:
> 
> > Both at the apiserver and in kubelet, I was thinking the easiest
> > starting
> > point would be to add links to list the different types of objects,
> > which
> > would literally be URLs referring to the corresponding API calls. We
> > could
> > perhaps conditionalize subsequent hyperlinks based on the mediatype
> > requested. That would at least make it possible to do through the UI
> > what's
> > already possible via the CLI.
> > 
> > —
> > Reply to this email directly or view it on GitHub
> > <
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/284#issuecomment-47619056>
> > 
> > .
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/284#issuecomment-47619196
> .
"
633	302	brendandburns	2014-07-01 15:10:40	CONTRIBUTOR	"Yes, etcd does support watch.  However, I empirically saw problem where
watches hung and didn't return data, that's why the sync is there.  The
CoreOS folks acknowledged that there were some problems there that are
going to be fixed in subsequent versions...

I agree that ideally we'd just watch all the time, but in practice, I think
resync is valuable insurance.
On Jun 30, 2014 9:45 PM, ""Tim Hockin"" notifications@github.com wrote:

> As long as we periodically force-resync, it's OK. But there was some
> discussion here about reducing the noise by making each source only submit
> updates when it believes something has actually changed. For example, set
> an inotify watch, load initial state, while true handle inotify events.
> 
> I don't know etcd well enough to say whether it works this way, but I'll
> bet 10 bucks it could.
> 
> This is not a very important change to make right now, so I am letting it
> simmer. I wanted to file an issue so I don't forget.
> 
> On Mon, Jun 30, 2014 at 9:40 PM, brendandburns notifications@github.com
> wrote:
> 
> > I thought about this some more. I don't think that there is a race here.
> > /.../pods is supposed to contain the complete state for the kubelet. So
> > if
> > we miss data, it doesn't matter, we get it on the subsequent watch.
> > 
> > If a container was added and then removed, and we miss it, then that is
> > ok, since the current ""desired state"" is removed anyway.
> > 
> > Am I missing something?
> > 
> > ## 
> > 
> > Reply to this email directly or view it on GitHub
> > <
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/302#issuecomment-47616296
> > 
> > .
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/302#issuecomment-47616528
> .
"
634	274	vmarmol	2014-07-01 16:05:18	CONTRIBUTOR	"@monnand I think that's a bug on cAdvisor no? We _should_ be reading stats.
"
635	284	dchen1107	2014-07-01 16:34:38	MEMBER	"Yes, I realized that right after I sent out last message, and plan to
propose to reserve a port for external traffic, so that I can config
firewalls for minons.

On Tue, Jul 1, 2014 at 8:06 AM, brendandburns notifications@github.com
wrote:

> How are you trying to access it?
> 
> By default I don't think that firewalls are set up to show external
> traffic
> to the Kubelet's http server.
> 
> Brendan
> On Jun 30, 2014 10:47 PM, ""Dawn Chen"" notifications@github.com wrote:
> 
> > +1 on bgrant0607@
> > 
> > btw, https://${apiserver}/logs does work now. I am working on kubelet
> > one.
> > But looks like kubelet_server's http page doesn't work at all.
> > 
> > On Mon, Jun 30, 2014 at 10:44 PM, bgrant0607 notifications@github.com
> > wrote:
> > 
> > > Both at the apiserver and in kubelet, I was thinking the easiest
> > > starting
> > > point would be to add links to list the different types of objects,
> > > which
> > > would literally be URLs referring to the corresponding API calls. We
> > > could
> > > perhaps conditionalize subsequent hyperlinks based on the mediatype
> > > requested. That would at least make it possible to do through the UI
> > > what's
> > > already possible via the CLI.
> > > 
> > > —
> > > Reply to this email directly or view it on GitHub
> > > <
> > 
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/284#issuecomment-47619056>
> > 
> > > .
> > 
> > —
> > Reply to this email directly or view it on GitHub
> > <
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/284#issuecomment-47619196>
> > 
> > .
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/284#issuecomment-47667760
> .
"
636	302	thockin	2014-07-01 16:36:21	MEMBER	"Even if we re-sync, we can keep enough state to detect a ""no change""
situation, and thereby reduce noise on the update channel.  But it's low
prio.

On Tue, Jul 1, 2014 at 8:10 AM, brendandburns notifications@github.com
wrote:

> Yes, etcd does support watch. However, I empirically saw problem where
> watches hung and didn't return data, that's why the sync is there. The
> CoreOS folks acknowledged that there were some problems there that are
> going to be fixed in subsequent versions...
> 
> I agree that ideally we'd just watch all the time, but in practice, I
> think
> resync is valuable insurance.
> On Jun 30, 2014 9:45 PM, ""Tim Hockin"" notifications@github.com wrote:
> 
> > As long as we periodically force-resync, it's OK. But there was some
> > discussion here about reducing the noise by making each source only
> > submit
> > updates when it believes something has actually changed. For example,
> > set
> > an inotify watch, load initial state, while true handle inotify events.
> > 
> > I don't know etcd well enough to say whether it works this way, but I'll
> > bet 10 bucks it could.
> > 
> > This is not a very important change to make right now, so I am letting
> > it
> > simmer. I wanted to file an issue so I don't forget.
> > 
> > On Mon, Jun 30, 2014 at 9:40 PM, brendandburns notifications@github.com
> > 
> > wrote:
> > 
> > > I thought about this some more. I don't think that there is a race
> > > here.
> > > /.../pods is supposed to contain the complete state for the kubelet.
> > > So
> > > if
> > > we miss data, it doesn't matter, we get it on the subsequent watch.
> > > 
> > > If a container was added and then removed, and we miss it, then that
> > > is
> > > ok, since the current ""desired state"" is removed anyway.
> > > 
> > > Am I missing something?
> > > 
> > > ## 
> > > 
> > > Reply to this email directly or view it on GitHub
> > > <
> > 
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/302#issuecomment-47616296
> > 
> > > .
> > 
> > ## 
> > 
> > Reply to this email directly or view it on GitHub
> > <
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/302#issuecomment-47616528>
> > 
> > .
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/302#issuecomment-47668313
> .
"
637	284	brendandburns	2014-07-01 16:36:55	CONTRIBUTOR	"Let's be careful here, remember that if you do this, it's exposed to the
world (e.g. the entire internet).  I think that routing it through a
handler on the master might be a more secure approach to this.

--brendan

On Tue, Jul 1, 2014 at 9:34 AM, Dawn Chen notifications@github.com wrote:

> Yes, I realized that right after I sent out last message, and plan to
> propose to reserve a port for external traffic, so that I can config
> firewalls for minons.
> 
> On Tue, Jul 1, 2014 at 8:06 AM, brendandburns notifications@github.com
> wrote:
> 
> > How are you trying to access it?
> > 
> > By default I don't think that firewalls are set up to show external
> > traffic
> > to the Kubelet's http server.
> > 
> > Brendan
> > On Jun 30, 2014 10:47 PM, ""Dawn Chen"" notifications@github.com wrote:
> > 
> > > +1 on bgrant0607@
> > > 
> > > btw, https://${apiserver}/logs does work now. I am working on kubelet
> > > one.
> > > But looks like kubelet_server's http page doesn't work at all.
> > > 
> > > On Mon, Jun 30, 2014 at 10:44 PM, bgrant0607 notifications@github.com
> > > 
> > > wrote:
> > > 
> > > > Both at the apiserver and in kubelet, I was thinking the easiest
> > > > starting
> > > > point would be to add links to list the different types of objects,
> > > > which
> > > > would literally be URLs referring to the corresponding API calls. We
> > > > could
> > > > perhaps conditionalize subsequent hyperlinks based on the mediatype
> > > > requested. That would at least make it possible to do through the UI
> > > > what's
> > > > already possible via the CLI.
> > > > 
> > > > —
> > > > Reply to this email directly or view it on GitHub
> > > > <
> > 
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/284#issuecomment-47619056>
> > 
> > > > .
> > > 
> > > —
> > > Reply to this email directly or view it on GitHub
> > > <
> > 
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/284#issuecomment-47619196>
> > 
> > > .
> > 
> > —
> > Reply to this email directly or view it on GitHub
> > <
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/284#issuecomment-47667760>
> > 
> > .
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/284#issuecomment-47678932
> .
"
638	284	lavalamp	2014-07-01 16:38:43	MEMBER	"Consider the alternative solution of letting apiserver (master) serve an address at which it'll proxy through to a minion. At the cost of extra load on apiserver, you have to worry about fewer firewall rules, which will make this easier to run in alternative places. And you'd have a convenient way to verify that master can talk to kubelets, which is required anyway.
"
639	301	brendandburns	2014-07-01 16:39:11	CONTRIBUTOR	"See #320 
"
640	199	thockin	2014-07-01 16:41:39	MEMBER	"I would go for docs/identifiers.md or something

On Mon, Jun 30, 2014 at 2:48 PM, Clayton Coleman notifications@github.com
wrote:

> Writing up a summary md - does this belong in api/doc, DESIGN.md, or
> another location?
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/199#issuecomment-47591844
> .
"
641	284	lavalamp	2014-07-01 16:42:49	MEMBER	"If Brendan and I independently come up with the same suggestion, it's probably a good one. :)
"
642	253	thockin	2014-07-01 16:56:46	MEMBER	"Are you ready for a thorough review on this one?

On Tue, Jul 1, 2014 at 7:50 AM, Clayton Coleman notifications@github.com
wrote:

> Currently I'm using full UUID - in practice and for readability I could
> drop it down to 64 bits and then do a base64 (omitting - and _). That would
> make generated ids immediately more human readable from the apiserver and
> kubelet, at the risk of moving to a 1 in 10 million chance of a collision
> with ~5 million containers.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/253#issuecomment-47665623
> .
"
643	253	smarterclayton	2014-07-01 16:59:35	CONTRIBUTOR	"General approach and uuid type (full uuid vs shorter random segment) would be helpful
"
644	320	thockin	2014-07-01 17:08:37	MEMBER	"Great change - I'm still learning go idioms, so take my feedback with a grain of salt
"
645	253	thockin	2014-07-01 17:13:29	MEMBER	"It would help a lot for review if you sent a PR just for the Id -> ID change

On Tue, Jul 1, 2014 at 9:59 AM, Clayton Coleman notifications@github.com
wrote:

> General approach and uuid type (full uuid vs shorter random segment) would
> be helpful
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/253#issuecomment-47682170
> .
"
646	293	brendandburns	2014-07-01 17:13:44	CONTRIBUTOR	"I can haz merge?
"
647	293	brendandburns	2014-07-01 17:15:36	CONTRIBUTOR	"oops, forgot to push changes for empty fqdn.  Pushed now.
"
648	293	smarterclayton	2014-07-01 17:15:45	CONTRIBUTOR	"LGTM
"
649	253	smarterclayton	2014-07-01 17:18:50	CONTRIBUTOR	"Done
"
650	318	lavalamp	2014-07-01 17:21:09	MEMBER	"Tests fixed. Broke them while making integration pass, heh.
"
651	293	lavalamp	2014-07-01 17:25:21	MEMBER	"Travis says no merge until you gofmt -s.
"
652	277	brendandburns	2014-07-01 17:28:44	CONTRIBUTOR	"Updated, ptal.

Thanks!
--brendan
"
653	321	brendandburns	2014-07-01 17:32:55	CONTRIBUTOR	"Merging this, travis is borked b/c of gofmt skew.
"
654	321	brendandburns	2014-07-01 17:33:04	CONTRIBUTOR	"(and thanks!)
"
655	322	lavalamp	2014-07-01 17:45:15	MEMBER	"LGTM
"
656	323	lavalamp	2014-07-01 17:47:36	MEMBER	"Do we really need three targets? Can we get rid of 1.2 or tip?
"
657	324	vmarmol	2014-07-01 17:50:07	CONTRIBUTOR	"Tested with kube-up/down. Let me know if there is anything else I should test.
"
658	253	thockin	2014-07-01 17:51:21	MEMBER	"Directionally good.

On Tue, Jul 1, 2014 at 10:18 AM, Clayton Coleman notifications@github.com
wrote:

> Done
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/253#issuecomment-47684401
> .
"
659	320	brendandburns	2014-07-01 17:52:20	CONTRIBUTOR	"rebased.
"
660	323	lavalamp	2014-07-01 17:55:47	MEMBER	"Yeah, let's please remove tip. All changes are failing gofmt in tip...
"
661	323	proppy	2014-07-01 17:56:40	CONTRIBUTOR	"@lavalamp #322 already disable gofmt for `tip`, do you also want to disable build and test for `tip`?
"
662	322	lavalamp	2014-07-01 17:57:11	MEMBER	"Travis failed on go1.2 and the error message isn't helpful...
"
663	322	proppy	2014-07-01 17:58:55	CONTRIBUTOR	"Yes, previous version was printing the name of the file that were not formatted.
"
664	323	lavalamp	2014-07-01 18:00:00	MEMBER	"@proppy Yeah, let's disable tip here and @brendandburns can add it back once he gets #322 working.
"
665	323	proppy	2014-07-01 18:01:31	CONTRIBUTOR	"PTAL
"
666	323	lavalamp	2014-07-01 18:03:48	MEMBER	"Merging this. Hopefully our builds will work again!
"
667	322	lavalamp	2014-07-01 18:06:02	MEMBER	"@proppy turned off building at tip, so rebase, turn tip back on, and fix this at your leisure. :)
"
668	194	brendandburns	2014-07-01 18:14:26	CONTRIBUTOR	"Closed by #319 
"
669	318	lavalamp	2014-07-01 18:15:22	MEMBER	"Can we merge this soon?
"
670	324	lavalamp	2014-07-01 18:20:23	MEMBER	"LGTM
"
671	309	monnand	2014-07-01 18:22:33	CONTRIBUTOR	"@lavalamp All comments addressed. PTAL.
"
672	284	dchen1107	2014-07-01 18:23:17	MEMBER	"I just found my last message from @google.com was lost from github.com:

On Tue, Jul 1, 2014 at 9:44 AM, Dawn Chen dawnchen@google.com wrote:

Even logs? Container / pod's stats / states should be easy and master is going to collect some of them anyway, but not sure about logs here.
"
673	284	brendandburns	2014-07-01 18:30:50	CONTRIBUTOR	"No, you don't need to send the data to the master.  Just gateway the HTTP
traffic, from master to minion.

e.g.

http://master/minion?id=foo

will delegate an HTTP call back out to the minion, and display the results.

Turn part of the master into an http proxy.

--brendan

On Tue, Jul 1, 2014 at 11:23 AM, Dawn Chen notifications@github.com wrote:

> I just found my last message from @google.com was lost from github.com:
> 
> On Tue, Jul 1, 2014 at 9:44 AM, Dawn Chen dawnchen@google.com wrote:
> 
> Even logs? Container / pod's stats / states should be easy and master is
> going to collect some of them anyway, but not sure about logs here.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/284#issuecomment-47692076
> .
"
674	309	lavalamp	2014-07-01 18:31:43	MEMBER	"Thanks for the change! Will merge when travis finishes.
"
675	324	brendandburns	2014-07-01 18:35:23	CONTRIBUTOR	"LGTM, modulo the version comment.
"
676	309	monnand	2014-07-01 18:37:56	CONTRIBUTOR	"Thank you, @lavalamp !
"
677	322	brendandburns	2014-07-01 18:40:19	CONTRIBUTOR	"Fixed, ptal.
"
678	309	lavalamp	2014-07-01 18:41:22	MEMBER	"Oh, we should have squashed this to get rid of the intermediate commits. Oops, next time :)
"
679	318	thockin	2014-07-01 18:45:35	MEMBER	"""We can’t automatically merge this pull request.""
"
680	324	vmarmol	2014-07-01 18:55:17	CONTRIBUTOR	"Updated version, PTAL
"
681	318	lavalamp	2014-07-01 19:10:14	MEMBER	"Argh, I did that to myself by merging Brendan's change. Rebased.
"
682	318	thockin	2014-07-01 19:23:09	MEMBER	"""x Failed — The Travis CI build failed ""
"
683	318	lavalamp	2014-07-01 19:28:20	MEMBER	"It's passing for me locally. Investigating.
"
684	312	brendandburns	2014-07-01 20:14:27	CONTRIBUTOR	"LGTM, looks like it needs a rebase, though...
"
685	326	brendandburns	2014-07-01 20:17:50	CONTRIBUTOR	"LGTM, modulo Daniel's comment.
"
686	193	brendandburns	2014-07-01 20:24:48	CONTRIBUTOR	"This is closed by a combination of the MinionRegistry, the new CloudMinionRegistry and flags in the default GCE set up.

On GCE, if a VM is deleted from the pool, its tasks will now be moved to a different machine.
"
687	193	lavalamp	2014-07-01 20:26:01	MEMBER	"I think that's only true for pods controlled by a replication controller?
"
688	325	monnand	2014-07-01 20:32:02	CONTRIBUTOR	"I think this PR should be P0 because it fixes a break on the CI test. It LGTM, but I do not have permission to merge. Anyone? 
"
689	325	lavalamp	2014-07-01 20:38:21	MEMBER	"It only matters when etcd is flaking, so sadly it will not fix any flaky tests, just improve behavior when they do flake.
"
690	300	brendandburns	2014-07-01 20:39:47	CONTRIBUTOR	"This is now /etc/kubernetes/manifests
"
691	288	brendandburns	2014-07-01 20:40:40	CONTRIBUTOR	"Closing this for now, we could do more, but I think we've gotten rid of the long pole.  Re-open if you disagree.
"
692	326	thockin	2014-07-01 20:49:48	MEMBER	"Done

On Tue, Jul 1, 2014 at 1:17 PM, brendandburns notifications@github.com
wrote:

> LGTM, modulo Daniel's comment.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/326#issuecomment-47704796
> .
"
693	312	thockin	2014-07-01 20:59:33	MEMBER	"rebased.
"
694	318	lavalamp	2014-07-01 21:20:13	MEMBER	"I see what's going on. #277 and this PR don't play well together. I don't yet understand how #277 (which waits for containerInfo to give confirmation that containers are running) passes integration by itself, since even before this change that path shouldn't be working in the integration test.
"
695	318	lavalamp	2014-07-01 22:39:00	MEMBER	"OK. I just went ahead and fixed it for reals. Kubelet now returns info for a whole _pod_ and not the bizarre thing we were doing before with passing a pod id to /containerInfo.
"
696	332	brendandburns	2014-07-01 22:39:14	CONTRIBUTOR	"Thanks (and sorry we missed that) can you sign our CLA as described in CONTRIB.md and we'll merge this in.

Thanks again!
--brendan
"
697	318	lavalamp	2014-07-01 22:41:01	MEMBER	"Note that I'm assuming that pod.ID == manifest.ID. Hopefully that will be true in the future.
"
698	329	lavalamp	2014-07-01 22:42:21	MEMBER	"Can you rebase so we can see what's new in this PR?
"
699	329	thockin	2014-07-01 22:59:46	MEMBER	"Done
"
700	301	vmarmol	2014-07-01 23:11:14	CONTRIBUTOR	"I'd like to assign this to myself since I'm working on making different syncs concurrent, but I don't have write access :)
"
701	199	smarterclayton	2014-07-01 23:20:31	CONTRIBUTOR	"Referenced pull includes a summary of this discussion, open questions:
- implication 3-1 (agree/disagree)?
- I defined ""pod unique name"" and ""pod unique identifier"" to represent thockin's kubelet items 3 and 4
"
702	334	brendandburns	2014-07-01 23:21:57	CONTRIBUTOR	"LGTM, I'll leave for @thockin and others to take a look at before we merge.
"
703	329	brendandburns	2014-07-01 23:24:12	CONTRIBUTOR	"LGTM, I'll let you decide about my comment.  If you want to punt, let's add a TODO and I'm happy to merge.
"
704	318	thockin	2014-07-01 23:25:35	MEMBER	"x Failed — The Travis CI build failed
"
705	318	lavalamp	2014-07-01 23:27:43	MEMBER	"Yeah, once again, it worked locally. Sigh.
"
706	329	thockin	2014-07-01 23:31:04	MEMBER	"TODO added and pushed
"
707	334	lavalamp	2014-07-01 23:32:45	MEMBER	"LGTM
"
708	330	thockin	2014-07-01 23:43:08	MEMBER	"rebased
"
709	318	lavalamp	2014-07-02 00:47:05	MEMBER	"OK, I'm not sure travis _should_ be passing, but it _is_, and so are my local runs. Let's get this approved and merged and then we can investigate why the mere presence of a pod's container's info (sans state{running=true}) is sufficient to convince the system that the pod has been created.

Note that this change is pretty different than it was when it got those LGTMs, so look carefully...
"
710	330	lavalamp	2014-07-02 00:51:16	MEMBER	"LGTM. Travis failure is a flake.
"
711	304	monnand	2014-07-02 01:05:04	CONTRIBUTOR	"I think this is fixed by #331. Thanks @dchen1107 
"
712	318	brendandburns	2014-07-02 05:13:34	CONTRIBUTOR	"This (still) looks ok to me, and since its been hanging out there forever, I'm going to merge it.
"
713	318	thockin	2014-07-02 05:14:08	MEMBER	"You beat me by less than a minute.  LGTM
"
714	333	brendandburns	2014-07-02 05:15:04	CONTRIBUTOR	"rebase?
"
715	333	thockin	2014-07-02 05:17:56	MEMBER	"Done
"
716	336	brendandburns	2014-07-02 16:17:27	CONTRIBUTOR	"Hey David
Thanks for the PR! we actually got an identical PR from another contributer that I merged, since it was on top of the list...

If you plan on sending more PRs can you sign our CLA (as described in the CONTRIB.md file)

Then we'll be able to easily merge PRs in the future.

Many thanks!
--brendan
"
717	337	lavalamp	2014-07-02 16:32:28	MEMBER	"Assigning to @bgrant0607 because he knows all about the /api directory. :)
"
718	339	lavalamp	2014-07-02 17:31:38	MEMBER	"Thanks!
"
719	340	lavalamp	2014-07-02 18:28:37	MEMBER	"Thanks for catching!
"
720	337	bgrant0607	2014-07-02 18:32:10	MEMBER	"LGTM.
"
721	284	monnand	2014-07-02 18:42:27	CONTRIBUTOR	"We are working on the backend storage for cAdvisor now. I just submitted a PR google/cadvisor#74, which will use [influxdb](http://influxdb.com/) as a backend storage to store stats data of all containers tracked by cAdvisor. I think this approach would be better than redirecting all stats to master for the following reasons:
- We could reduce the workload of the master, which is a very important component for the cluster.
- In the future, cAdvisor could support multiple backend storage and users could just check containers' history by only querying those storage components. 
- The storage software (influxdb, mysql, postgres, etc.) itself could handle all security issues and users are more familiar about how to build a safe mysql/postgres/influxdb environment.
- Specific to [influxdb](http://influxdb.com/), it supports a SQL-like language and could display nice graphs based on the results of queries. This feature itself could save a lot of time for us (assuming that most of us are not quite good at UI.)

Because stats data may be huge, it might be better to let another storage service to handle it. For other data, I think going through master node would be fine.
"
722	328	monnand	2014-07-02 20:22:23	CONTRIBUTOR	"@lavalamp @brendanburns @vmarmol All comments are addressed. PTAL.
"
723	307	lavalamp	2014-07-02 20:57:51	MEMBER	"Comment addressed, and made these work. Test in integration test.

I think it's pretty cool, this should make updates atomic for all apiserver resources. 
"
724	310	lavalamp	2014-07-02 20:58:35	MEMBER	"Can we rebase and turn this on? Or are there more races to fix?
"
725	328	monnand	2014-07-02 21:47:58	CONTRIBUTOR	"@lavalamp PTAL. I cleaned the path before split it. But still, we need to drop empty strings.
"
726	310	monnand	2014-07-02 21:51:44	CONTRIBUTOR	"@lavalamp I think we can. But someone need to refresh the CI to make sure it works. I do not have the permission.
"
727	342	lavalamp	2014-07-02 22:01:12	MEMBER	"LGTM
"
728	328	monnand	2014-07-02 22:21:04	CONTRIBUTOR	"@lavalamp PTAL.
"
729	342	thockin	2014-07-02 22:31:47	MEMBER	"I'm going to self-merge this - I call it trivial.
"
730	307	lavalamp	2014-07-02 22:42:58	MEMBER	"Comments addressed. PTAL
"
731	343	lavalamp	2014-07-02 23:18:25	MEMBER	"Perhaps squash the ""pub"" commit before we merge? `rebase -i HEAD~2` and use ""fixup""-- there may be easier ways, but that works.
"
732	343	thockin	2014-07-02 23:18:57	MEMBER	"Squashed.  Forgot to do that, sorry
"
733	343	lavalamp	2014-07-02 23:25:16	MEMBER	"Thanks for the change!
"
734	345	lavalamp	2014-07-02 23:40:58	MEMBER	"Aside from a few comments, this LGTM. Thanks for this change, it should go a long ways towards making our IDs sensible!
"
735	328	lavalamp	2014-07-02 23:43:57	MEMBER	"This is looking good, I think. Can you `git rebase -i HEAD~7` and change the intermediate commits to `fixup` so our commit history looks pretty?
"
736	310	lavalamp	2014-07-02 23:45:16	MEMBER	"@monnand, I think if you just

git fetch upstream
git rebase upstream/master
git push -f origin race-detector

It will auto-refresh and travis will do another build.
"
737	345	brendandburns	2014-07-02 23:46:14	CONTRIBUTOR	"Thanks for the PR!  LGTM, modulo Daniel's comments.
"
738	328	monnand	2014-07-02 23:47:39	CONTRIBUTOR	"@lavalamp Sure. I'm working on it.
"
739	328	monnand	2014-07-02 23:56:14	CONTRIBUTOR	"@lavalamp Done. I squashed 7 recent commits. Do you want me to squash all comments?
"
740	328	monnand	2014-07-03 00:01:01	CONTRIBUTOR	"@lavalamp I think you may want me to squash all comments into single one. Done.
"
741	310	monnand	2014-07-03 00:02:45	CONTRIBUTOR	"rebased. But nothing happen. Let me add a comment to the sell script and it will trigger the CI.
"
742	328	lavalamp	2014-07-03 00:03:48	MEMBER	"Thanks, LGTM! I'll let @thockin or @brendandburns give a final look and they can merge.
"
743	328	monnand	2014-07-03 00:04:51	CONTRIBUTOR	"@lavalamp Thank you!
"
744	310	monnand	2014-07-03 00:09:18	CONTRIBUTOR	"Don't merge it. Any recent commit changed the controller package? There are new data races in that package.
"
745	310	monnand	2014-07-03 00:34:56	CONTRIBUTOR	"I checked it and the reason is using `sync.Cond`. I send another PR #348 to fix this.
"
746	348	monnand	2014-07-03 00:37:00	CONTRIBUTOR	"In this PR, I also turned on the race detector. If the CI passed, then it means there's no data races detected by the race detector. Once this PR merged, the race detector will be turned on by default. #310 will not be necessary.
"
747	348	monnand	2014-07-03 00:46:39	CONTRIBUTOR	"There's another data race in kubelet package introduced by 1798e0fe (in kubelet.go, line 705). I'm going to fix it.
"
748	348	monnand	2014-07-03 01:07:51	CONTRIBUTOR	"Fixed data race introduced by 1798e0f. Let's see what CI say.
"
749	348	monnand	2014-07-03 01:15:25	CONTRIBUTOR	"OK. I did not notice there's a `proxy/config` package. There's data race.
"
750	346	smarterclayton	2014-07-03 02:10:00	CONTRIBUTOR	"LGTM
"
751	348	monnand	2014-07-03 05:16:25	CONTRIBUTOR	"God. The data race in `proxy/config` is a hard one. `NewServiceConfig()` returns a value, not a pointer. It took me several hours to figure it out that two objects are sharing same map! 

Besides, there's another easy-to-fix data race.

Lessons learned: Do not over use goroutine and channels. They are awesome, but in most cases, we don't need to pass messages around. Go is not erlang, there's side effect and it's hard to debug --- even with a race detector.

On my machine, all test passed with race detector turned on. 
"
752	348	monnand	2014-07-03 05:36:01	CONTRIBUTOR	"CI only failed on go 1.2. However, I was not able to reproduce it even with go1.2. Anyone wants to take a look? Probably we should remove Go 1.2 support in .travis.yml?  ping @brendanburns @lavalamp @thockin 
"
753	348	monnand	2014-07-03 05:57:04	CONTRIBUTOR	"This is interesting!

With go 1.2 tool chain, if I run: `go test -race -cover -coverprofile=tmp.out github.com/GoogleCloudPlatform/kubernetes/pkg/apiserve`, it will report data race.

If I run `go test -race github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver`, it will pass the test.

In short, with go1.2, do not run race detector with code coverage program together.

I don't know if it is a bug in our code, or a bug in race detector, or a bug in the code coverage program. I tried to understand the race detector's message, but got nothing useful. I think  it's unlikely to be a bug in our code. Do we need to change the test script? Or simply ignore go 1.2?
"
754	328	monnand	2014-07-03 05:59:15	CONTRIBUTOR	"Thank you @brendanburns !

BTW, you may want to take a look at #348. It is hilarious. (read my last comment.)
"
755	348	brendandburns	2014-07-03 06:02:41	CONTRIBUTOR	"Let's optionalize coverage, and turn it off in Travis.

Users can opt-in to turning it on, when they want to see coverage stats.

(also perhaps send a bug report to the go team, although given that its in
1.2, it might be off their roadmap to fix it)

--brendan

On Wed, Jul 2, 2014 at 10:57 PM, monnand notifications@github.com wrote:

> This is interesting!
> 
> With go 1.2 tool chain, if I run: go test -race -cover
> -coverprofile=tmp.out
> github.com/GoogleCloudPlatform/kubernetes/pkg/apiserve, it will report
> data race.
> 
> If I run go test -race
> github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver, it will pass the
> test. I don't know if it is a bug in our code, or a bug in race detector,
> or a bug in the code coverage program. I tried to understand the race
> detector's message, but got nothing useful. I think it's unlikely to be a
> bug in our code. Do we need to change the test script? Or simply ignore go
> 1.2?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/348#issuecomment-47869545
> .
"
756	348	monnand	2014-07-03 06:09:10	CONTRIBUTOR	"@brendandburns Turned off the coverage. Not quite familiar with shell script. PTAL.

I think we don't need to file this bug to Go team because it is fixed in Go 1.3. And Go only maintain one stable branch.
"
757	350	monnand	2014-07-03 06:09:48	CONTRIBUTOR	"LGTM. But I think we need another person's approval.
"
758	350	brendandburns	2014-07-03 06:15:30	CONTRIBUTOR	"I'm going to merge this now, this is a bad enough behavior (it's killing the cadvisor pods, for example) that I'd like to get the fix in before the long weekend.
"
759	348	monnand	2014-07-03 06:16:30	CONTRIBUTOR	"@brendanburns Turning off the coverage does not help. But this time, it is definitely a bug in go. It reports an error:

```
/tmp/go-build776683792/github.com/GoogleCloudPlatform/kubernetes/pkg/labels/_test/_testmain.go:5: can't find import: ""regexp""
FAIL    github.com/GoogleCloudPlatform/kubernetes/pkg/labels [build failed]
```

I have same problem when I use travis with cAdvisor. I think we may want to only use 1.3 and tip.
"
760	348	monnand	2014-07-03 06:33:57	CONTRIBUTOR	"@brendanburns I removed go 1.2 from .travis.yml and added the coverage back, which is useful and un-harmful now. PTAL. And we passed the CI with race detector turning on! :+1: 
"
761	345	monnand	2014-07-03 06:37:51	CONTRIBUTOR	"I have a small request: Would you please run the race detector first? Or could we wait for #348 getting merged?

I'm sorry, after fixing data races for two days, I think it might be better to make sure the code could pass the race detector before merging it.
"
762	310	monnand	2014-07-03 06:39:34	CONTRIBUTOR	"#348 is ready to review. Once #348 is merged, I will close this issue because #348 has turned the race detector on.
"
763	302	tristanz	2014-07-03 08:33:18	NONE	"I believe this is exactly the purpose of `waitIndex` in etcd.  You can to an initial sync and then issue each watch with the last `modifiedIndex` as the new `waitIndex`.  This should ensure no missed events.  There's no need to re-sync.  See:

https://godoc.org/github.com/coreos/go-etcd/etcd#Client.Watch

The only caveat  is etcd's snapshot policy, but the default is to always keep a good deal of history for this exact purpose and you can tune it as desired.

https://github.com/coreos/etcd/issues/285
"
764	336	dgageot	2014-07-03 10:16:05	NONE	"CLA signed for next time
"
765	167	discordianfish	2014-07-03 12:05:21	CONTRIBUTOR	"OK, I've created #351 
"
766	351	thockin	2014-07-03 15:27:22	MEMBER	"I have two questions

1) Does the API correctly handle pulling an image that is already being pulled?  The CLI recognizes this and blocks the caller until it is done, which is simple to handle.

2) Why is this better than shelling out?  Other than cleanliness, shelling out has the nice property that it is less code.  I have this same question in general - why bother with the API and the library deps to use it, when I can just assemble a commandline and exec it?
"
767	351	brendandburns	2014-07-03 16:30:13	CONTRIBUTOR	"Personally I like using the api, since it provides a structured response,
and higher granularity input.

It also provides better future proofing.  APIs have deprecation policies,
etc.  CLI flags and options, not so much.

I haven't looked at the PR (on my phone) but can we make certain that the
bits of code derived from the docker code, are clearly marked and isolated
in a separate file, with a clear copyright header?

Thanks
Brendan
On Jul 3, 2014 8:27 AM, ""Tim Hockin"" notifications@github.com wrote:

> I have two questions
> 
> 1) Does the API correctly handle pulling an image that is already being
> pulled? The CLI recognizes this and blocks the caller until it is done,
> which is simple to handle.
> 
> 2) Why is this better than shelling out? Other than cleanliness, shelling
> out has the nice property that it is less code. I have this same question
> in general - why bother with the API and the library deps to use it, when I
> can just assemble a commandline and exec it?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/351#issuecomment-47944655
> .
"
768	345	smarterclayton	2014-07-03 16:40:17	CONTRIBUTOR	"@monnand  no additional data races reported (other than the existing one in TestWatchControllers)
"
769	345	thockin	2014-07-03 16:40:52	MEMBER	"I'm unclear on what exactly JSONBase.ID is supposed to represent.  Is it a globally unique ID for each api-managed object?  Is it a transaction ID?  Something in-between?

I'm also finding it hard to think about this change because I am still mulling the other discussion about the different forms of name and ID.  It feels weird that ID is in JSONBase for some objects and not others.

I'm not blocking this change at this point, but I feel like I don't quite get it yet.  Something somewhere has not clicked for me :)
"
770	351	smarterclayton	2014-07-03 16:47:21	CONTRIBUTOR	"Actually, with Docker the CLI deprecation policy has been much stronger than the API deprecation policy.  With 1.0 in place the API now has a much stronger change policy, but I would say they're equally stable now.
"
771	351	discordianfish	2014-07-03 17:48:54	CONTRIBUTOR	"Ok, I discussed that parsing in #docker-dev and it seems it's not necessary to do that because the 'fromImage' parameter (which is called, a bit misfortune, 'Repository' in go-dockerclient) gets parsed server side for repo and so on. So I've removed the parsing.

The docker API now is stable, nobody should run docker < 1.0 already because the security fixes in 1.0.
Regarding the parallel pulls, I will look into that tomorrow. I also need to verify that the fromImage parameter can include tags or whether we need to parse the tag from the name first. So hold off with merging for now.
"
772	348	monnand	2014-07-03 21:22:26	CONTRIBUTOR	"Thank you, @brendanburns 
"
773	345	brendandburns	2014-07-03 21:25:25	CONTRIBUTOR	"I intended ID to be a unique GUID that identifies a particular type of resource in the system.

So all Pods should have unique ID, all replication controllers should have unique ID (though a replication controller and a pod might have the same id, since they're in different namespaces)

as far as I intended, it is not a transaction ID, except in the case of an ServerOp, and even their it is a GUID, its just a GUID for an object that represents a transaction.

I think its different than the name of a container inside a pod, for example, since that is a semantic construct used for identifying (and possibly for DNS allocation, etc), but will never be used by the API itself to reference to anything.

For a while, I actually had ID (GUID) and Name (semantic identifier) but that actually got too confusing, so I ripped out Name...
"
774	310	monnand	2014-07-03 21:30:48	CONTRIBUTOR	"#348 has merged. I'll close this issue now.
"
775	353	brendandburns	2014-07-04 03:44:11	CONTRIBUTOR	"Hey Clayton,
I think in general, I was trying to have it only return when it was ""done"" but as you point out, there are lots of problems with this, I think that falling back to ""in our control"" is perhaps the correct semantic.

We've been thinking along similar lines, namely that we should only do acceptance testing (e.g. ""is this pod feasible"") before returning 200 and stopping waiting, instead of waiting for the task to go fully running.

I took a rough design that @lavalamp proposed, and pasted it into:

https://github.com/GoogleCloudPlatform/kubernetes/issues/354

So if that design makes sense to you, and achieves the the same goals as what you are suggesting (I think it does) let's move towards that.  The first thing might to roll back the other PR, and then move forward from the previous state.

Thanks
--brendan
"
776	355	brendandburns	2014-07-04 06:25:22	CONTRIBUTOR	"I'm going to merge this, it's passed once, and it passes on my machine, and (more importantly) it makes the e2e tests pass.

We need to get rid of nginx, I think.  Its doing retries in bad places, and making the e2e unreliable.
"
777	351	discordianfish	2014-07-04 12:55:02	CONTRIBUTOR	"So the parallel pulls should behave the same: It blocks until the pull finished, then returns:
https://github.com/dotcloud/docker/blob/master/server/server.go#L1365

But the tags need to provided explicitly. So if Pull(image string) should support tags (user/image:tag), we need to either parse the tag or extend kubernetes container structs to support a tags attribute.

Parsing the tag is rather ugly because we basically need to parse everything which, as you saw earlier, is pretty complex.

So to me the right way to solve this is either:
1. Make Docker parse the tag from the string
2. Add tag to kubernetes container representation

What do you think?
"
778	341	meirf	2014-07-04 20:08:52	CONTRIBUTOR	"I've started working on this. I will submit a PR over the next day or so.
"
779	351	thockin	2014-07-05 02:57:21	MEMBER	"Where do you see tag in the docker API?   Maybe I'm looking in the wrong
place - I don't see them mentioned at all..

I'm in favor of (1) - this is not something the client should be expected
to parse, I think.

On Fri, Jul 4, 2014 at 5:55 AM, discordianfish notifications@github.com
wrote:

> So the parallel pulls should behave the same: It blocks until the pull
> finished, then returns:
> https://github.com/dotcloud/docker/blob/master/server/server.go#L1365
> 
> But the tags need to provided explicitly. So if Pull(image string) should
> support tags (user/image:tag), we need to either parse the tag or extend
> kubernetes container structs to support a tags attribute.
> 
> Parsing the tag is rather ugly because we basically need to parse
> everything which, as you saw earlier, is pretty complex.
> 
> So to me the right way to solve this is either:
> 1. Make Docker parse the tag from the string
> 2. Add tag to kubernetes container representation
> 
> What do you think?
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/351#issuecomment-48041114
> .
"
780	310	thockin	2014-07-05 03:06:54	MEMBER	"Is current master supposed to test cleanly?  It doesn't for me.  Lots of
races..

On Thu, Jul 3, 2014 at 2:30 PM, monnand notifications@github.com wrote:

> Closed #310 https://github.com/GoogleCloudPlatform/kubernetes/pull/310.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/310#event-138110355
> .
"
781	350	thockin	2014-07-05 03:19:40	MEMBER	"LGTM.  That is a really nefarious bug.  Why the language allows it is baffling to me (and according to google, many others, too)
"
782	345	thockin	2014-07-05 03:35:25	MEMBER	"Should we use JSONBase.ID  instead of structure-specific IDs with
structure-specific semantics (e.g. ""this ID survives pod moves and
restarts"" vs ""this ID is changed on every reschedule"") ?

Or put another way, the ContainerManifest does not include JSONBase.
 Should it?

On Thu, Jul 3, 2014 at 2:25 PM, brendandburns notifications@github.com
wrote:

> I intended ID to be a unique GUID that identifies a particular type of
> resource in the system.
> 
> So all Pods should have unique ID, all replication controllers should have
> unique ID (though a replication controller and a pod might have the same
> id, since they're in different namespaces)
> 
> as far as I intended, it is not a transaction ID, except in the case of an
> ServerOp, and even their it is a GUID, its just a GUID for an object that
> represents a transaction.
> 
> I think its different than the name of a container inside a pod, for
> example, since that is a semantic construct used for identifying (and
> possibly for DNS allocation, etc), but will never be used by the API itself
> to reference to anything.
> 
> For a while, I actually had ID (GUID) and Name (semantic identifier) but
> that actually got too confusing, so I ripped out Name...
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/345#issuecomment-47986654
> .
"
783	345	smarterclayton	2014-07-05 17:35:08	CONTRIBUTOR	"[This implies](https://github.com/GoogleCloudPlatform/kubernetes/pull/345#discussion_r14491699) to me that ContainerManifest should not have an ID - it was a temporary issue that the presence of a pod uuid resolves, so I marked it as deprecated in the latest commit.  Now, if ContainerManifest becomes an API resource as commented in another issue, I think we would want to make it a JSON base.
"
784	350	brendandburns	2014-07-05 23:40:01	CONTRIBUTOR	"Fwiw, this style of bug can bite you in C++ too (in fact it has bitten me)
when you do async RPCs inside a loop using a loop local request object.

I agree go makes it more likely due to its idioms.

Brendan
On Jul 4, 2014 8:19 PM, ""Tim Hockin"" notifications@github.com wrote:

> LGTM. That is a really nefarious bug. Why the language allows it is
> baffling to me (and according to google, many others, too)
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/350#issuecomment-48077407
> .
"
785	361	verdverm	2014-07-06 23:11:35	NONE	"#267 Allow minions to be securely registered/deregistered
"
786	360	thockin	2014-07-07 03:25:11	MEMBER	"Caveat: I am not super familiar with the selector parsing, but I was interested, so I looked.

As a general rule, I have found that little embedded DSLs like this are almost never worth the time to spec and parse properly, specially when simply open-coded like this.  I find the proposed syntax of << >> and $ to be sort of non-obvious (what is wrong with parentheses and commas?).  It's also not obvious from this reading whether a label value can have embedded metacharacters (<, >, space, quotes, etc) and what the escaping rules would be.

If this has to be a grammar, it should be dead obvious.  But does it have to be?  Can it be a structured message instead?
"
787	345	thockin	2014-07-07 03:39:35	MEMBER	"Yeah, we have some confusion between what is a pod and what is a manifest.
 From Kubelet's point of view a manifest IS a pod, and it is a resource.
 From apiserver's point of view it's just a piece of a larger resource.

Brendan and I were discussing separating the objects based on point-of-view
(API Server vs Kubelet) but I have not done so yet.  If
ContainerManifest.ID is deprecated, what do I use to identify a pod on a
kubelet?

Got time to do a hangout this week maybe?

On Sat, Jul 5, 2014 at 10:35 AM, Clayton Coleman notifications@github.com
wrote:

> This implies
> https://github.com/GoogleCloudPlatform/kubernetes/pull/345#discussion_r14491699
> to me that ContainerManifest should not have an ID - it was a temporary
> issue that the presence of a pod uuid resolves, so I marked it as
> deprecated in the latest commit. Now, if ContainerManifest becomes an API
> resource as commented in another issue, I think we would want to make it a
> JSON base.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/345#issuecomment-48091958
> .
"
788	345	smarterclayton	2014-07-07 03:53:50	CONTRIBUTOR	"Sure - I had started to take a stab at what it would take to abstract the kubelet from a direct etcd dependency in #356 and noted the same thing.  Would be good to go over the options in a hangout.  
"
789	345	thockin	2014-07-07 04:05:59	MEMBER	"You're east-coast time, right?

Tuesday afternoon (your time, after 11:00a California time) is open for
me...

For anyone else, this is not a private meeting, but we would like to focus
on progress as much as possible :)

On Sun, Jul 6, 2014 at 8:53 PM, Clayton Coleman notifications@github.com
wrote:

> Sure - I had started to take a stab at what it would take to abstract the
> kubelet from a direct etcd dependency in #356
> https://github.com/GoogleCloudPlatform/kubernetes/pull/356 and noted
> the same thing. Would be good to go over the options in a hangout.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/345#issuecomment-48138980
> .
"
790	345	verdverm	2014-07-07 05:55:05	NONE	"I'd be down to join a hangout, though I'm not sure how much I have to contribute at this point
"
791	351	discordianfish	2014-07-07 10:26:06	CONTRIBUTOR	"@thockin See ""Create an image"" here: http://docs.docker.com/reference/api/docker_remote_api_v1.12/#22-images
But the API docs are a bit misleading in that one, so I've created https://github.com/dotcloud/docker/issues/6837

And to me it also looks like the server should parse that, given it parses all other parameters from the name as well. But maybe there are reasons for not doing it. I'll discuss that, but if this gets changed kubernetes would require at least some future version of Docker. Would that be okay? Or should this be rather backward compatible? In that case we still need to parse the tag to support older docker daemons.
"
792	360	meirf	2014-07-07 11:57:00	CONTRIBUTOR	"@thockin, thank you for your feedback 

The original parsing used simple `split`ting so I tried to keep it. This forced a departure from using commas as a value separator in the set since the comma was already used a selector separator. Parsing might instead be done using regular expressions with capturing parentheses, but I agree that a different approach should be considered than this one-off encoding/parsing if set functionality is added.

What type of structured message do you have in mind?
"
793	345	smarterclayton	2014-07-07 13:29:10	CONTRIBUTOR	"Let's try 2pm EDT on Tuesday
"
794	359	smarterclayton	2014-07-07 13:31:16	CONTRIBUTOR	"Fixed
"
795	351	discordianfish	2014-07-07 15:18:53	CONTRIBUTOR	"@thockin I've opened https://github.com/dotcloud/docker/issues/6876 to track adding parsing of the tag to the daemon. But since it probably make sense to support already existing Docker versions, I've added the parsing to kubernetes for :). See updated code.
"
796	363	brendandburns	2014-07-07 16:59:05	CONTRIBUTOR	"Hey Brian,
Any chance you have cycles to fill this in somewhat?  If not I can definitely take a crack at it.

Thanks
--brendan
"
797	362	brendandburns	2014-07-07 17:01:03	CONTRIBUTOR	"Thanks for the report.  This looks like its in the Google Cloud SDK, I'll forward the report on to them and see if they know about anything.

Will report back when I know more.

Thanks again!
--brendan
"
798	346	brendandburns	2014-07-07 17:02:37	CONTRIBUTOR	"Friendly monday morning ping ;) @thockin you're my only hope ;)
"
799	346	thockin	2014-07-07 17:06:58	MEMBER	"LGTM, modulo one question on Manifest.ID
"
800	346	brendandburns	2014-07-07 17:10:46	CONTRIBUTOR	"Comment addressed, ptal.

--brendan
"
801	364	brendandburns	2014-07-07 17:14:16	CONTRIBUTOR	"Ugh, ok, I bit the bullet and moved integration test back to being sync: true.  I don't like this, but I'd rather get to green and then work from there.
"
802	351	thockin	2014-07-07 17:30:30	MEMBER	"I find no ""Creating"" anywhere on the page at
http://docs.docker.com/reference/api/docker_remote_api_v1.12/#22-images

?

On Mon, Jul 7, 2014 at 3:26 AM, discordianfish notifications@github.com
wrote:

> @thockin https://github.com/thockin See ""Create an image"" here:
> http://docs.docker.com/reference/api/docker_remote_api_v1.12/#22-images
> But the API docs are a bit misleading in that one, so I've created
> dotcloud/docker#6837 https://github.com/dotcloud/docker/issues/6837
> 
> And to me it also looks like the server should parse that, given it parses
> all other parameters from the name as well. But maybe there are reasons for
> not doing it. I'll discuss that, but if this gets changed kubernetes would
> require at least some future version of Docker. Would that be okay? Or
> should this be rather backward compatible? In that case we still need to
> parse the tag to support older docker daemons.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/351#issuecomment-48162607
> .
"
803	351	discordianfish	2014-07-07 17:39:57	CONTRIBUTOR	"@thockin Not ""Creating"", but ""Create"" :) But yes it's rather hard to navigate the docs..
"
804	310	monnand	2014-07-07 17:46:41	CONTRIBUTOR	"@thockin Merging #348 would have fixed all detect-able races. But I'm not sure if any PR after #348 introduced new races. The currently master breaks the CI because of some time out error. I do not have if it is related to the races you mentioned.
"
805	310	thockin	2014-07-07 17:51:56	MEMBER	"When I run tests on current tip, I get errors

$ ./hack/test-go.sh
ok   github.com/GoogleCloudPlatform/kubernetes/pkg/api 1.030s coverage:
81.0% of statements
I0707 10:51:01.132286 31033 logger.go:84] GET /prefix/version/simple:

# (462.796us) 200

WARNING: DATA RACE
Write by goroutine 14:
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.func·004()
http://github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.func%C2%B7004()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/operation.go:35
+0x45
  github.com/GoogleCloudPlatform/kubernetes/pkg/util.func·001()
http://github.com/GoogleCloudPlatform/kubernetes/pkg/util.func%C2%B7001()
      /home/thockin/src/kubernetes/output/go/src/
github.com/GoogleCloudPlatform/kubernetes/pkg/util/util.go:56 +0x5d
  github.com/GoogleCloudPlatform/kubernetes/pkg/util.Forever()
      /home/thockin/src/kubernetes/output/go/src/
github.com/GoogleCloudPlatform/kubernetes/pkg/util/util.go:57 +0x79

Previous write by goroutine 7:
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.func·004()
http://github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.func%C2%B7004()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/operation.go:35
+0x45
  github.com/GoogleCloudPlatform/kubernetes/pkg/util.func·001()
http://github.com/GoogleCloudPlatform/kubernetes/pkg/util.func%C2%B7001()
      /home/thockin/src/kubernetes/output/go/src/
github.com/GoogleCloudPlatform/kubernetes/pkg/util/util.go:56 +0x5d
  github.com/GoogleCloudPlatform/kubernetes/pkg/util.Forever()
      /home/thockin/src/kubernetes/output/go/src/
github.com/GoogleCloudPlatform/kubernetes/pkg/util/util.go:57 +0x79

Goroutine 14 (running) created at:
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.NewOperations()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/operation.go:35
+0x15f
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.New()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:75
+0x4b
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.TestErrorList()
      /home/thockin/src/kubernetes/output/go/src/
github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/apiserver_test.go:155
+0x16c
  testing.tRunner()
      /usr/local/go/src/pkg/testing/testing.go:391 +0x10f

Goroutine 7 (running) created at:
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.NewOperations()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/operation.go:35
+0x15f
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.New()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:75
+0x4b
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.TestSimpleList()
      /home/thockin/src/kubernetes/output/go/src/
github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/apiserver_test.go:138
+0x127
  testing.tRunner()

#       /usr/local/go/src/pkg/testing/testing.go:391 +0x10f

# 

WARNING: DATA RACE
Write by goroutine 14:

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*Operations).expire()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/operation.go:89
+0x47
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.func·004()
http://github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.func%C2%B7004()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/operation.go:35
+0x7d
  github.com/GoogleCloudPlatform/kubernetes/pkg/util.func·001()
http://github.com/GoogleCloudPlatform/kubernetes/pkg/util.func%C2%B7001()
      /home/thockin/src/kubernetes/output/go/src/
github.com/GoogleCloudPlatform/kubernetes/pkg/util/util.go:56 +0x5d
  github.com/GoogleCloudPlatform/kubernetes/pkg/util.Forever()
      /home/thockin/src/kubernetes/output/go/src/
github.com/GoogleCloudPlatform/kubernetes/pkg/util/util.go:57 +0x79

Previous write by goroutine 7:

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*Operations).expire()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/operation.go:89
+0x47
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.func·004()
http://github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.func%C2%B7004()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/operation.go:35
+0x7d
  github.com/GoogleCloudPlatform/kubernetes/pkg/util.func·001()
http://github.com/GoogleCloudPlatform/kubernetes/pkg/util.func%C2%B7001()
      /home/thockin/src/kubernetes/output/go/src/
github.com/GoogleCloudPlatform/kubernetes/pkg/util/util.go:56 +0x5d
  github.com/GoogleCloudPlatform/kubernetes/pkg/util.Forever()
      /home/thockin/src/kubernetes/output/go/src/
github.com/GoogleCloudPlatform/kubernetes/pkg/util/util.go:57 +0x79

Goroutine 14 (running) created at:
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.NewOperations()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/operation.go:35
+0x15f
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.New()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:75
+0x4b
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.TestErrorList()
      /home/thockin/src/kubernetes/output/go/src/
github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/apiserver_test.go:155
+0x16c
  testing.tRunner()
      /usr/local/go/src/pkg/testing/testing.go:391 +0x10f

Goroutine 7 (running) created at:
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.NewOperations()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/operation.go:35
+0x15f
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.New()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:75
+0x4b
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.TestSimpleList()
      /home/thockin/src/kubernetes/output/go/src/
github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/apiserver_test.go:138
+0x127
  testing.tRunner()

#       /usr/local/go/src/pkg/testing/testing.go:391 +0x10f

# 

WARNING: DATA RACE
Write by goroutine 14:

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*Operations).expire()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/operation.go:101
+0x277
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.func·004()
http://github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.func%C2%B7004()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/operation.go:35
+0x7d
  github.com/GoogleCloudPlatform/kubernetes/pkg/util.func·001()
http://github.com/GoogleCloudPlatform/kubernetes/pkg/util.func%C2%B7001()
      /home/thockin/src/kubernetes/output/go/src/
github.com/GoogleCloudPlatform/kubernetes/pkg/util/util.go:56 +0x5d
  github.com/GoogleCloudPlatform/kubernetes/pkg/util.Forever()
      /home/thockin/src/kubernetes/output/go/src/
github.com/GoogleCloudPlatform/kubernetes/pkg/util/util.go:57 +0x79

Previous write by goroutine 7:

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*Operations).expire()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/operation.go:101
+0x277
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.func·004()
http://github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.func%C2%B7004()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/operation.go:35
+0x7d
  github.com/GoogleCloudPlatform/kubernetes/pkg/util.func·001()
http://github.com/GoogleCloudPlatform/kubernetes/pkg/util.func%C2%B7001()
      /home/thockin/src/kubernetes/output/go/src/
github.com/GoogleCloudPlatform/kubernetes/pkg/util/util.go:56 +0x5d
  github.com/GoogleCloudPlatform/kubernetes/pkg/util.Forever()
      /home/thockin/src/kubernetes/output/go/src/
github.com/GoogleCloudPlatform/kubernetes/pkg/util/util.go:57 +0x79

Goroutine 14 (running) created at:
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.NewOperations()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/operation.go:35
+0x15f
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.New()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:75
+0x4b
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.TestErrorList()
      /home/thockin/src/kubernetes/output/go/src/
github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/apiserver_test.go:155
+0x16c
  testing.tRunner()
      /usr/local/go/src/pkg/testing/testing.go:391 +0x10f

Goroutine 7 (running) created at:
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.NewOperations()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/operation.go:35
+0x15f
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.New()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:75
+0x4b
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.TestSimpleList()
      /home/thockin/src/kubernetes/output/go/src/
github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/apiserver_test.go:138
+0x127
  testing.tRunner()

#       /usr/local/go/src/pkg/testing/testing.go:391 +0x10f

I0707 10:51:01.135193 31033 logger.go:84] GET /prefix/version/simple:
(147.07us) 500
goroutine 17 [running]:
github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(_respLogger).WriteHeader(0xc210051e40,
0x1f4)
 github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/logger.go:104
+0x10b
github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(_ApiServer).error(0xc21007fa50,
0x7fc077b06e80, 0xc21001edc0, 0x7fc077b08b00, 0xc210051e40)

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:178
+0x73
github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*ApiServer).handleREST(0xc21007fa50,
0xc21004c0d0, 0x1, 0x1, 0xc2100474d0, ...)

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:259
+0x6b8
github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*ApiServer).ServeHTTP(0xc21007fa50,
0x7fc077b08ac8, 0xc21000fd20, 0xc2100910d0)

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:153
+0xdbd
net/http/httptest.(_waitGroupHandler).ServeHTTP(0xc21004ce40,
0x7fc077b08ac8, 0xc21000fd20, 0xc2100910d0)
/usr/local/go/src/pkg/net/http/httptest/server.go:200 +0xe0
net/http.serverHandler.ServeHTTP(0xc21001fdc0, 0x7fc077b08ac8,
0xc21000fd20, 0xc2100910d0)
/usr/local/go/src/pkg/net/http/server.go:1597 +0x1cb
net/http.(_conn).serve(0xc210059600)
 /usr/local/go/src/pkg/net/http/server.go:1167 +0xc01
created by net/http.(*Server).Serve
/usr/local/go/src/pkg/net/http/server.go:1644 +0x2c1
I0707 10:51:01.137497 31033 logger.go:84] GET /prefix/version/simple:
(190.068us) 200
I0707 10:51:01.140352 31033 logger.go:84] GET /prefix/version/simple/id:
(139.615us) 200
I0707 10:51:01.143403 31033 logger.go:84] DELETE /prefix/version/simple/id:
(744.879us) 202
I0707 10:51:01.146837 31033 logger.go:84] PUT /prefix/version/simple/id:

# (901.881us) 202

WARNING: DATA RACE
Write by goroutine 51:
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.func·001()
http://github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.func%C2%B7001()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:34
+0x4e

Previous write by goroutine 41:
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.func·001()
http://github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.func%C2%B7001()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:34
+0x4e

Goroutine 51 (running) created at:
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.MakeAsync()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:58
+0x113

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*SimpleRESTStorage).Update()
      /home/thockin/src/kubernetes/output/go/src/
github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/apiserver_test.go:121
+0x1f9

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*ApiServer).handleREST()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:351
+0x1249

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*ApiServer).ServeHTTP()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:153
+0xdbc
  net/http/httptest.(_waitGroupHandler).ServeHTTP()
      /usr/local/go/src/pkg/net/http/httptest/server.go:200 +0xdf
  net/http.serverHandler.ServeHTTP()
      /usr/local/go/src/pkg/net/http/server.go:1597 +0x1ca
  net/http.(_conn).serve()
      /usr/local/go/src/pkg/net/http/server.go:1167 +0xc00

Goroutine 41 (finished) created at:
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.MakeAsync()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:58
+0x113

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*SimpleRESTStorage).Delete()
      /home/thockin/src/kubernetes/output/go/src/
github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/apiserver_test.go:89
+0x1a0

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*ApiServer).handleREST()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:322
+0x31d

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*ApiServer).ServeHTTP()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:153
+0xdbc
  net/http/httptest.(_waitGroupHandler).ServeHTTP()
      /usr/local/go/src/pkg/net/http/httptest/server.go:200 +0xdf
  net/http.serverHandler.ServeHTTP()
      /usr/local/go/src/pkg/net/http/server.go:1597 +0x1ca
  net/http.(_conn).serve()

#       /usr/local/go/src/pkg/net/http/server.go:1167 +0xc00

# 

WARNING: DATA RACE
Write by goroutine 51:
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.func·001()
http://github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.func%C2%B7001()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:52
+0x2e3

Previous write by goroutine 41:
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.func·001()
http://github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.func%C2%B7001()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:52
+0x2e3

Goroutine 51 (running) created at:
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.MakeAsync()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:58
+0x113

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*SimpleRESTStorage).Update()
      /home/thockin/src/kubernetes/output/go/src/
github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/apiserver_test.go:121
+0x1f9

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*ApiServer).handleREST()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:351
+0x1249

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*ApiServer).ServeHTTP()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:153
+0xdbc
  net/http/httptest.(_waitGroupHandler).ServeHTTP()
      /usr/local/go/src/pkg/net/http/httptest/server.go:200 +0xdf
  net/http.serverHandler.ServeHTTP()
      /usr/local/go/src/pkg/net/http/server.go:1597 +0x1ca
  net/http.(_conn).serve()
      /usr/local/go/src/pkg/net/http/server.go:1167 +0xc00

Goroutine 41 (finished) created at:
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.MakeAsync()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:58
+0x113

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*SimpleRESTStorage).Delete()
      /home/thockin/src/kubernetes/output/go/src/
github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/apiserver_test.go:89
+0x1a0

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*ApiServer).handleREST()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:322
+0x31d

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*ApiServer).ServeHTTP()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:153
+0xdbc
  net/http/httptest.(_waitGroupHandler).ServeHTTP()
      /usr/local/go/src/pkg/net/http/httptest/server.go:200 +0xdf
  net/http.serverHandler.ServeHTTP()
      /usr/local/go/src/pkg/net/http/server.go:1597 +0x1ca
  net/http.(_conn).serve()

#       /usr/local/go/src/pkg/net/http/server.go:1167 +0xc00

# 

WARNING: DATA RACE
Write by goroutine 52:

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*Operation).wait()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/operation.go:106
+0x3c

Previous write by goroutine 42:

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*Operation).wait()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/operation.go:106
+0x3c

Goroutine 52 (running) created at:

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*Operations).NewOperation()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/operation.go:48
+0x15e

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*ApiServer).finishReq()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:190
+0x8f

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*ApiServer).handleREST()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:358
+0x134c

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*ApiServer).ServeHTTP()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:153
+0xdbc
  net/http/httptest.(_waitGroupHandler).ServeHTTP()
      /usr/local/go/src/pkg/net/http/httptest/server.go:200 +0xdf
  net/http.serverHandler.ServeHTTP()
      /usr/local/go/src/pkg/net/http/server.go:1597 +0x1ca
  net/http.(_conn).serve()
      /usr/local/go/src/pkg/net/http/server.go:1167 +0xc00

Goroutine 42 (finished) created at:

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*Operations).NewOperation()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/operation.go:48
+0x15e

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*ApiServer).finishReq()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:190
+0x8f

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*ApiServer).handleREST()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:329
+0x420

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*ApiServer).ServeHTTP()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:153
+0xdbc
  net/http/httptest.(_waitGroupHandler).ServeHTTP()
      /usr/local/go/src/pkg/net/http/httptest/server.go:200 +0xdf
  net/http.serverHandler.ServeHTTP()
      /usr/local/go/src/pkg/net/http/server.go:1597 +0x1ca
  net/http.(_conn).serve()

#       /usr/local/go/src/pkg/net/http/server.go:1167 +0xc00

# 

WARNING: DATA RACE
Write by goroutine 53:

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*Operations).insert()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/operation.go:54
+0x3c

Previous write by goroutine 43:

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*Operations).insert()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/operation.go:54
+0x3c

Goroutine 53 (running) created at:

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*Operations).NewOperation()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/operation.go:49
+0x17f

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*ApiServer).finishReq()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:190
+0x8f

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*ApiServer).handleREST()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:358
+0x134c

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*ApiServer).ServeHTTP()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:153
+0xdbc
  net/http/httptest.(_waitGroupHandler).ServeHTTP()
      /usr/local/go/src/pkg/net/http/httptest/server.go:200 +0xdf
  net/http.serverHandler.ServeHTTP()
      /usr/local/go/src/pkg/net/http/server.go:1597 +0x1ca
  net/http.(_conn).serve()
      /usr/local/go/src/pkg/net/http/server.go:1167 +0xc00

Goroutine 43 (finished) created at:

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*Operations).NewOperation()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/operation.go:49
+0x17f

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*ApiServer).finishReq()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:190
+0x8f

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*ApiServer).handleREST()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:329
+0x420

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*ApiServer).ServeHTTP()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:153
+0xdbc
  net/http/httptest.(_waitGroupHandler).ServeHTTP()
      /usr/local/go/src/pkg/net/http/httptest/server.go:200 +0xdf
  net/http.serverHandler.ServeHTTP()
      /usr/local/go/src/pkg/net/http/server.go:1597 +0x1ca
  net/http.(_conn).serve()

#       /usr/local/go/src/pkg/net/http/server.go:1167 +0xc00

# 

WARNING: DATA RACE
Write by goroutine 51:
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.func·001()
http://github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.func%C2%B7001()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:55
+0x286

Previous write by goroutine 41:
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.func·001()
http://github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.func%C2%B7001()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:55
+0x286

Goroutine 51 (running) created at:
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.MakeAsync()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:58
+0x113

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*SimpleRESTStorage).Update()
      /home/thockin/src/kubernetes/output/go/src/
github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/apiserver_test.go:121
+0x1f9

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*ApiServer).handleREST()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:351
+0x1249

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*ApiServer).ServeHTTP()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:153
+0xdbc
  net/http/httptest.(_waitGroupHandler).ServeHTTP()
      /usr/local/go/src/pkg/net/http/httptest/server.go:200 +0xdf
  net/http.serverHandler.ServeHTTP()
      /usr/local/go/src/pkg/net/http/server.go:1597 +0x1ca
  net/http.(_conn).serve()
      /usr/local/go/src/pkg/net/http/server.go:1167 +0xc00

Goroutine 41 (finished) created at:
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.MakeAsync()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:58
+0x113

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*SimpleRESTStorage).Delete()
      /home/thockin/src/kubernetes/output/go/src/
github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/apiserver_test.go:89
+0x1a0

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*ApiServer).handleREST()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:322
+0x31d

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*ApiServer).ServeHTTP()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:153
+0xdbc
  net/http/httptest.(_waitGroupHandler).ServeHTTP()
      /usr/local/go/src/pkg/net/http/httptest/server.go:200 +0xdf
  net/http.serverHandler.ServeHTTP()
      /usr/local/go/src/pkg/net/http/server.go:1597 +0x1ca
  net/http.(_conn).serve()

#       /usr/local/go/src/pkg/net/http/server.go:1167 +0xc00

I0707 10:51:01.149748 31033 logger.go:84] GET /foobar: (341.377us) 404
goroutine 58 [running]:
github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(_respLogger).WriteHeader(0xc2100ba900,
0x194)
 github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/logger.go:104
+0x10b
github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(_ApiServer).notFound(0xc2100bced0,
0xc2100398f0, 0x7fc077b08b00, 0xc2100ba900)

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:159
+0x70
github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*ApiServer).ServeHTTP(0xc2100bced0,
0x7fc077b08ac8, 0xc21000f500, 0xc2100398f0)

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:128
+0x77f
net/http/httptest.(_waitGroupHandler).ServeHTTP(0xc2100c3800,
0x7fc077b08ac8, 0xc21000f500, 0xc2100398f0)
/usr/local/go/src/pkg/net/http/httptest/server.go:200 +0xe0
net/http.serverHandler.ServeHTTP(0xc2100a9550, 0x7fc077b08ac8,
0xc21000f500, 0xc2100398f0)
/usr/local/go/src/pkg/net/http/server.go:1597 +0x1cb
net/http.(_conn).serve(0xc210059b80)
 /usr/local/go/src/pkg/net/http/server.go:1167 +0xc01
created by net/http.(_Server).Serve
/usr/local/go/src/pkg/net/http/server.go:1644 +0x2c1
I0707 10:51:01.152165 31033 logger.go:84] GET /prefix/version: (329.537us)
404
goroutine 65 [running]:
github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(_respLogger).WriteHeader(0xc21009f5a0,
0x194)
 github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/logger.go:104
+0x10b
github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*ApiServer).notFound(0xc21009b1b0,
0xc2100d4270, 0x7fc077b08b00, 0xc21009f5a0)

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:159
+0x70
github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*ApiServer).ServeHTTP(0xc21009b1b0,
0x7fc077b08ac8, 0xc21000f640, 0xc2100d4270)

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:135
+0x8fd
net/http/httptest.(_waitGroupHandler).ServeHTTP(0xc2100c3d60,
0x7fc077b08ac8, 0xc21000f640, 0xc2100d4270)
/usr/local/go/src/pkg/net/http/httptest/server.go:200 +0xe0
net/http.serverHandler.ServeHTTP(0xc2100a95f0, 0x7fc077b08ac8,
0xc21000f640, 0xc2100d4270)
/usr/local/go/src/pkg/net/http/server.go:1597 +0x1cb
net/http.(_conn).serve(0xc210059c80)
 /usr/local/go/src/pkg/net/http/server.go:1167 +0xc01
created by net/http.(_Server).Serve
/usr/local/go/src/pkg/net/http/server.go:1644 +0x2c1
I0707 10:51:01.154562 31033 logger.go:84] GET /prefix/version/foobar:
(330.374us) 404
goroutine 72 [running]:
github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(_respLogger).WriteHeader(0xc21009f900,
0x194)
 github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/logger.go:104
+0x10b
github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*ApiServer).notFound(0xc21009bf00,
0xc2100d4680, 0x7fc077b08b00, 0xc21009f900)

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:159
+0x70
github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*ApiServer).ServeHTTP(0xc21009bf00,
0x7fc077b08ac8, 0xc21000f820, 0xc2100d4680)

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/apiserver.go:149
+0xcf3
net/http/httptest.(_waitGroupHandler).ServeHTTP(0xc21009c320,
0x7fc077b08ac8, 0xc21000f820, 0xc2100d4680)
/usr/local/go/src/pkg/net/http/httptest/server.go:200 +0xe0
net/http.serverHandler.ServeHTTP(0xc2100a9690, 0x7fc077b08ac8,
0xc21000f820, 0xc2100d4680)
/usr/local/go/src/pkg/net/http/server.go:1597 +0x1cb
net/http.(_conn).serve(0xc210059d80)
 /usr/local/go/src/pkg/net/http/server.go:1167 +0xc01
created by net/http.(*Server).Serve
/usr/local/go/src/pkg/net/http/server.go:1644 +0x2c1

'foobar' has no storage object
I0707 10:51:01.157623 31033 logger.go:84] POST /prefix/version/foo:
(887.762us) 202
E0707 10:51:01.158720 31033 apiserver.go:233] Failed to parse:
&errors.errorString{s:""time: invalid duration not a timeout""} 'not a
timeout'
I0707 10:51:01.361860 31033 logger.go:84] POST
/prefix/version/foo?sync=true: (201.10636ms) 200
I0707 10:51:01.565104 31033 logger.go:84] POST

# /prefix/version/foo?sync=true&timeout=200ms: (200.864504ms) 202

WARNING: DATA RACE
Write by goroutine 113:

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*Operation).WaitFor()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/operation.go:119
+0x3c
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.func·013()
http://github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.func%C2%B7013()
      /home/thockin/src/kubernetes/output/go/src/
github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/operation_test.go:56
+0x61

Previous write by goroutine 108:

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*Operation).WaitFor()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/operation.go:119
+0x3c
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.TestOperation()
      /home/thockin/src/kubernetes/output/go/src/
github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/operation_test.go:61
+0x59e
  testing.tRunner()
      /usr/local/go/src/pkg/testing/testing.go:391 +0x10f

Goroutine 113 (running) created at:
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.TestOperation()
      /home/thockin/src/kubernetes/output/go/src/
github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/operation_test.go:58
+0x558
  testing.tRunner()
      /usr/local/go/src/pkg/testing/testing.go:391 +0x10f

Goroutine 108 (running) created at:
  testing.RunTests()
      /usr/local/go/src/pkg/testing/testing.go:471 +0xb3c
  testing.Main()
      /usr/local/go/src/pkg/testing/testing.go:403 +0xa2
  main.main()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/_testmain.go:125

# +0x19a

# PASS

WARNING: DATA RACE
Read by main goroutine:
  testing.coverReport()
      /usr/local/go/src/pkg/testing/cover.go:66 +0x43a
  testing.after()
      /usr/local/go/src/pkg/testing/testing.go:558 +0x8b6
  testing.Main()
      /usr/local/go/src/pkg/testing/testing.go:412 +0x1e9
  main.main()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/_testmain.go:125
+0x19a

Previous write by goroutine 122:

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.(*Operation).WaitFor()

github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/_test/operation.go:124
+0x119
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.func·013()
http://github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.func%C2%B7013()
      /home/thockin/src/kubernetes/output/go/src/
github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/operation_test.go:56
+0x61

Goroutine 122 (finished) created at:
  github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver.TestOperation()
      /home/thockin/src/kubernetes/output/go/src/
github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver/operation_test.go:58
+0x558
  testing.tRunner()

#       /usr/local/go/src/pkg/testing/testing.go:391 +0x10f

coverage: 66.4% of statements
Found 10 data race(s)
FAIL github.com/GoogleCloudPlatform/kubernetes/pkg/apiserver 2.055s

On Mon, Jul 7, 2014 at 10:46 AM, monnand notifications@github.com wrote:

> @thockin https://github.com/thockin Merging #348
> https://github.com/GoogleCloudPlatform/kubernetes/pull/348 would have
> fixed all detect-able races. But I'm not sure if any PR after #348
> https://github.com/GoogleCloudPlatform/kubernetes/pull/348 introduced
> new races. The currently master breaks the CI because of some time out
> error. I do not have if it is related to the races you mentioned.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/310#issuecomment-48212153
> .
"
806	310	monnand	2014-07-07 17:59:39	CONTRIBUTOR	"@thockin Aha, this is a quite familiar error. You are using go 1.2, right? There's a bug in go 1.2's race detector and I was bitten by it in #348. Use go 1.3 and it will be fixed.
"
807	360	thockin	2014-07-07 18:03:40	MEMBER	"Structured like:

type Selector struct {
    Requirements []Requirement
}

type Requirement struct {
    key string
   // comparator is something like an enum of exists, in, not in
    comparator Comparator
    // Only one of the following can be used at once.  Maybe use pointers?
 or some other union-like construct
    str_values []string
    int_values []int
    range_values []IntRange
}

So instead of saying

selector = ""x=<<foo$bar>>""

you would say

selector = Selector{
    Requirements: []Requirement{
        key: ""x""
        comparator: IN
        str_values: []string{""foo"", ""bar""}
    }
}

basically an AST, rather than a string to be parsed

On Mon, Jul 7, 2014 at 4:57 AM, meirf notifications@github.com wrote:

> @thockin https://github.com/thockin, thank you for your feedback
> 
> The original parsing used simple splitting so I tried to keep it. This
> forced a departure from using commas as a value separator in the set since
> the comma was already used a selector separator. Parsing might instead be
> done using regular expressions with capturing parentheses, but I agree that
> a different approach should be considered than this one-off
> encoding/parsing.
> 
> What type of structured message do you have in mind?
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/360#issuecomment-48169382
> .
"
808	310	thockin	2014-07-07 18:06:15	MEMBER	"Do we have an official statement of which versions of Go are supported and
not supported?  If 1.2 doesn't work, we need such a statement.  Better yet,
we need the tools to return an error, rather than try to run.

On Mon, Jul 7, 2014 at 10:59 AM, monnand notifications@github.com wrote:

> @thockin https://github.com/thockin Aha, this is a quite familiar
> error. You are using go 1.2, right? There's a bug in go 1.2's race detector
> and I was bitten by it in #348
> https://github.com/GoogleCloudPlatform/kubernetes/pull/348. Use go 1.3
> and it will be fixed.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/310#issuecomment-48214762
> .
"
809	345	thockin	2014-07-07 18:35:36	MEMBER	"I will post a hangout link on this thread.

Proposed topics:

API structure split: master vs slave
API versioning
IDs and Names

On Mon, Jul 7, 2014 at 6:29 AM, Clayton Coleman notifications@github.com
wrote:

> Let's try 2pm EDT on Tuesday
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/345#issuecomment-48177702
> .
"
810	361	brendandburns	2014-07-07 18:57:31	CONTRIBUTOR	"This is actually possible right now (on GCE at least), take a look at pkg/registry/minion_registry.go
"
811	158	rgarcia	2014-07-07 19:48:36	NONE	"It'd be great to have more directions for getting the example app running on a local cluster. I was able to get `hack/local-up-cluster.sh` working, but then failed to get the [guestbook example](https://github.com/GoogleCloudPlatform/kubernetes/blob/master/examples/guestbook/guestbook.md) up and running. `kubecfg.sh` failed due to `gcutil` missing, but even after installing that there were other gcutil-related errors.

Overall a frustrating kicking of the tires :-/
"
812	367	verdverm	2014-07-07 20:32:51	NONE	"I have noticed the FirstFit (default?) scheduler  co-locates pods when there are open machines available. Each of these machines has a single cpu.

It would be nice to use information about available cpu and a pod's expected cpu requirements

sed 's/cpu/other_machine_stat/'
"
813	367	monnand	2014-07-07 20:34:47	CONTRIBUTOR	"Currently, kubelet could get stats from cAdvisor which would be useful for scheduler. It could provide different percentiles of CPU and memory usage of a container (including the root container, i.e. the machine).
"
814	367	thockin	2014-07-07 20:35:04	MEMBER	"That's just ""scheduling"", as opposed to machine constraints, though very
coarsely they feel similar :)

On Mon, Jul 7, 2014 at 1:32 PM, Tony Worm notifications@github.com wrote:

> I have noticed the FirstFit (default?) scheduler co-locates pods when
> there are open machines available. Each of these machines has a single cpu.
> 
> It would be nice to use information about available cpu and a pod's
> expected cpu requirements
> 
> sed 's/cpu/other_machine_stat/'
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/367#issuecomment-48236654
> .
"
815	367	timothysc	2014-07-07 20:42:42	MEMBER	"Labels ""seems"" like the ideal place to enable a rank & requirements to define constraints.  However labels would need to be regularly published by minions. 

e.g.
rank = memory
requirements = gpu & clusterXYZ

I have a couple of concerns here: 
1. This treads into the full scale scheduling world. 
2. Config syntax = ?, DSL? ... 
"
816	367	thockin	2014-07-07 20:47:54	MEMBER	"Let's worry about semantics before syntax.  We have a similar issue open
for label selectors in general - we can discuss syntax there.

On Mon, Jul 7, 2014 at 1:42 PM, Timothy St. Clair notifications@github.com
wrote:

> Labels ""seems"" like the ideal place to enable a rank & requirements to
> define constraints. For example:
> 
> rank = memory
> requirements = gpu & clusterXYZ
> 
> I have a couple of concerns here:
> 1. This treads into the full scale scheduling world.
> 2. Config syntax = ?, DSL? ...
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/367#issuecomment-48237875
> .
"
817	367	timothysc	2014-07-07 20:56:12	MEMBER	"FWIW I often view constraints as a SQL query on a nvp store. 

SELECT Resources 
FROM Pool 
WHERE Requirements
ORDER BY Rank 
...

The hardest part are the 'fields' in an nvp store. 
"
818	366	brendandburns	2014-07-07 21:09:55	CONTRIBUTOR	"Ok, I think this is ready to merge.  I had to cut down the number of iterations to 5 to make it pass reliably (I hope..)

Let's get this in and get back to green.

Thanks!
--brendan
"
819	317	bgrant0607	2014-07-07 21:28:14	MEMBER	"See also #160 . @vmarmol 
"
820	317	vmarmol	2014-07-07 21:30:41	CONTRIBUTOR	"@dchen1107 is doing some work in this area as 
"
821	367	bgrant0607	2014-07-07 21:34:12	MEMBER	"Scheduling based on resources and constraints are 2 significantly different issues.

We have several issues open about resource (and QoS) awareness: #147 , #160 , #168 , #274 , #317.

Constraint syntax/semantics: We should start with the proposed label selector mechanism, #341 .
"
822	366	monnand	2014-07-07 22:32:49	CONTRIBUTOR	"Is it possible to run race detector for integration test as well? I think we could use either `go build -race` or `go run -race`.
"
823	160	monnand	2014-07-07 22:40:25	CONTRIBUTOR	"Currently, #328 and #174 could let kubelet retrieve some basic stats info from cAdvisor. On cAdvisor side, google/cadvisor#74 could put all stats into [influxdb](http://influxdb.com). 

If we want to display the data, we could write a separate UI program to retrieve stats from the backend storage (There is only [influxdb](http://influxdb.com) now) directly (reading the influxdb) or indirectly (retrieving from cAdvisor). Or, we could put this feature into the api server.
"
824	363	bgrant0607	2014-07-08 04:42:33	MEMBER	"Yes, I can take this one.
"
825	358	thockin	2014-07-08 04:50:11	MEMBER	"Ping - 3 commits for 3 logical changes.
"
826	368	brendandburns	2014-07-08 04:53:56	CONTRIBUTOR	"What is necessary to activate this, is it a kernel parameter?  Do we need to build in a different module?

Thanks
--brendan
"
827	369	brendandburns	2014-07-08 04:56:50	CONTRIBUTOR	"Thanks for the PR!

It actually had an unexpected effect of making me realize this section is no longer necessary and should be deleted, we now supply an htpasswd tool in ./third_party/

Any chance you could update this PR to delete that whole section.

And also, can you sign our CLA as described in CONTRIB.md

Thanks!
--brendan
"
828	358	brendandburns	2014-07-08 05:03:24	CONTRIBUTOR	"Small stuff.  Largely a preference for for-each style loops:

for _, obj := range foo {

vs

for ix := range foo {
  obj := foo[ix]
  ...
}
"
829	167	philips	2014-07-08 15:43:02	CONTRIBUTOR	"@jbeda FWIW, read replicas are part of the motivation for our work on an updated raft implementation. They are coming after we get that work in: https://github.com/coreos/etcd/pull/874
"
830	372	smarterclayton	2014-07-08 17:00:23	CONTRIBUTOR	"LGTM
"
831	167	discordianfish	2014-07-08 17:24:34	CONTRIBUTOR	"Do you guys have interest in anything else from this PR? I still would like to see a easy way to deploy kubernetes as docker images itself. Happy to help if you have concrete suggestions.
"
832	157	brendandburns	2014-07-08 17:42:27	CONTRIBUTOR	"@roberthbailey will handle this one.
"
833	345	thockin	2014-07-08 17:53:45	MEMBER	"https://plus.google.com/hangouts/_/grkrl7elarpergdg6ksepxjclea

I hope this works - never set up a public hangout before.

On Mon, Jul 7, 2014 at 11:35 AM, Tim Hockin thockin@google.com wrote:

> I will post a hangout link on this thread.
> 
> Proposed topics:
> 
> API structure split: master vs slave
> API versioning
> IDs and Names
> 
> On Mon, Jul 7, 2014 at 6:29 AM, Clayton Coleman notifications@github.com
> wrote:
> 
> > Let's try 2pm EDT on Tuesday
> > 
> > ## 
> > 
> > Reply to this email directly or view it on GitHub
> > https://github.com/GoogleCloudPlatform/kubernetes/pull/345#issuecomment-48177702
> > .
"
834	345	verdverm	2014-07-08 18:01:35	NONE	"Link says the party is over
"
835	345	thockin	2014-07-08 18:04:22	MEMBER	"try

https://plus.google.com/hangouts/_/g6eqprge2izsz3xmvubuajky2ua

On Tue, Jul 8, 2014 at 11:01 AM, Tony Worm notifications@github.com wrote:

> Link says the party is over
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/345#issuecomment-48376615
> .
"
836	341	timothysc	2014-07-08 18:16:54	MEMBER	"If folks intend to use labels for constraint matching, there will likely need to be comparison operators. 
"
837	147	timothysc	2014-07-08 18:33:50	MEMBER	"How does QoS tiers relate to resource requests?
"
838	168	timothysc	2014-07-08 19:47:30	MEMBER	"+1.  
"
839	274	timothysc	2014-07-08 19:50:46	MEMBER	"imho appending cAdvisor stats as a ""Kublet Label"" would provide uniform mechanics for viewing resources as well as constraint matching.  
"
840	160	timothysc	2014-07-08 19:54:27	MEMBER	"Why can't the kublet just regularly send a 'nvp thunk' including cAdvisor data to a store which the scheduler can then use well established variables for initial algorithm, then extra discrimination via constraint. 
"
841	372	brendandburns	2014-07-08 19:55:38	CONTRIBUTOR	"Thanks for the clean up.

Closes #35 
"
842	358	brendandburns	2014-07-08 19:58:24	CONTRIBUTOR	"Ok, I'm ok w/ leaving the iteration as is.  (are you certain the compiler is eliding the copy in this case, anyway?  I mean you're calling range ... which presumably has a multi-value return, even if you're dropping one of the values.

but LGTM, rebase and I'm ok to submit.
"
843	187	derekwaynecarr	2014-07-08 20:15:14	MEMBER	"I am going to try and take a look at this.
"
844	345	thockin	2014-07-08 20:15:37	MEMBER	"Notes from this discussion:
- API structure split: master vs slave

We all agreed in general that we should differentiate top-level API
structures between the API server and the Kubelet.  Common structures like
ContainerManifest will stay common and shared, but (for example) there
probably needs to be a different ""Pod"" structure for master and slave.

Google will find someone to work on this issue.
- API versioning

We all agreed in general that we need to start versioning the APIs and
having an explicit point in the processing logic where we convert from
versioned external structures to internal structures.  We will try to avoid
explicitly versioning internal structures for simplicity, but nobody was
deeply against it, if need be.  If we find ourselves with truly
incompatible behavior, we will need to introduce them carefully with
explicit notes of API breakage.

Nobody volunteered to work on this, so Google will find someone if possible
- IDs and names

We agreed on the following:
- ""Pod ID"" that exists for the lifetime of the pod in k8s
- ""Pod Instance ID"" that exists for the lifetime of a pod on a host
- ""Container attempt ID"" that is assigned to each run of a container
  (docker's ID)

We did not discuss, but previously agreed on:
- ""Pod namespace"" that represents the origin of the pod
- ""Pod name""
- ""Container name""

We agreed to not have the following:
- A generation number that changes on stop/start cycles.

I'll write this up more fully in PR #349.  Clayton is working on this.

Thanks everyone.  way faster than email.

Tim

On Mon, Jul 7, 2014 at 11:35 AM, Tim Hockin thockin@google.com wrote:

> I will post a hangout link on this thread.
> 
> Proposed topics:
> 
> API structure split: master vs slave
> API versioning
> IDs and Names
> 
> On Mon, Jul 7, 2014 at 6:29 AM, Clayton Coleman notifications@github.com
> wrote:
> 
> > Let's try 2pm EDT on Tuesday
> > 
> > ## 
> > 
> > Reply to this email directly or view it on GitHub
> > https://github.com/GoogleCloudPlatform/kubernetes/pull/345#issuecomment-48177702
> > .
"
845	365	brendandburns	2014-07-08 20:16:42	CONTRIBUTOR	"Comments addressed.  Please re-check.

Thanks!
--brendan
"
846	187	brendandburns	2014-07-08 20:29:33	CONTRIBUTOR	"Cool, thanks!

--brendan
"
847	349	thockin	2014-07-08 21:48:51	MEMBER	"@smarterclayton cleaned up after today's chat.  Still two FIXMEs about loosening requirements - I think I know which side you fall on, but I want to confirm and clarify.
"
848	358	thockin	2014-07-08 22:25:00	MEMBER	"I found a commit to the go compiler to elide the copy in the one-return case, but all discussions around eliding it entirely devolve into ""just iterate by index"" or someone proposing a language change which goes nowhere. :(  This seems like an opportunity for the language to be highly efficient, and it's not.

Rebased
"
849	349	thockin	2014-07-08 22:51:26	MEMBER	"re: name vs ID and autogen.  ACK.  Do you think the auto-generated name has to be deterministic?  Or is random sufficient?  I think random is good enough.  If you didn't care enough to specify the name to apiserver, you get something random.  Very much like docker itself.  This does not provide idempotency, but it's hard to say what idempotency means here.  If the user specified neither a name nor an ID, how do we divine their intentions?

Now, Kubelet gets value out of making it deterministic because we can assume that the same pod spec from the same file means the same thing, with or without a name or ID.

new commit added - please take a look.  If happy I can squash before merge.
"
850	349	smarterclayton	2014-07-08 22:53:25	CONTRIBUTOR	"LGTM
"
851	349	thockin	2014-07-08 23:39:31	MEMBER	"Will wait for 2nd LGTM from Brendan or someone else.

On Tue, Jul 8, 2014 at 3:53 PM, Clayton Coleman notifications@github.com
wrote:

> LGTM
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/349#issuecomment-48409706
> .
"
852	377	monnand	2014-07-08 23:44:29	CONTRIBUTOR	"LGTM. Anyone wants to review and probably merge it? cc/ @thockin @brendanburns 
"
853	377	vmarmol	2014-07-08 23:45:53	CONTRIBUTOR	"LGTM
"
854	341	bgrant0607	2014-07-09 00:21:36	MEMBER	"@timothysc Could you please provide a motivating example for comparisons? We could accept integer ranges, including open ranges. However, the restrictive selection semantics are careful chosen to facilitate set overlap detection, efficient indexing and reverse indexing, and human understandability.
"
855	341	timothysc	2014-07-09 00:45:29	MEMBER	"It is my understanding that Labels 'currently' only apply to pods.  If labeling were extended to Kublets then users could define things such as: 

rank = memory 
(implies sort, meaning < operator required) 

requirements = memory > 4GB 
(Only land my pod on machines with > 4GB) 
"
856	365	bgrant0607	2014-07-09 00:48:46	MEMBER	"Good enough for a first cut. Thanks.
"
857	147	bgrant0607	2014-07-09 00:54:53	MEMBER	"@timothysc 

At the node level, QoS would be specified together with resource requests to the execution layer, such as an exec driver over lmctfy (which might eventually be layered on libcontainer).

At the scheduling level, a variety of policies are possible. We could start with the naive approach of not considering QoS in scheduling. Eventually we could support some kind of overcommitment for lower QoS tiers.
"
858	341	bgrant0607	2014-07-09 01:27:44	MEMBER	"Ok, thanks. As I commented in #367 , labels / constraints shouldn't be used for resource-based placement or binpacking. The scheduler should place pods based on their minimum resource requirements and resources available.
"
859	341	timothysc	2014-07-09 01:36:40	MEMBER	"You could also view it as 

requirements = distance(podX) < 3  
"
860	341	bgrant0607	2014-07-09 02:03:39	MEMBER	"Labels are explicit and literal.

Locality requirements need to be expressed at a higher level. Expressing them as individual hard constraints on pods is likely to lead to a greedy scheduler to paint itself into a corner.

Scheduling is a nuanced service, with workload-specific and provider-specific objectives and constraints, topological and architectural considerations, security concerns, dependencies, QoS and fairness considerations, workload interference, etc. Let's not try to shoehorn too much into the label mechanism.
"
861	341	thockin	2014-07-09 02:07:52	MEMBER	"I really think scheduling resources and scheduling constraints are two
different things, both useful, but not the same.

For your other example, distance to another pod, I would also not suggest
labels.  Labels are relatively static and low in cardinality.  Asking for
distance to another pod (for some definition of distance) may be a legit
constrainy but doesn't sound like labels to me.

I also started with the expectation that label selectors would be an
arbitrary expression, including regex matching or prefix/suffix matching.
But I did not come up with a solid use case for more than simple sets.

Tim
On Jul 8, 2014 5:45 PM, ""Timothy St. Clair"" notifications@github.com
wrote:

> It is my understanding that Labels 'currently' only apply to pods. If
> labeling were extended to Kublets then users could define things such as:
> 
> rank = memory
> (implies sort, meaning < operator required)
> 
> requirements = memory > 4GB
> (Only land my pod on machines with > 4GB)
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/341#issuecomment-48417431
> .
"
862	341	timothysc	2014-07-09 02:13:45	MEMBER	"@thockin ok I'll buy the idea of not trying to overload labels.  
I'll try to hash it out on #367 then. 
"
863	341	bgrant0607	2014-07-09 02:14:59	MEMBER	"Regular expressions and prefix/suffix matching are hostile to the goals I mentioned earlier. We should not support them. The escape hatch is attaching a new label using whatever arbitrary computation you wish to decide where to attach it, or to dump the data into a database and use SQL or whatever.
"
864	341	timothysc	2014-07-09 02:21:53	MEMBER	"I proposed the SQL method on NVP sets earlier today.
"
865	341	thockin	2014-07-09 02:45:37	MEMBER	"I am behind on email, so I promise to digest the other threads tonight :)
On Jul 8, 2014 7:22 PM, ""Timothy St. Clair"" notifications@github.com
wrote:

> I proposed the SQL method on NVP sets earlier today.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/341#issuecomment-48422790
> .
"
866	367	timothysc	2014-07-09 02:45:44	MEMBER	"I'm ok with doing the selection from a set of offers/resources from the scheduler.  

Provided the offers have enough NVP information to enable discrimination.
"
867	367	thockin	2014-07-09 02:51:28	MEMBER	"I don't know about NVP - where can I read more on it?
On Jul 8, 2014 7:45 PM, ""Timothy St. Clair"" notifications@github.com
wrote:

> I'm ok with doing the selection from a set of offers/resources from the
> scheduler.
> 
> Provided the offers have enough NVP information to enable discrimination.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/367#issuecomment-48424027
> .
"
868	367	bgrant0607	2014-07-09 03:16:40	MEMBER	"Searching for ""NVP SQL"" or ""name value pair SQL"" or ""key value pair SQL"" comes up with lots of hits. Common arguments against are performance and loss of control over DB schema. But I'm getting the feeling that we're barking up the wrong forest.

@timothysc What are you trying to do? Right now, k8s has essentially no intelligent scheduling. However, that's not a desirable end state. If what you want is a scheduler, we should figure out how to support scheduling plugins and/or layers on top of k8s.
"
869	160	monnand	2014-07-09 03:27:51	CONTRIBUTOR	"@timothysc cAdvisor now could dump all stats into [influxdb](http://influxdb.com). Kubelet will retrieve a summary of such stats from cAdvisor and in turn pulled by master to do scheduling decisions. I think its similar to what you described?
"
870	367	thockin	2014-07-09 03:32:31	MEMBER	"Name Value Pairs?   Now I feel dumb :)

On Tue, Jul 8, 2014 at 7:51 PM, Tim Hockin thockin@google.com wrote:

> I don't know about NVP - where can I read more on it?
> On Jul 8, 2014 7:45 PM, ""Timothy St. Clair"" notifications@github.com
> wrote:
> 
> > I'm ok with doing the selection from a set of offers/resources from the
> > scheduler.
> > 
> > Provided the offers have enough NVP information to enable discrimination.
> > 
> > ## 
> > 
> > Reply to this email directly or view it on GitHub
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/367#issuecomment-48424027
> > .
"
871	367	bgrant0607	2014-07-09 04:08:57	MEMBER	"Something somewhat different than label selectors is per-attribute limits for spreading. Aurora is one system that supports this model:
https://aurora.incubator.apache.org/documentation/latest/configuration-reference/#specifying-scheduling-constraints

This is more relevant to physical rather than virtual deployments. I'd consider it a distinct mechanism from constraints. @timothysc If you'd like this, we should file a separate issue. However, I'd prefer a a new failure tolerance scheduling policy object that specifies a label selector to identify the set of instances to be spread. We could debate about how to describe what kind and/or how much spreading to apply, but I'd initially just leave it entirely up to the infrastructure. 
"
872	378	brendandburns	2014-07-09 04:45:53	CONTRIBUTOR	"I'm open to using this, although I don't necessarily see the value, but I'd like to understand exactly what the workflow does.

Can you add some documentation on how it is used (and how you would update dependencies) to the developer section of the README.md?

Thanks!
--brendan
"
873	375	brendandburns	2014-07-09 04:48:43	CONTRIBUTOR	"LGTM.
"
874	365	brendandburns	2014-07-09 04:51:47	CONTRIBUTOR	"Comments addressed.  ptal.

Thanks!
--brendan
"
875	35	brendandburns	2014-07-09 05:01:46	CONTRIBUTOR	"This is closed by #372 
"
876	365	bgrant0607	2014-07-09 05:16:20	MEMBER	"Thanks. I'm having some caching troubles with Github. Just try to ensure
that the API example field names match types.go (e.g. livenessProbe vs.
liveProbe). It would be nice to update the schema and regenerate the html,
also, but that could be a separate PR if you prefer.
"
877	365	brendandburns	2014-07-09 05:22:01	CONTRIBUTOR	"ok, fixed (again ;)
"
878	302	brendandburns	2014-07-09 05:23:48	CONTRIBUTOR	"The problem I saw was with watches hanging and not getting new data.  I suppose we could just timeout the watch, and then issue a new watch with the old last modified, and we'd likely get the same behavior.
"
879	349	thockin	2014-07-09 14:51:40	MEMBER	"rebased and squashed
"
880	371	thockin	2014-07-09 14:57:59	MEMBER	"rebased
"
881	367	timothysc	2014-07-09 14:58:12	MEMBER	"I completely agree its more relevant to physical rather then virtual deployments.

I was somewhat testing the possibility of enabling the capabilities for more general purpose scheduling, on par with a mini-Condor approach but it's not a requirement.  

Aurora or Marathon -esk capabilities will fill the gap. 
https://github.com/mesosphere/marathon/wiki/Constraints
"
882	379	thockin	2014-07-09 15:37:17	MEMBER	"This is sort of annoying, but can you fix the typo in the comment?   s/up/IP/ ?  LGTM otherwise.
"
883	66	bgrant0607	2014-07-09 15:58:44	MEMBER	"FWIW, here's a description of Marathon's liveness checks:
https://github.com/mesosphere/marathon/wiki/Health-Checks

HTTP responses between 200-399 are considered live. The max # of consecutive failures is configurable (as with GCE's LB readiness checks). 

Aurora's are similar:
http://aurora.incubator.apache.org/documentation/latest/configuration-tutorial/
"
884	381	thockin	2014-07-09 16:00:00	MEMBER	"Thanks for all the recent PRs and issues!  Have you signed Google's CLA, as
described in https://github.com/thockin/kubernetes/blob/master/CONTRIB.md ?
 I don't see your name on the list, but maybe I am missing it...

On Tue, Jul 8, 2014 at 11:05 PM, Yuki Yugui Sonoda <notifications@github.com

> wrote:
> 
> ---
> 
> You can merge this Pull Request by running
> 
>   git pull https://github.com/yugui/kubernetes feature/api-scope
> 
> Or view, comment on, or merge it at:
> 
>   https://github.com/GoogleCloudPlatform/kubernetes/pull/381
> Commit Summary
> - Allows adding custom api scopes to service accounts available in
> 
> File Changes
> - _M_ cluster/config-default.sh
>   https://github.com/GoogleCloudPlatform/kubernetes/pull/381/files#diff-0
>   (1)
> - _M_ cluster/config-test.sh
>   https://github.com/GoogleCloudPlatform/kubernetes/pull/381/files#diff-1
>   (2)
> - _M_ cluster/kube-up.sh
>   https://github.com/GoogleCloudPlatform/kubernetes/pull/381/files#diff-2
>   (2)
> 
> Patch Links:
> - https://github.com/GoogleCloudPlatform/kubernetes/pull/381.patch
> - https://github.com/GoogleCloudPlatform/kubernetes/pull/381.diff
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/381.
"
885	368	vmarmol	2014-07-09 16:37:02	CONTRIBUTOR	"Just to update the Grub config and reboot.
"
886	382	brendandburns	2014-07-09 16:57:44	CONTRIBUTOR	"Raphael,
Thanks for the PR!  Have you signed our CLA, its described in [CONTRIB.md](https://github.com/GoogleCloudPlatform/kubernetes/blob/master/CONTRIB.md), once that's signed, I can merge this in.

Thanks again.
--brendan
"
887	368	brendandburns	2014-07-09 17:12:37	CONTRIBUTOR	"Ok, we need to do this in the image build stage, so that it is built into the container VM image.

--brendan
"
888	368	brendandburns	2014-07-09 17:14:36	CONTRIBUTOR	"@dchen1107 , are you still looking to build an image?
"
889	368	jkaplowitz	2014-07-09 17:16:01	CONTRIBUTOR	"It's already there for the container VM image, either in the currently live
one or at least the build tree for future builds. @vmarmol clarified to me
that his request was about our vanilla GCE images, or at least some build
where customers would be following standard Kubernetes install instructions
rather than having Kubelet preinstalled.

Ok, we need to do this in the image build stage, so that it is built into
the container VM image.

--brendan

—
Reply to this email directly or view it on GitHub
https://github.com/GoogleCloudPlatform/kubernetes/issues/368#issuecomment-48504604
.
"
890	378	roberthbailey	2014-07-09 17:17:41	MEMBER	"Out of curiosity (not having fully grocked the update-deps shell script or your PR), have you considered using https://github.com/kr/goven instead of godep?
"
891	368	vmarmol	2014-07-09 17:21:00	CONTRIBUTOR	"I think this boils down to changing the default image from `backports-debian-7-wheezy` to the containers-vm image or something else we can have the cgroup enabled. I can try that and see how it goes (unless someone thinks its a terrible idea).
"
892	368	brendandburns	2014-07-09 17:21:29	CONTRIBUTOR	"SGTM, we were headed there anyway as part of the desire to run all of the Kubernetes binaries inside of containers.
"
893	368	jkaplowitz	2014-07-09 17:24:45	CONTRIBUTOR	"So this would be a default for our container-optimized docs/examples with
Kubelet itself moved into a container? If so, SGTM from me as well. If you
mean a GCE-wide default beyond container-centric contexts, that would need
more discussion outside of this GitHub issue.
On Jul 9, 2014 10:21 AM, ""brendandburns"" notifications@github.com wrote:

> SGTM, we were headed there anyway as part of the desire to run all of the
> Kubernetes binaries inside of containers.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/368#issuecomment-48505766
> .
"
894	368	vmarmol	2014-07-09 17:26:09	CONTRIBUTOR	"Just default for the Kubernetes scripts for now :) the examples are next and I think we stop there.
"
895	368	jkaplowitz	2014-07-09 17:26:48	CONTRIBUTOR	"SGTM.
On Jul 9, 2014 10:26 AM, ""Victor Marmol"" notifications@github.com wrote:

> Just default for the Kubernetes scripts for now :) the examples are next
> and I think we stop there.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/368#issuecomment-48506399
> .
"
896	378	monnand	2014-07-09 18:02:15	CONTRIBUTOR	"@brendandburns Sure. I can add a section in the README.md.
@roberthbailey I did considered goven, however, it will change import paths, which is not desirable.
"
897	379	brendandburns	2014-07-09 18:17:58	CONTRIBUTOR	"done.  ptal

thanks
--brendan
"
898	378	roberthbailey	2014-07-09 18:18:34	MEMBER	"@monnand thanks, just wanted to make sure it'd been considered, since it also seems designed to solve the dependency problem.
"
899	381	brendandburns	2014-07-09 18:19:09	CONTRIBUTOR	"Yuki is a Googler (in Tokyo?) (in the future, you can tell since she's in the Google Cloud github org)
"
900	378	monnand	2014-07-09 18:25:48	CONTRIBUTOR	"@roberthbailey Thank you!  I did a brief survey on dependency management in Go before starting this PR. It's a topic still under hot debate. [This list](https://code.google.com/p/go-wiki/wiki/PackageManagementTools) is a good starting point. I choose godep because it does exactly what we did using shell scripts and requires least change. 
"
901	383	brendandburns	2014-07-09 18:47:37	CONTRIBUTOR	"LGTM, this is good for a first fix.  @thockin ?
"
902	378	monnand	2014-07-09 18:52:43	CONTRIBUTOR	"@brendandburns Added htpasswd back to third_party and changed README.md. PTAL.
"
903	365	brendandburns	2014-07-09 19:19:29	CONTRIBUTOR	"ok, re-added the test (which somehow disappeared), validated things, ptal.

Thanks!
--brendan
"
904	383	thockin	2014-07-09 19:32:58	MEMBER	"Thanks!
"
905	381	thockin	2014-07-09 19:35:32	MEMBER	"OH!  My mistake.  Thanks!

On Wed, Jul 9, 2014 at 11:19 AM, brendandburns notifications@github.com
wrote:

> Merged #381 https://github.com/GoogleCloudPlatform/kubernetes/pull/381.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/381#event-139843797
> .
"
906	369	jdef	2014-07-09 20:30:12	CONTRIBUTOR	"Looks like README.md in the master branch is already cleaned up. Given that, there's probably no need for this PR
"
907	368	verdverm	2014-07-09 20:33:48	NONE	"Tried the  container-vm like this:

in config-default.sh

```
IMAGE=https://www.googleapis.com/compute/v1/projects/google-containers/global/images/container-vm-v20140624
```

VMs booted, so `docker ps`

```
tony@kube-minion-1:~$ sudo docker ps
2014/07/09 20:24:20 Error response from daemon: client and server don't have same version (client : 1.13, server: 1.12)
```

Unable to bring up a replicationController
"
908	368	jkaplowitz	2014-07-09 20:43:47	CONTRIBUTOR	"Hm. We released that with Docker 1.0.1. I hope there's not already an
incompatibility with 1.1.0 less than a month after 1.0.1 was released.

On Wed, Jul 9, 2014 at 1:33 PM, Tony Worm notifications@github.com wrote:

> Tried the container-vm like this:
> 
> in config-default.sh
> 
> IMAGE=https://www.googleapis.com/compute/v1/projects/google-containers/global/images/container-vm-v20140624
> 
> VMs booted, so docker ps
> 
> tony@kube-minion-1:~$ sudo docker ps
> 2014/07/09 20:24:20 Error response from daemon: client and server don't have same version (client : 1.13, server: 1.12)
> 
> Unable to bring up a replicationController
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/368#issuecomment-48530336
> .
"
909	368	vmarmol	2014-07-09 20:45:35	CONTRIBUTOR	"In general, Docker expects the CLI and daemon to be the same version which is the error here. Somehow those are different in the Kubernetes setup.
"
910	368	jkaplowitz	2014-07-09 20:47:58	CONTRIBUTOR	"There shouldn't be any Docker version skew within the image as shipped. If
the Kubernetes setup scripts in our GitHub tree involve reinstalling Docker
in possibly a different way, I could see potential issues to fix there.

On Wed, Jul 9, 2014 at 1:45 PM, Victor Marmol notifications@github.com
wrote:

> In general, Docker expects the CLI and daemon to be the same version which
> is the error here. Somehow those are different in the Kubernetes setup.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/368#issuecomment-48531616
> .
"
911	368	brendandburns	2014-07-09 20:48:34	CONTRIBUTOR	"Hrm, that seems highly unlikely.  The Kubernetes set up uses
http://get.docker.io so unless something's borked there I don't see how it
could happen.

(unless cAdvisor ships with a docker client?)

--brendan

On Wed, Jul 9, 2014 at 1:45 PM, Victor Marmol notifications@github.com
wrote:

> In general, Docker expects the CLI and daemon to be the same version which
> is the error here. Somehow those are different in the Kubernetes setup.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/368#issuecomment-48531616
> .
"
912	368	vmarmol	2014-07-09 20:50:00	CONTRIBUTOR	"I think the image brings up the Docker daemon (v 1.0.1) before Kubernetes installs the new Docker image (v 1.1.0). So the CLI invocations are of the new kind with the old client. Kubernetes should not need to install Docker if it is already installed.
"
913	368	brendandburns	2014-07-09 20:50:15	CONTRIBUTOR	"Ah, I missed that you had tried by switching to container vm.

Can you disable the docker install on the minion?

Go into cluster/saltbase/salt/top.sls and comment out 'docker' for the
'kubernetes-pool' section.  That will disable the docker install, since it
is already present in the container VM.

--brendan

On Wed, Jul 9, 2014 at 1:48 PM, jkaplowitz notifications@github.com wrote:

> There shouldn't be any Docker version skew within the image as shipped. If
> the Kubernetes setup scripts in our GitHub tree involve reinstalling
> Docker
> in possibly a different way, I could see potential issues to fix there.
> 
> On Wed, Jul 9, 2014 at 1:45 PM, Victor Marmol notifications@github.com
> wrote:
> 
> > In general, Docker expects the CLI and daemon to be the same version
> > which
> > is the error here. Somehow those are different in the Kubernetes setup.
> > 
> > —
> > Reply to this email directly or view it on GitHub
> > <
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/368#issuecomment-48531616>
> > 
> > .
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/368#issuecomment-48531881
> .
"
914	368	brendandburns	2014-07-09 20:50:35	CONTRIBUTOR	"You will need to ./cluster/kube-down.sh and re-create your cluster.

--brendan

On Wed, Jul 9, 2014 at 1:49 PM, Brendan Burns bburns@google.com wrote:

> Ah, I missed that you had tried by switching to container vm.
> 
> Can you disable the docker install on the minion?
> 
> Go into cluster/saltbase/salt/top.sls and comment out 'docker' for the
> 'kubernetes-pool' section.  That will disable the docker install, since it
> is already present in the container VM.
> 
> --brendan
> 
> On Wed, Jul 9, 2014 at 1:48 PM, jkaplowitz notifications@github.com
> wrote:
> 
> > There shouldn't be any Docker version skew within the image as shipped.
> > If
> > the Kubernetes setup scripts in our GitHub tree involve reinstalling
> > Docker
> > in possibly a different way, I could see potential issues to fix there.
> > 
> > On Wed, Jul 9, 2014 at 1:45 PM, Victor Marmol notifications@github.com
> > wrote:
> > 
> > > In general, Docker expects the CLI and daemon to be the same version
> > > which
> > > is the error here. Somehow those are different in the Kubernetes setup.
> > > 
> > > —
> > > Reply to this email directly or view it on GitHub
> > > <
> > > https://github.com/GoogleCloudPlatform/kubernetes/issues/368#issuecomment-48531616>
> > > 
> > > .
> > 
> > —
> > Reply to this email directly or view it on GitHub
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/368#issuecomment-48531881
> > .
"
915	368	jkaplowitz	2014-07-09 20:51:14	CONTRIBUTOR	"Yup, that seems to be the issue there.

On Wed, Jul 9, 2014 at 1:50 PM, brendandburns notifications@github.com
wrote:

> Ah, I missed that you had tried by switching to container vm.
> 
> Can you disable the docker install on the minion?
> 
> Go into cluster/saltbase/salt/top.sls and comment out 'docker' for the
> 'kubernetes-pool' section. That will disable the docker install, since it
> is already present in the container VM.
> 
> --brendan
> 
> On Wed, Jul 9, 2014 at 1:48 PM, jkaplowitz notifications@github.com
> wrote:
> 
> > There shouldn't be any Docker version skew within the image as shipped.
> > If
> > the Kubernetes setup scripts in our GitHub tree involve reinstalling
> > Docker
> > in possibly a different way, I could see potential issues to fix there.
> > 
> > On Wed, Jul 9, 2014 at 1:45 PM, Victor Marmol notifications@github.com
> > 
> > wrote:
> > 
> > > In general, Docker expects the CLI and daemon to be the same version
> > > which
> > > is the error here. Somehow those are different in the Kubernetes
> > > setup.
> > > 
> > > —
> > > Reply to this email directly or view it on GitHub
> > > <
> > 
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/368#issuecomment-48531616>
> > 
> > > .
> > 
> > —
> > Reply to this email directly or view it on GitHub
> > <
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/368#issuecomment-48531881>
> > 
> > .
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/368#issuecomment-48532156
> .
"
916	368	verdverm	2014-07-09 21:10:40	NONE	"ok, `docker ps -a` works, but there's nothing there, on any of the minions

I started a controller, which gets registered with k8s, 1 of 4 pods started in the output.
but still nothing there

```
Services
=====================
Name                Labels              Selector            Port
----------          ----------          ----------          ----------

Controllers
=====================
Name                Image(s)                     Selector            Replicas
----------          ----------                   ----------          ----------
algebra             23.251.148.42:5000/algebra   name=algebra        4

Pods
=====================
Name                Image(s)                     Host                                  Labels
----------          ----------                   ----------                            ----------
9acb0442            23.251.148.42:5000/algebra   kube-minion-3.c.cloud-pge.internal/   name=algebra,replicationController=algebra
```

I also noticed this at the end of the dev-build-and-up.sh

```
Security note: The server above uses a self signed certificate.  This is
    subject to ""Man in the middle"" type attacks.
fatal: Not a git repository (or any parent up to mount point /home)
Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).
```
"
917	368	dchen1107	2014-07-09 21:40:33	MEMBER	"I will build a new container vm image with docker 1.1. Should that mitigate the issue here? 
"
918	368	vmarmol	2014-07-09 21:42:28	CONTRIBUTOR	"@dchen1107 I think at this point it is just working through the issues of being on a different image. They should be similar enough to get it to work.

@verdverm not sure about the last error, but is there anything in the Kubelet log to say why the other 3 haven't come up? They make take some time depending on the size of the Docker image.
"
919	368	jkaplowitz	2014-07-09 21:43:01	CONTRIBUTOR	"Only until the next build comes out. Really the fix is to have kubernetes's
setup not install docker if it's already installed, and possibly to replace
any existing kubelet install that the container vm ships with as part of
the build you're installing.

On Wed, Jul 9, 2014 at 2:40 PM, Dawn Chen notifications@github.com wrote:

> I will build a new container vm image with docker 1.1. Should that
> mitigate the issue here?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/368#issuecomment-48538704
> .
"
920	365	bgrant0607	2014-07-09 21:59:01	MEMBER	"LGTM
"
921	365	smarterclayton	2014-07-09 22:03:01	CONTRIBUTOR	"LGTM
"
922	368	brendandburns	2014-07-09 22:08:04	CONTRIBUTOR	"The fact that they aren't in the pod list means that the master never got
the request to schedule them.

My guess is that your cluster only has a single machine, and your pod is
exposing an external port, so the master is refusing to schedule two pods
onto the same machine since the ports will conflict (we need to add a
'pending' state to handle these sorts of situations in the UX...)

--brendan

On Wed, Jul 9, 2014 at 2:10 PM, Tony Worm notifications@github.com wrote:

> ok, docker ps -a works, but there's nothing there, on any of the minions
> 
> I started a controller, which gets registered with k8s, 1 of 4 pods
> started in the output.
> but still nothing there
> 
> # Services
> 
> Name                Labels              Selector            Port
> 
> ---
> 
> # Controllers
> 
> Name                Image(s)                     Selector            Replicas
> 
> ---
> 
> algebra             23.251.148.42:5000/algebra   name=algebra        4
> 
> # Pods
> 
> Name                Image(s)                     Host                                  Labels
> 
> ---
> 
> 9acb0442            23.251.148.42:5000/algebra   kube-minion-3.c.cloud-pge.internal/   name=algebra,replicationController=algebra
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/368#issuecomment-48534621
> .
"
923	368	verdverm	2014-07-09 22:11:15	NONE	"I'm spinning up 4 minions, the same startup script works with the debian backport
After stopping k8s from installing docker the client-server error went away.
Now, after starting a k8s cluster, I ssh in and `docker ps -a` reports nothing.

I then run my usual startup script, which can talk to the k8s master,
but the minions don't appear to be set up correctly to handle their duties
"
924	260	bgrant0607	2014-07-09 22:11:19	MEMBER	"As part of this, I'd remove service objects from the core apiserver and facilitate the use of other load balancers, such as HAProxy and nginx.
"
925	170	bgrant0607	2014-07-09 22:13:05	MEMBER	"If we were to remove replicationController from the core apiserver into a separate service, I'd leave the pod template in the core.
"
926	368	vmarmol	2014-07-09 22:14:57	CONTRIBUTOR	"@verdverm is the Kubelet complaining of something? It may be that the environment is not setup correctly for them (we did want to move that to Docker containers eventually)
"
927	146	bgrant0607	2014-07-09 22:18:53	MEMBER	"We should also make it possible to plug in other naming/discovery mechanisms, such as etcd, Eureka, and Consul. One prerequisite is that it needs to be possible to get pod IP addresses #385.
"
928	260	smarterclayton	2014-07-09 22:32:28	CONTRIBUTOR	"It would be nice if the logical definition of a service (the query and/or global name) was able to be used/specialized in multiple ways - as a simple load balancer installed via the infrastructure, as a more feature complete load balancer like nginx or haproxy also offered by the infrastructure, as a queryable endpoint an integrator could poll/wait on (GET /services/foo -> { endpoints: [{host, port}, ...] }), or as information available to hosts to expose local load balancers.  Obviously these could be multiple different use cases and as such split into their own resources, but having some flexibility to specify intent (unify under a lb) distinct from mechanism makes it easier to satisfy a wide range of reqts.
"
929	368	verdverm	2014-07-09 22:35:02	NONE	"how would I find out if the Kubelet is complaining?

on master or minion?
"
930	368	brendandburns	2014-07-09 22:38:02	CONTRIBUTOR	"Can you try ""kubecfg.sh list minions""?

Brendan
 On Jul 9, 2014 3:35 PM, ""Tony Worm"" notifications@github.com wrote:

> how would I find out if the Kubelet is complaining?
> 
> on master or minion?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/368#issuecomment-48544079
> .
"
931	386	smarterclayton	2014-07-09 22:42:25	CONTRIBUTOR	"Would it be better to make this explicit in the manifest (VAR_A=$K8_POD_ID) rather than implicit?  I ask because in Openshift we found this led to lazy coupling, and in scenarios where you have code that wants a unique id, you want to be able to alias names instead.  At least that couples k8s to an image via a specific resource provided by k8s, vs making the image dependent on k8s.
"
932	260	bgrant0607	2014-07-09 22:54:57	MEMBER	"@smarterclayton I agree with separating policy and mechanism. 

Primitives we need:
1. The ability to poll/watch a set identified by a label selector. Not sure if there is an issue filed yet.
2. The ability to query pod IP addresses (#385).

This would be enough to compose with other naming/discovery mechanisms and/or load balancers. We could then build a higher-level layer on top of the core that bundles common patterns with a simple API.
"
933	365	thockin	2014-07-09 23:14:27	MEMBER	"Merging.  We can fix anything else in followups.
"
934	368	verdverm	2014-07-09 23:25:34	NONE	"no complaints, still no dockers running on any of the minions

```
Minion identifier
----------
kube-minion-1.c.cloud-pge.internal
kube-minion-2.c.cloud-pge.internal
kube-minion-3.c.cloud-pge.internal
kube-minion-4.c.cloud-pge.internal
```
"
935	386	bgrant0607	2014-07-09 23:42:42	MEMBER	"@smarterclayton Hmm. I see the case for doing the substitution in k8s rather than in the container, though I do have some concerns about introducing yet another substitution mechanism.

I was thinking that, at least for now, any additional information the container wanted about itself could be retrieved by querying the apiserver, which would obviously also couple the querier to k8s. 

I think we are going to need to standardize on a ""downward-facing API"", including this type of contextual information (and also IP address, DNS name, auth info, etc.), but also a variety of notifications and/or hooks (#140). If we had hooks, we could make the context info available to the hooks, which could then normalize the info for the application. I see hooks as being necessarily specific to the hosting framework and other glue services.
"
936	368	vmarmol	2014-07-09 23:46:53	CONTRIBUTOR	"what does the minion logs say?: /var/log/kubelet.log I think

On Wed, Jul 9, 2014 at 4:25 PM, Tony Worm notifications@github.com wrote:

> no complaints, still no dockers running on any of the minions
> 
> ## Minion identifier
> 
> kube-minion-1.c.cloud-pge.internal
> kube-minion-2.c.cloud-pge.internal
> kube-minion-3.c.cloud-pge.internal
> kube-minion-4.c.cloud-pge.internal
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/368#issuecomment-48548117
> .
"
937	368	brendandburns	2014-07-09 23:51:08	CONTRIBUTOR	"Also, look at /var/log/controller-manager.log on the kubernetes-master

Thanks!
--brendan

On Wed, Jul 9, 2014 at 4:47 PM, Victor Marmol notifications@github.com
wrote:

> what does the minion logs say?: /var/log/kubelet.log I think
> 
> On Wed, Jul 9, 2014 at 4:25 PM, Tony Worm notifications@github.com
> wrote:
> 
> > no complaints, still no dockers running on any of the minions
> > 
> > ## Minion identifier
> > 
> > kube-minion-1.c.cloud-pge.internal
> > kube-minion-2.c.cloud-pge.internal
> > kube-minion-3.c.cloud-pge.internal
> > kube-minion-4.c.cloud-pge.internal
> > 
> > —
> > Reply to this email directly or view it on GitHub
> > <
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/368#issuecomment-48548117>
> > 
> > .
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/368#issuecomment-48549687
> .
"
938	368	verdverm	2014-07-09 23:55:37	NONE	"MASTER:

```
I0709 23:22:24.679437 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:22:24.679458 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:22:24.679839 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:22:24.679866 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:22:34.679349 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:22:34.679439 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:22:34.679455 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:22:34.679853 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:22:34.679925 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:22:44.679285 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:22:44.679374 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:22:44.679389 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:22:44.679769 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:22:44.679844 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:22:54.679373 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:22:54.679455 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:22:54.679498 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:22:54.679920 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:22:54.679991 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:23:04.679349 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:23:04.679441 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:23:04.679456 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:23:04.679861 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:23:04.679933 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:23:14.679286 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:23:14.679366 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:23:14.679390 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:23:14.679800 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:23:14.679895 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:23:17.779099 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true]
I0709 23:23:17.779251 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:23:17.779283 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&wait=true&waitIndex=4]
I0709 23:23:17.779294 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true&waitIndex=4  | method  GET]
I0709 23:23:17.779334 15148 replication_controller.go:128] Got watch: &etcd.Response{Action:""set"", Node:(*etcd.Node)(0xc210089840), PrevNode:(*etcd.Node)(nil), EtcdIndex:0x2, RaftIndex:0x270, RaftTerm:0x0}
I0709 23:23:17.779842 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true&waitIndex=4]
I0709 23:23:17.930444 15148 replication_controller.go:187] []api.Pod(nil)
I0709 23:23:17.930471 15148 replication_controller.go:190] Too few replicas, creating 4
I0709 23:28:17.779260 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true&waitIndex=4]
E0709 23:28:17.779316 15148 replication_controller.go:113] etcd.Watch stopped unexpectedly: unexpected end of JSON input (&json.SyntaxError{msg:""unexpected end of JSON input"", Offset:0})
```

MINION-3

```
Couldn't unmarshal configuration: YAML error: resolveTable item not yet handled: < (with <!DOCTYPE html> <html lang=en> <meta charset=utf-8> <meta name=viewport content=""initial-scale=1, minimum-scale=1, width=device-width""> <title>Error 404 (Not Found)!!1</title> <style> *{margin:0;padding:0}html,code{font:15px/22px arial,sans-serif}html{background:#fff;color:#222;padding:15px}body{margin:7% auto 0;max-width:390px;min-height:180px;padding:30px 0 15px}* > body{background:url(//www.google.com/images/errors/robot.png) 100% 5px no-repeat;padding-right:205px}p{margin:11px 0 22px;overflow:hidden}ins{color:#777;text-decoration:none}a img{border:0}@media screen and (max-width:772px){body{background:none;margin-top:0;max-width:none;padding-right:0}}#logo{background:url(//www.google.com/images/errors/logo_sm_2.png) no-repeat}@media only screen and (min-resolution:192dpi){#logo{background:url(//www.google.com/images/errors/logo_sm_2_hr.png) no-repeat 0% 0%/100% 100%;-moz-border-image:url(//www.google.com/images/errors/logo_sm_2_hr.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/errors/logo_sm_2_hr.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:55px;width:150px} </style> <a href=//www.google.com/><span id=logo aria-label=Google></span></a> <p><b>404.</b> <ins>That’s an error.</ins> <p>The requested URL <code>/computeMetadata/v1beta1/instance/attributes/google-container-manifest</code> was not found on this server.  <ins>That’s all we know.</ins>)
```
"
939	368	brendanburns	2014-07-09 23:58:44	CONTRIBUTOR	"Is there more on the master after ""Too few replicas, creating 4""?

Thanks
--brendan

On Wed, Jul 9, 2014 at 4:55 PM, Tony Worm notifications@github.com wrote:

> MASTER:
> 
> I0709 23:22:24.679437 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:22:24.679458 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:22:24.679839 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:22:24.679866 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:22:34.679349 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:22:34.679439 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:22:34.679455 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:22:34.679853 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:22:34.679925 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:22:44.679285 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:22:44.679374 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:22:44.679389 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:22:44.679769 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:22:44.679844 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:22:54.679373 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:22:54.679455 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:22:54.679498 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:22:54.679920 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:22:54.679991 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:23:04.679349 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:23:04.679441 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:23:04.679456 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:23:04.679861 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:23:04.679933 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:23:14.679286 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:23:14.679366 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:23:14.679390 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:23:14.679800 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:23:14.679895 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:23:17.779099 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true]
> I0709 23:23:17.779251 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:23:17.779283 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&wait=true&waitIndex=4]
> I0709 23:23:17.779294 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true&waitIndex=4  | method  GET]
> I0709 23:23:17.779334 15148 replication_controller.go:128] Got watch: &etcd.Response{Action:""set"", Node:(_etcd.Node)(0xc210089840), PrevNode:(_etcd.Node)(nil), EtcdIndex:0x2, RaftIndex:0x270, RaftTerm:0x0}
> I0709 23:23:17.779842 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true&waitIndex=4]
> I0709 23:23:17.930444 15148 replication_controller.go:187] []api.Pod(nil)
> I0709 23:23:17.930471 15148 replication_controller.go:190] Too few replicas, creating 4
> I0709 23:28:17.779260 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true&waitIndex=4]
> E0709 23:28:17.779316 15148 replication_controller.go:113] etcd.Watch stopped unexpectedly: unexpected end of JSON input (&json.SyntaxError{msg:""unexpected end of JSON input"", Offset:0})
> 
> MINION-3
> 
> Couldn't unmarshal configuration: YAML error: resolveTable item not yet handled: < (with <!DOCTYPE html> <html lang=en> <meta charset=utf-8> <meta name=viewport content=""initial-scale=1, minimum-scale=1, width=device-width""> <title>Error 404 (Not Found)!!1</title> <style> _{margin:0;padding:0}html,code{font:15px/22px arial,sans-serif}html{background:#fff;color:#222;padding:15px}body{margin:7% auto 0;max-width:390px;min-height:180px;padding:30px 0 15px}_ > body{background:url(//www.google.com/images/errors/robot.png) 100% 5px no-repeat;padding-right:205px}p{margin:11px 0 22px;overflow:hidden}ins{color:#777;text-decoration:none}a img{border:0}@media screen and (max-width:772px){body{background:none;margin-top:0;max-width:none;padding-right:0}}#logo{background:url(//www.google.com/images/errors/logo_sm_2.png) no-repeat}@media only screen and (min-resolution:192dpi){#logo{background:url(//www.google.com/images/errors/logo_sm_2_hr.png) no-repeat 0% 0%/100% 100%;-moz-border-image:url(//www.google.com/images/errors/logo_sm_2_hr.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/errors/logo_sm_2_hr.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:55px;width:150px} </style> <a href=//www.google.com/><span id=logo aria-label=Google></span></a> <p><b>404.</b> <ins>That’s an error.</ins> <p>The requested URL <code>/computeMetadata/v1beta1/instance/attributes/google-container-manifest</code> was not found on this server.  <ins>That’s all we know.</ins>)
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/368#issuecomment-48550279
> .
"
940	368	verdverm	2014-07-09 23:59:57	NONE	"FULL MASTER LOG

```

I0709 23:13:44.679194 15148 logs.go:38] etcd DEBUG: watch [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:13:44.679269 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:13:44.679308 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&wait=true]
I0709 23:13:44.679320 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true  | method  GET]
I0709 23:13:44.679475 15148 logs.go:38] etcd DEBUG: [network error: Get http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true: dial tcp 10.240.16.104:4001: connection refused]
E0709 23:13:44.679489 15148 replication_controller.go:113] etcd.Watch stopped unexpectedly: 501: All the given peers are not reachable (Tried to connect to each peer twice and failed) [0] (&etcd.EtcdError{ErrorCode:501, Message:""All the given peers are not reachable"", Cause:""Tried to connect to each peer twice and failed"", Index:0x0})
I0709 23:13:54.679612 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:13:54.679676 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:13:54.679689 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:13:54.679725 15148 logs.go:38] etcd DEBUG: watch [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:13:54.679740 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:13:54.679758 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&wait=true]
I0709 23:13:54.679770 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true  | method  GET]
I0709 23:13:54.679946 15148 logs.go:38] etcd DEBUG: [network error: Get http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true: dial tcp 10.240.16.104:4001: connection refused]
E0709 23:13:54.679959 15148 replication_controller.go:113] etcd.Watch stopped unexpectedly: 501: All the given peers are not reachable (Tried to connect to each peer twice and failed) [0] (&etcd.EtcdError{ErrorCode:501, Message:""All the given peers are not reachable"", Cause:""Tried to connect to each peer twice and failed"", Index:0x0})
I0709 23:13:54.680010 15148 logs.go:38] etcd DEBUG: [network error: Get http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false: dial tcp 10.240.16.104:4001: connection refused]
E0709 23:13:54.680021 15148 replication_controller.go:208] Synchronization error: 501: All the given peers are not reachable (Tried to connect to each peer twice and failed) [0] (&etcd.EtcdError{ErrorCode:501, Message:""All the given peers are not reachable"", Cause:""Tried to connect to each peer twice and failed"", Index:0x0})
I0709 23:14:04.680814 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:14:04.680915 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:14:04.680938 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:14:04.680975 15148 logs.go:38] etcd DEBUG: watch [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:14:04.680996 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:14:04.681032 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&wait=true]
I0709 23:14:04.681077 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true  | method  GET]
I0709 23:14:04.681287 15148 logs.go:38] etcd DEBUG: [network error: Get http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true: dial tcp 10.240.16.104:4001: connection refused]
E0709 23:14:04.681306 15148 replication_controller.go:113] etcd.Watch stopped unexpectedly: 501: All the given peers are not reachable (Tried to connect to each peer twice and failed) [0] (&etcd.EtcdError{ErrorCode:501, Message:""All the given peers are not reachable"", Cause:""Tried to connect to each peer twice and failed"", Index:0x0})
I0709 23:14:04.681400 15148 logs.go:38] etcd DEBUG: [network error: Get http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false: dial tcp 10.240.16.104:4001: connection refused]
E0709 23:14:04.681426 15148 replication_controller.go:208] Synchronization error: 501: All the given peers are not reachable (Tried to connect to each peer twice and failed) [0] (&etcd.EtcdError{ErrorCode:501, Message:""All the given peers are not reachable"", Cause:""Tried to connect to each peer twice and failed"", Index:0x0})
I0709 23:14:14.681983 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:14:14.682051 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:14:14.682067 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:14:14.682095 15148 logs.go:38] etcd DEBUG: watch [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:14:14.682106 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:14:14.682205 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&wait=true]
I0709 23:14:14.682217 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true  | method  GET]
I0709 23:14:14.683200 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true]
I0709 23:14:14.683241 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:14:14.683264 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:14:24.679537 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:14:24.679602 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:14:24.679617 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:14:24.679983 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:14:24.680033 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:14:34.679408 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:14:34.679570 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:14:34.679593 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:14:34.680092 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:14:34.680127 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:14:44.679287 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:14:44.679337 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:14:44.679351 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:14:44.679833 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:14:44.679873 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:14:54.679366 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:14:54.679433 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:14:54.679447 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:14:54.679837 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:14:54.679876 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:15:04.679335 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:15:04.679424 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:15:04.679438 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:15:04.679834 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:15:04.679864 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:15:14.679289 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:15:14.679384 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:15:14.679400 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:15:14.679804 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:15:14.679845 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:15:24.679342 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:15:24.679428 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:15:24.679444 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:15:24.679820 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:15:24.679918 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:15:34.679339 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:15:34.679398 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:15:34.679412 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:15:34.679771 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:15:34.679857 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:15:44.679303 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:15:44.679389 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:15:44.679411 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:15:44.679789 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:15:44.679873 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:15:54.679329 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:15:54.679405 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:15:54.679421 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:15:54.679796 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:15:54.679825 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:16:04.679349 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:16:04.679443 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:16:04.679475 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:16:04.679859 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:16:04.679928 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:16:14.679314 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:16:14.679391 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:16:14.679426 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:16:14.679841 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:16:14.679941 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:16:24.679372 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:16:24.679839 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:16:24.679860 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:16:24.680266 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:16:24.680343 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:16:34.679352 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:16:34.679415 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:16:34.679430 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:16:34.679812 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:16:34.679882 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:16:44.679285 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:16:44.679372 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:16:44.679387 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:16:44.679786 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:16:44.679820 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:16:54.679333 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:16:54.679401 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:16:54.679426 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:16:54.679813 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:16:54.679888 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:17:04.679343 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:17:04.679433 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:17:04.679448 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:17:04.679824 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:17:04.679895 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:17:14.679299 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:17:14.679358 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:17:14.679374 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:17:14.679732 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:17:14.679813 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:17:24.679322 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:17:24.679402 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:17:24.679417 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:17:24.679786 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:17:24.679856 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:17:34.679340 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:17:34.679411 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:17:34.679426 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:17:34.679816 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:17:34.679894 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:17:44.679315 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:17:44.679383 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:17:44.679398 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:17:44.679820 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:17:44.679905 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:17:54.679348 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:17:54.679422 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:17:54.679436 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:17:54.679848 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:17:54.679942 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:18:04.679506 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:18:04.679613 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:18:04.679631 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:18:04.679987 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:18:04.680085 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:18:14.679315 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:18:14.679383 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:18:14.679397 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:18:14.679817 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:18:14.679910 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:18:24.679333 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:18:24.679416 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:18:24.679430 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:18:24.679853 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:18:24.679940 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:18:34.679329 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:18:34.679398 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:18:34.679413 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:18:34.679810 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:18:34.679889 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:18:44.679290 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:18:44.679371 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:18:44.679385 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:18:44.679812 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:18:44.679890 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:18:54.679346 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:18:54.679455 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:18:54.679470 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:18:54.679866 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:18:54.679944 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:19:04.679347 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:19:04.679438 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:19:04.679453 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:19:04.679869 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:19:04.679955 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:19:14.679326 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:19:14.679402 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:19:14.679417 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:19:14.679785 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:19:14.679856 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
E0709 23:19:14.683273 15148 replication_controller.go:113] etcd.Watch stopped unexpectedly: 501: All the given peers are not reachable (Tried to connect to each peer twice and failed) [0] (&etcd.EtcdError{ErrorCode:501, Message:""All the given peers are not reachable"", Cause:""Tried to connect to each peer twice and failed"", Index:0x0})
I0709 23:19:24.683482 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:19:24.683559 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:19:24.683574 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:19:24.683614 15148 logs.go:38] etcd DEBUG: watch [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:19:24.683626 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:19:24.683651 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&wait=true]
I0709 23:19:24.683662 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true  | method  GET]
I0709 23:19:24.684269 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true]
I0709 23:19:24.684533 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:19:24.684622 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:19:34.679347 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:19:34.679422 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:19:34.679436 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:19:34.679845 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:19:34.679930 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:19:44.679277 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:19:44.679346 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:19:44.679361 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:19:44.679744 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:19:44.679814 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:19:54.679339 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:19:54.679427 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:19:54.679441 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:19:54.679833 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:19:54.679904 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:20:04.679346 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:20:04.679437 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:20:04.679452 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:20:04.679862 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:20:04.679940 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:20:14.679321 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:20:14.679399 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:20:14.679418 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:20:14.679823 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:20:14.679847 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:20:24.679335 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:20:24.679427 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:20:24.679443 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:20:24.679853 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:20:24.679928 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:20:34.679396 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:20:34.679483 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:20:34.679498 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:20:34.679864 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:20:34.679937 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:20:44.679271 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:20:44.679352 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:20:44.679367 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:20:44.679747 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:20:44.679818 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:20:54.679349 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:20:54.679420 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:20:54.679435 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:20:54.679804 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:20:54.679902 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:21:04.679346 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:21:04.679424 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:21:04.679439 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:21:04.679835 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:21:04.679904 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:21:14.679286 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:21:14.679351 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:21:14.679365 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:21:14.679767 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:21:14.679866 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:21:24.679354 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:21:24.679431 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:21:24.679446 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:21:24.679847 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:21:24.679927 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:21:34.679332 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:21:34.679427 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:21:34.679444 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:21:34.679887 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:21:34.679961 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:21:44.679294 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:21:44.679382 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:21:44.679399 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:21:44.679816 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:21:44.679906 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:21:54.679344 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:21:54.679426 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:21:54.679441 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:21:54.679846 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:21:54.679933 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:22:04.679351 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:22:04.679470 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:22:04.679486 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:22:04.679923 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:22:04.679998 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:22:14.679313 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:22:14.679397 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:22:14.679411 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:22:14.679841 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:22:14.679911 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:22:24.679364 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:22:24.679437 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:22:24.679458 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:22:24.679839 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:22:24.679866 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:22:34.679349 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:22:34.679439 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:22:34.679455 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:22:34.679853 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:22:34.679925 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:22:44.679285 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:22:44.679374 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:22:44.679389 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:22:44.679769 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:22:44.679844 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:22:54.679373 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:22:54.679455 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:22:54.679498 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:22:54.679920 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:22:54.679991 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:23:04.679349 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:23:04.679441 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:23:04.679456 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:23:04.679861 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:23:04.679933 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:23:14.679286 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:23:14.679366 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:23:14.679390 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
I0709 23:23:14.679800 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:23:14.679895 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
I0709 23:23:17.779099 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true]
I0709 23:23:17.779251 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
I0709 23:23:17.779283 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&wait=true&waitIndex=4]
I0709 23:23:17.779294 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true&waitIndex=4  | method  GET]
I0709 23:23:17.779334 15148 replication_controller.go:128] Got watch: &etcd.Response{Action:""set"", Node:(*etcd.Node)(0xc210089840), PrevNode:(*etcd.Node)(nil), EtcdIndex:0x2, RaftIndex:0x270, RaftTerm:0x0}
I0709 23:23:17.779842 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true&waitIndex=4]
I0709 23:23:17.930444 15148 replication_controller.go:187] []api.Pod(nil)
I0709 23:23:17.930471 15148 replication_controller.go:190] Too few replicas, creating 4
I0709 23:28:17.779260 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true&waitIndex=4]
E0709 23:28:17.779316 15148 replication_controller.go:113] etcd.Watch stopped unexpectedly: unexpected end of JSON input (&json.SyntaxError{msg:""unexpected end of JSON input"", Offset:0})
```
"
941	368	brendandburns	2014-07-10 00:02:06	CONTRIBUTOR	"Is the controller manager still running:

either
/etc/init.d/controller-manager status

or

ps -ef | grep controller-manager

?

I wonder if the request to create the pod is hanging?

On Wed, Jul 9, 2014 at 5:00 PM, Tony Worm notifications@github.com wrote:

> FULL MASTER LOG
> 
> I0709 23:13:44.679194 15148 logs.go:38] etcd DEBUG: watch [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:13:44.679269 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:13:44.679308 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&wait=true]
> I0709 23:13:44.679320 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true  | method  GET]
> I0709 23:13:44.679475 15148 logs.go:38] etcd DEBUG: [network error: Get http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true: dial tcp 10.240.16.104:4001: connection refused]
> E0709 23:13:44.679489 15148 replication_controller.go:113] etcd.Watch stopped unexpectedly: 501: All the given peers are not reachable (Tried to connect to each peer twice and failed) [0](&etcd.EtcdError{ErrorCode:501, Message:""All the given peers are not reachable"", Cause:""Tried to connect to each peer twice and failed"", Index:0x0})
> I0709 23:13:54.679612 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:13:54.679676 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:13:54.679689 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:13:54.679725 15148 logs.go:38] etcd DEBUG: watch [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:13:54.679740 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:13:54.679758 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&wait=true]
> I0709 23:13:54.679770 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true  | method  GET]
> I0709 23:13:54.679946 15148 logs.go:38] etcd DEBUG: [network error: Get http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true: dial tcp 10.240.16.104:4001: connection refused]
> E0709 23:13:54.679959 15148 replication_controller.go:113] etcd.Watch stopped unexpectedly: 501: All the given peers are not reachable (Tried to connect to each peer twice and failed) [0](&etcd.EtcdError{ErrorCode:501, Message:""All the given peers are not reachable"", Cause:""Tried to connect to each peer twice and failed"", Index:0x0})
> I0709 23:13:54.680010 15148 logs.go:38] etcd DEBUG: [network error: Get http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false: dial tcp 10.240.16.104:4001: connection refused]
> E0709 23:13:54.680021 15148 replication_controller.go:208] Synchronization error: 501: All the given peers are not reachable (Tried to connect to each peer twice and failed) [0](&etcd.EtcdError{ErrorCode:501, Message:""All the given peers are not reachable"", Cause:""Tried to connect to each peer twice and failed"", Index:0x0})
> I0709 23:14:04.680814 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:14:04.680915 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:14:04.680938 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:14:04.680975 15148 logs.go:38] etcd DEBUG: watch [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:14:04.680996 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:14:04.681032 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&wait=true]
> I0709 23:14:04.681077 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true  | method  GET]
> I0709 23:14:04.681287 15148 logs.go:38] etcd DEBUG: [network error: Get http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true: dial tcp 10.240.16.104:4001: connection refused]
> E0709 23:14:04.681306 15148 replication_controller.go:113] etcd.Watch stopped unexpectedly: 501: All the given peers are not reachable (Tried to connect to each peer twice and failed) [0](&etcd.EtcdError{ErrorCode:501, Message:""All the given peers are not reachable"", Cause:""Tried to connect to each peer twice and failed"", Index:0x0})
> I0709 23:14:04.681400 15148 logs.go:38] etcd DEBUG: [network error: Get http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false: dial tcp 10.240.16.104:4001: connection refused]
> E0709 23:14:04.681426 15148 replication_controller.go:208] Synchronization error: 501: All the given peers are not reachable (Tried to connect to each peer twice and failed) [0](&etcd.EtcdError{ErrorCode:501, Message:""All the given peers are not reachable"", Cause:""Tried to connect to each peer twice and failed"", Index:0x0})
> I0709 23:14:14.681983 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:14:14.682051 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:14:14.682067 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:14:14.682095 15148 logs.go:38] etcd DEBUG: watch [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:14:14.682106 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:14:14.682205 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&wait=true]
> I0709 23:14:14.682217 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true  | method  GET]
> I0709 23:14:14.683200 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true]
> I0709 23:14:14.683241 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:14:14.683264 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:14:24.679537 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:14:24.679602 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:14:24.679617 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:14:24.679983 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:14:24.680033 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:14:34.679408 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:14:34.679570 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:14:34.679593 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:14:34.680092 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:14:34.680127 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:14:44.679287 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:14:44.679337 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:14:44.679351 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:14:44.679833 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:14:44.679873 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:14:54.679366 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:14:54.679433 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:14:54.679447 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:14:54.679837 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:14:54.679876 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:15:04.679335 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:15:04.679424 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:15:04.679438 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:15:04.679834 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:15:04.679864 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:15:14.679289 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:15:14.679384 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:15:14.679400 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:15:14.679804 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:15:14.679845 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:15:24.679342 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:15:24.679428 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:15:24.679444 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:15:24.679820 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:15:24.679918 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:15:34.679339 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:15:34.679398 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:15:34.679412 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:15:34.679771 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:15:34.679857 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:15:44.679303 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:15:44.679389 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:15:44.679411 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:15:44.679789 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:15:44.679873 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:15:54.679329 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:15:54.679405 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:15:54.679421 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:15:54.679796 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:15:54.679825 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:16:04.679349 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:16:04.679443 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:16:04.679475 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:16:04.679859 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:16:04.679928 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:16:14.679314 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:16:14.679391 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:16:14.679426 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:16:14.679841 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:16:14.679941 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:16:24.679372 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:16:24.679839 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:16:24.679860 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:16:24.680266 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:16:24.680343 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:16:34.679352 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:16:34.679415 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:16:34.679430 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:16:34.679812 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:16:34.679882 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:16:44.679285 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:16:44.679372 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:16:44.679387 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:16:44.679786 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:16:44.679820 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:16:54.679333 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:16:54.679401 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:16:54.679426 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:16:54.679813 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:16:54.679888 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:17:04.679343 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:17:04.679433 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:17:04.679448 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:17:04.679824 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:17:04.679895 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:17:14.679299 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:17:14.679358 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:17:14.679374 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:17:14.679732 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:17:14.679813 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:17:24.679322 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:17:24.679402 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:17:24.679417 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:17:24.679786 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:17:24.679856 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:17:34.679340 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:17:34.679411 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:17:34.679426 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:17:34.679816 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:17:34.679894 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:17:44.679315 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:17:44.679383 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:17:44.679398 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:17:44.679820 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:17:44.679905 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:17:54.679348 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:17:54.679422 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:17:54.679436 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:17:54.679848 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:17:54.679942 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:18:04.679506 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:18:04.679613 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:18:04.679631 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:18:04.679987 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:18:04.680085 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:18:14.679315 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:18:14.679383 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:18:14.679397 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:18:14.679817 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:18:14.679910 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:18:24.679333 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:18:24.679416 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:18:24.679430 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:18:24.679853 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:18:24.679940 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:18:34.679329 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:18:34.679398 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:18:34.679413 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:18:34.679810 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:18:34.679889 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:18:44.679290 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:18:44.679371 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:18:44.679385 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:18:44.679812 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:18:44.679890 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:18:54.679346 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:18:54.679455 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:18:54.679470 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:18:54.679866 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:18:54.679944 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:19:04.679347 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:19:04.679438 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:19:04.679453 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:19:04.679869 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:19:04.679955 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:19:14.679326 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:19:14.679402 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:19:14.679417 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:19:14.679785 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:19:14.679856 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> E0709 23:19:14.683273 15148 replication_controller.go:113] etcd.Watch stopped unexpectedly: 501: All the given peers are not reachable (Tried to connect to each peer twice and failed) [0](&etcd.EtcdError{ErrorCode:501, Message:""All the given peers are not reachable"", Cause:""Tried to connect to each peer twice and failed"", Index:0x0})
> I0709 23:19:24.683482 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:19:24.683559 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:19:24.683574 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:19:24.683614 15148 logs.go:38] etcd DEBUG: watch [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:19:24.683626 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:19:24.683651 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&wait=true]
> I0709 23:19:24.683662 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true  | method  GET]
> I0709 23:19:24.684269 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true]
> I0709 23:19:24.684533 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:19:24.684622 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:19:34.679347 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:19:34.679422 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:19:34.679436 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:19:34.679845 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:19:34.679930 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:19:44.679277 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:19:44.679346 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:19:44.679361 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:19:44.679744 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:19:44.679814 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:19:54.679339 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:19:54.679427 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:19:54.679441 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:19:54.679833 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:19:54.679904 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:20:04.679346 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:20:04.679437 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:20:04.679452 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:20:04.679862 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:20:04.679940 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:20:14.679321 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:20:14.679399 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:20:14.679418 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:20:14.679823 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:20:14.679847 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:20:24.679335 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:20:24.679427 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:20:24.679443 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:20:24.679853 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:20:24.679928 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:20:34.679396 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:20:34.679483 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:20:34.679498 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:20:34.679864 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:20:34.679937 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:20:44.679271 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:20:44.679352 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:20:44.679367 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:20:44.679747 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:20:44.679818 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:20:54.679349 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:20:54.679420 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:20:54.679435 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:20:54.679804 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:20:54.679902 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:21:04.679346 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:21:04.679424 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:21:04.679439 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:21:04.679835 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:21:04.679904 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:21:14.679286 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:21:14.679351 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:21:14.679365 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:21:14.679767 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:21:14.679866 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:21:24.679354 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:21:24.679431 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:21:24.679446 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:21:24.679847 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:21:24.679927 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:21:34.679332 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:21:34.679427 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:21:34.679444 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:21:34.679887 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:21:34.679961 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:21:44.679294 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:21:44.679382 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:21:44.679399 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:21:44.679816 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:21:44.679906 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:21:54.679344 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:21:54.679426 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:21:54.679441 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:21:54.679846 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:21:54.679933 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:22:04.679351 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:22:04.679470 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:22:04.679486 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:22:04.679923 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:22:04.679998 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:22:14.679313 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:22:14.679397 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:22:14.679411 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:22:14.679841 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:22:14.679911 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:22:24.679364 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:22:24.679437 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:22:24.679458 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:22:24.679839 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:22:24.679866 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:22:34.679349 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:22:34.679439 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:22:34.679455 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:22:34.679853 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:22:34.679925 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:22:44.679285 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:22:44.679374 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:22:44.679389 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:22:44.679769 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:22:44.679844 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:22:54.679373 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:22:54.679455 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:22:54.679498 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:22:54.679920 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:22:54.679991 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:23:04.679349 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:23:04.679441 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:23:04.679456 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:23:04.679861 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:23:04.679933 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:23:14.679286 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:23:14.679366 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:23:14.679390 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false  | method  GET]
> I0709 23:23:14.679800 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:23:14.679895 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&sorted=false]
> I0709 23:23:17.779099 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true]
> I0709 23:23:17.779251 15148 logs.go:38] etcd DEBUG: get [/registry/controllers http://10.240.16.104:4001] [%!s(MISSING)]
> I0709 23:23:17.779283 15148 logs.go:38] etcd DEBUG: [Connecting to etcd: attempt 1 for keys/registry/controllers?consistent=true&recursive=true&wait=true&waitIndex=4]
> I0709 23:23:17.779294 15148 logs.go:38] etcd DEBUG: [send.request.to  http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true&waitIndex=4  | method  GET]
> I0709 23:23:17.779334 15148 replication_controller.go:128] Got watch: &etcd.Response{Action:""set"", Node:(_etcd.Node)(0xc210089840), PrevNode:(_etcd.Node)(nil), EtcdIndex:0x2, RaftIndex:0x270, RaftTerm:0x0}
> I0709 23:23:17.779842 15148 logs.go:38] etcd DEBUG: [recv.response.from http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true&waitIndex=4]
> I0709 23:23:17.930444 15148 replication_controller.go:187] []api.Pod(nil)
> I0709 23:23:17.930471 15148 replication_controller.go:190] Too few replicas, creating 4
> I0709 23:28:17.779260 15148 logs.go:38] etcd DEBUG: [recv.success. http://10.240.16.104:4001/v2/keys/registry/controllers?consistent=true&recursive=true&wait=true&waitIndex=4]
> E0709 23:28:17.779316 15148 replication_controller.go:113] etcd.Watch stopped unexpectedly: unexpected end of JSON input (&json.SyntaxError{msg:""unexpected end of JSON input"", Offset:0})
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/368#issuecomment-48550561
> .
"
942	368	verdverm	2014-07-10 00:04:24	NONE	"```
tony@kube-master:/var/log$ ps -ef | grep controller-manager
998      15148     1  0 Jul09 ?        00:00:00 /usr/local/bin/controller-manager --master=127.0.0.1:8080 -etcd_servers=http://10.240.16.104:4001
tony     17663 17234  0 00:03 pts/1    00:00:00 grep controller-manager
tony@kube-master:/var/log$ /etc/init.d/controller-manager status
[ ok ] controller-manager is running.
```
"
943	386	smarterclayton	2014-07-10 00:07:25	CONTRIBUTOR	"That's true, although one strong goal in docker has been to try and offer via libchan/libswarm a ""Docker"" abstraction where on multiple platforms images can invoke consistent actions that affect the platform.  That applies to more generic operations, whereas k8s specific actions will always require some downwards API.  The abstraction is a long term play that requires active participation by a number of stakeholders and also implemented, demonstrated use cases, so it's not an argument for avoiding trying.

I don't think the downward API is bad, but some of the use cases (unique id for an instance, what ip do I have) might benefit from looser coupling or by trying to find the most lowest common denominator expression.  In the end image authors can always ignore those settings, but you do want image users to have the tools to bend a recaltricant image to their will.
"
944	368	brendandburns	2014-07-10 00:22:32	CONTRIBUTOR	"OK, my guess is the api request to create a pod is hanging (clearly we
should add some timeouts ...)

What's in /var/log/apiserver.log on the master?

Brendan
On Jul 9, 2014 5:04 PM, ""Tony Worm"" notifications@github.com wrote:

> tony@kube-master:/var/log$ ps -ef | grep controller-manager
> 998      15148     1  0 Jul09 ?        00:00:00 /usr/local/bin/controller-manager --master=127.0.0.1:8080 -etcd_servers=http://10.240.16.104:4001
> tony     17663 17234  0 00:03 pts/1    00:00:00 grep controller-manager
> tony@kube-master:/var/log$ /etc/init.d/controller-manager status
> [ ok ] controller-manager is running.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/368#issuecomment-48550875
> .
"
945	368	verdverm	2014-07-10 00:41:28	NONE	"/var/log/apiserver.log is full of errors

```
E0710 00:37:35.989748 10488 pod_cache.go:86] Error synchronizing container: &url.Error{Op:""Get"", URL:""http://kube-minion-3.c.cloud-pge.internal:10250/podInfo?podID=9acb0442"", Err:(*net.OpError)(0xc210209cc0)}
I0710 00:37:38.139230 10488 logger.go:111] GET /api/v1beta1/operations/2: (78.984us) 202
E0710 00:37:42.597234 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
E0710 00:37:52.892041 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
I0710 00:37:58.140174 10488 logger.go:111] GET /api/v1beta1/operations/2: (53.212us) 202
E0710 00:38:03.130514 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
E0710 00:38:06.132851 10488 pod_cache.go:86] Error synchronizing container: &url.Error{Op:""Get"", URL:""http://kube-minion-3.c.cloud-pge.internal:10250/podInfo?podID=9acb0442"", Err:(*net.OpError)(0xc210293940)}
E0710 00:38:13.406755 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
I0710 00:38:18.141094 10488 logger.go:111] GET /api/v1beta1/operations/2: (58.027us) 202
E0710 00:38:23.661463 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
E0710 00:38:34.000058 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
E0710 00:38:36.281340 10488 pod_cache.go:86] Error synchronizing container: &url.Error{Op:""Get"", URL:""http://kube-minion-3.c.cloud-pge.internal:10250/podInfo?podID=9acb0442"", Err:(*net.OpError)(0xc2103145c0)}
I0710 00:38:38.141953 10488 logger.go:111] GET /api/v1beta1/operations/2: (39.632us) 202
E0710 00:38:44.234893 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
E0710 00:38:54.510522 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
I0710 00:38:58.142792 10488 logger.go:111] GET /api/v1beta1/operations/2: (47.495us) 202
E0710 00:39:04.818036 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
E0710 00:39:06.406022 10488 pod_cache.go:86] Error synchronizing container: &url.Error{Op:""Get"", URL:""http://kube-minion-3.c.cloud-pge.internal:10250/podInfo?podID=9acb0442"", Err:(*net.OpError)(0xc21014ebc0)}
E0710 00:39:15.067191 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
I0710 00:39:18.143782 10488 logger.go:111] GET /api/v1beta1/operations/2: (55.755us) 202
E0710 00:39:25.331916 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
E0710 00:39:35.688293 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
E0710 00:39:36.561170 10488 pod_cache.go:86] Error synchronizing container: &url.Error{Op:""Get"", URL:""http://kube-minion-3.c.cloud-pge.internal:10250/podInfo?podID=9acb0442"", Err:(*net.OpError)(0xc210395200)}
I0710 00:39:38.144647 10488 logger.go:111] GET /api/v1beta1/operations/2: (61.725us) 202
E0710 00:39:45.973080 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
E0710 00:39:56.267239 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
I0710 00:39:58.145535 10488 logger.go:111] GET /api/v1beta1/operations/2: (43.775us) 202
E0710 00:40:06.510327 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
E0710 00:40:06.732296 10488 pod_cache.go:86] Error synchronizing container: &url.Error{Op:""Get"", URL:""http://kube-minion-3.c.cloud-pge.internal:10250/podInfo?podID=9acb0442"", Err:(*net.OpError)(0xc2102b4100)}
```
"
946	368	verdverm	2014-07-10 00:42:04	NONE	"I noticed cAdvisor is not running in any of the minions
"
947	368	verdverm	2014-07-10 00:43:05	NONE	"here's the head of the apiserver.log

```
E0709 23:13:06.129932 10488 pod_cache.go:80] Error synchronizing container list: &etcd.EtcdError{ErrorCode:501, Message:""All the given peers are not reachable"", Cause:""Tried to connect to each peer twice and failed"", Index:0x0}
E0709 23:13:36.386628 10488 pod_cache.go:80] Error synchronizing container list: &etcd.EtcdError{ErrorCode:501, Message:""All the given peers are not reachable"", Cause:""Tried to connect to each peer twice and failed"", Index:0x0}
E0709 23:14:06.624785 10488 pod_cache.go:80] Error synchronizing container list: &etcd.EtcdError{ErrorCode:501, Message:""All the given peers are not reachable"", Cause:""Tried to connect to each peer twice and failed"", Index:0x0}
I0709 23:14:32.376591 10488 logger.go:111] GET /api/v1beta1/pods: (250.086536ms) 200
I0709 23:22:18.029275 10488 logger.go:111] GET /api/v1beta1/minions?labels=: (361.476716ms) 200
I0709 23:22:39.509469 10488 logger.go:111] GET /api/v1beta1/services?labels=: (531.306us) 200
I0709 23:22:41.115735 10488 logger.go:111] GET /api/v1beta1/replicationControllers?labels=: (577.55us) 200
I0709 23:22:42.279955 10488 logger.go:111] GET /api/v1beta1/pods?labels=: (132.530447ms) 200
I0709 23:23:17.778127 10488 logger.go:111] POST /api/v1beta1/replicationControllers?labels=: (428.798us) 202
I0709 23:23:17.930070 10488 logger.go:111] GET /api/v1beta1/pods?labels=name%3Dalgebra: (150.152397ms) 200
I0709 23:23:17.931302 10488 logger.go:111] POST /api/v1beta1/pods: (351.445us) 202
E0709 23:23:18.546175 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
E0709 23:23:28.796310 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
I0709 23:23:37.822511 10488 logger.go:111] GET /api/v1beta1/operations/1: (37.04us) 202
I0709 23:23:37.932141 10488 logger.go:111] GET /api/v1beta1/operations/2: (37.74us) 202
E0709 23:23:39.149032 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
E0709 23:23:40.772891 10488 pod_cache.go:86] Error synchronizing container: &url.Error{Op:""Get"", URL:""http://kube-minion-3.c.cloud-pge.internal:10250/podInfo?podID=9acb0442"", Err:(*net.OpError)(0xc2102b5e40)}
E0709 23:23:49.391719 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
I0709 23:23:57.864421 10488 logger.go:111] GET /api/v1beta1/operations/1: (36.899us) 202
I0709 23:23:57.933013 10488 logger.go:111] GET /api/v1beta1/operations/2: (34.499us) 202
E0709 23:23:59.678929 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
E0709 23:24:10.015214 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
E0709 23:24:10.924896 10488 pod_cache.go:86] Error synchronizing container: &url.Error{Op:""Get"", URL:""http://kube-minion-3.c.cloud-pge.internal:10250/podInfo?podID=9acb0442"", Err:(*net.OpError)(0xc2103132c0)}
I0709 23:24:17.907743 10488 logger.go:111] GET /api/v1beta1/operations/1: (54.777us) 202
I0709 23:24:17.933756 10488 logger.go:111] GET /api/v1beta1/operations/2: (33.736us) 202
I0709 23:24:18.007363 10488 logger.go:111] GET /api/v1beta1/services?labels=: (639.489us) 200
I0709 23:24:19.562447 10488 logger.go:111] GET /api/v1beta1/replicationControllers?labels=: (776.742us) 200
E0709 23:24:20.304152 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
E0709 23:24:20.775735 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
I0709 23:24:20.775842 10488 logger.go:111] GET /api/v1beta1/pods?labels=: (168.672866ms) 200
E0709 23:24:30.629913 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
I0709 23:24:37.934663 10488 logger.go:111] GET /api/v1beta1/operations/2: (48.649us) 202
I0709 23:24:37.950103 10488 logger.go:111] GET /api/v1beta1/operations/1: (38.2us) 202
E0709 23:24:40.928183 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
E0709 23:24:41.130443 10488 pod_cache.go:86] Error synchronizing container: &url.Error{Op:""Get"", URL:""http://kube-minion-3.c.cloud-pge.internal:10250/podInfo?podID=9acb0442"", Err:(*net.OpError)(0xc210293fc0)}
E0709 23:24:51.192263 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
I0709 23:24:57.935518 10488 logger.go:111] GET /api/v1beta1/operations/2: (36.987us) 202
I0709 23:24:57.995266 10488 logger.go:111] GET /api/v1beta1/operations/1: (43.676us) 202
E0709 23:25:01.440263 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
E0709 23:25:11.272386 10488 pod_cache.go:86] Error synchronizing container: &url.Error{Op:""Get"", URL:""http://kube-minion-3.c.cloud-pge.internal:10250/podInfo?podID=9acb0442"", Err:(*net.OpError)(0xc210339180)}
E0709 23:25:11.723786 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
I0709 23:25:17.936402 10488 logger.go:111] GET /api/v1beta1/operations/2: (36.953us) 202
I0709 23:25:18.042979 10488 logger.go:111] GET /api/v1beta1/operations/1: (46.98us) 202
E0709 23:25:22.031073 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
E0709 23:25:32.301266 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
I0709 23:25:37.937317 10488 logger.go:111] GET /api/v1beta1/operations/2: (37.115us) 202
I0709 23:25:38.085363 10488 logger.go:111] GET /api/v1beta1/operations/1: (36.737us) 202
E0709 23:25:41.431432 10488 pod_cache.go:86] Error synchronizing container: &url.Error{Op:""Get"", URL:""http://kube-minion-3.c.cloud-pge.internal:10250/podInfo?podID=9acb0442"", Err:(*net.OpError)(0xc210313f80)}
E0709 23:25:42.562650 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
E0709 23:25:52.855564 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
I0709 23:25:57.938119 10488 logger.go:111] GET /api/v1beta1/operations/2: (36.193us) 202
I0709 23:25:58.128160 10488 logger.go:111] GET /api/v1beta1/operations/1: (38.593us) 202
E0709 23:26:03.352062 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
E0709 23:26:11.603278 10488 pod_cache.go:86] Error synchronizing container: &url.Error{Op:""Get"", URL:""http://kube-minion-3.c.cloud-pge.internal:10250/podInfo?podID=9acb0442"", Err:(*net.OpError)(0xc21025e780)}
E0709 23:26:13.630040 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
I0709 23:26:17.939022 10488 logger.go:111] GET /api/v1beta1/operations/2: (37.38us) 202
I0709 23:26:18.173360 10488 logger.go:111] GET /api/v1beta1/operations/1: (37.163us) 202
E0709 23:26:23.885397 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
I0709 23:26:26.786686 10488 logger.go:111] POST /api/v1beta1/services?labels=: (249.584us) 202
E0709 23:26:34.201922 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
I0709 23:26:37.939971 10488 logger.go:111] GET /api/v1beta1/operations/2: (60.075us) 202
E0709 23:26:41.751316 10488 pod_cache.go:86] Error synchronizing container: &url.Error{Op:""Get"", URL:""http://kube-minion-3.c.cloud-pge.internal:10250/podInfo?podID=9acb0442"", Err:(*net.OpError)(0xc2102e3780)}
E0709 23:26:44.474538 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
E0709 23:26:54.723643 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
I0709 23:26:57.941113 10488 logger.go:111] GET /api/v1beta1/operations/2: (52.434us) 202
E0709 23:27:05.002365 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
E0709 23:27:11.938910 10488 pod_cache.go:86] Error synchronizing container: &url.Error{Op:""Get"", URL:""http://kube-minion-3.c.cloud-pge.internal:10250/podInfo?podID=9acb0442"", Err:(*net.OpError)(0xc2103ad100)}
E0709 23:27:15.281687 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
I0709 23:27:17.942073 10488 logger.go:111] GET /api/v1beta1/operations/2: (71.731us) 202
E0709 23:27:25.548852 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
E0709 23:27:35.879749 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
```
"
948	368	vmarmol	2014-07-10 01:08:30	CONTRIBUTOR	"cAdvisor is a Docker container so the Kubelet may be having issues starting
those.

On Wed, Jul 9, 2014 at 5:43 PM, Tony Worm notifications@github.com wrote:

> here's the head of the apiserver.log
> 
> E0709 23:13:06.129932 10488 pod_cache.go:80] Error synchronizing container list: &etcd.EtcdError{ErrorCode:501, Message:""All the given peers are not reachable"", Cause:""Tried to connect to each peer twice and failed"", Index:0x0}
> E0709 23:13:36.386628 10488 pod_cache.go:80] Error synchronizing container list: &etcd.EtcdError{ErrorCode:501, Message:""All the given peers are not reachable"", Cause:""Tried to connect to each peer twice and failed"", Index:0x0}
> E0709 23:14:06.624785 10488 pod_cache.go:80] Error synchronizing container list: &etcd.EtcdError{ErrorCode:501, Message:""All the given peers are not reachable"", Cause:""Tried to connect to each peer twice and failed"", Index:0x0}
> I0709 23:14:32.376591 10488 logger.go:111] GET /api/v1beta1/pods: (250.086536ms) 200
> I0709 23:22:18.029275 10488 logger.go:111] GET /api/v1beta1/minions?labels=: (361.476716ms) 200
> I0709 23:22:39.509469 10488 logger.go:111] GET /api/v1beta1/services?labels=: (531.306us) 200
> I0709 23:22:41.115735 10488 logger.go:111] GET /api/v1beta1/replicationControllers?labels=: (577.55us) 200
> I0709 23:22:42.279955 10488 logger.go:111] GET /api/v1beta1/pods?labels=: (132.530447ms) 200
> I0709 23:23:17.778127 10488 logger.go:111] POST /api/v1beta1/replicationControllers?labels=: (428.798us) 202
> I0709 23:23:17.930070 10488 logger.go:111] GET /api/v1beta1/pods?labels=name%3Dalgebra: (150.152397ms) 200
> I0709 23:23:17.931302 10488 logger.go:111] POST /api/v1beta1/pods: (351.445us) 202
> E0709 23:23:18.546175 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
> E0709 23:23:28.796310 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
> I0709 23:23:37.822511 10488 logger.go:111] GET /api/v1beta1/operations/1: (37.04us) 202
> I0709 23:23:37.932141 10488 logger.go:111] GET /api/v1beta1/operations/2: (37.74us) 202
> E0709 23:23:39.149032 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
> E0709 23:23:40.772891 10488 pod_cache.go:86] Error synchronizing container: &url.Error{Op:""Get"", URL:""http://kube-minion-3.c.cloud-pge.internal:10250/podInfo?podID=9acb0442"", Err:(_net.OpError)(0xc2102b5e40)}
> E0709 23:23:49.391719 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
> I0709 23:23:57.864421 10488 logger.go:111] GET /api/v1beta1/operations/1: (36.899us) 202
> I0709 23:23:57.933013 10488 logger.go:111] GET /api/v1beta1/operations/2: (34.499us) 202
> E0709 23:23:59.678929 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
> E0709 23:24:10.015214 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
> E0709 23:24:10.924896 10488 pod_cache.go:86] Error synchronizing container: &url.Error{Op:""Get"", URL:""http://kube-minion-3.c.cloud-pge.internal:10250/podInfo?podID=9acb0442"", Err:(_net.OpError)(0xc2103132c0)}
> I0709 23:24:17.907743 10488 logger.go:111] GET /api/v1beta1/operations/1: (54.777us) 202
> I0709 23:24:17.933756 10488 logger.go:111] GET /api/v1beta1/operations/2: (33.736us) 202
> I0709 23:24:18.007363 10488 logger.go:111] GET /api/v1beta1/services?labels=: (639.489us) 200
> I0709 23:24:19.562447 10488 logger.go:111] GET /api/v1beta1/replicationControllers?labels=: (776.742us) 200
> E0709 23:24:20.304152 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
> E0709 23:24:20.775735 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
> I0709 23:24:20.775842 10488 logger.go:111] GET /api/v1beta1/pods?labels=: (168.672866ms) 200
> E0709 23:24:30.629913 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
> I0709 23:24:37.934663 10488 logger.go:111] GET /api/v1beta1/operations/2: (48.649us) 202
> I0709 23:24:37.950103 10488 logger.go:111] GET /api/v1beta1/operations/1: (38.2us) 202
> E0709 23:24:40.928183 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
> E0709 23:24:41.130443 10488 pod_cache.go:86] Error synchronizing container: &url.Error{Op:""Get"", URL:""http://kube-minion-3.c.cloud-pge.internal:10250/podInfo?podID=9acb0442"", Err:(_net.OpError)(0xc210293fc0)}
> E0709 23:24:51.192263 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
> I0709 23:24:57.935518 10488 logger.go:111] GET /api/v1beta1/operations/2: (36.987us) 202
> I0709 23:24:57.995266 10488 logger.go:111] GET /api/v1beta1/operations/1: (43.676us) 202
> E0709 23:25:01.440263 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
> E0709 23:25:11.272386 10488 pod_cache.go:86] Error synchronizing container: &url.Error{Op:""Get"", URL:""http://kube-minion-3.c.cloud-pge.internal:10250/podInfo?podID=9acb0442"", Err:(_net.OpError)(0xc210339180)}
> E0709 23:25:11.723786 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
> I0709 23:25:17.936402 10488 logger.go:111] GET /api/v1beta1/operations/2: (36.953us) 202
> I0709 23:25:18.042979 10488 logger.go:111] GET /api/v1beta1/operations/1: (46.98us) 202
> E0709 23:25:22.031073 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
> E0709 23:25:32.301266 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
> I0709 23:25:37.937317 10488 logger.go:111] GET /api/v1beta1/operations/2: (37.115us) 202
> I0709 23:25:38.085363 10488 logger.go:111] GET /api/v1beta1/operations/1: (36.737us) 202
> E0709 23:25:41.431432 10488 pod_cache.go:86] Error synchronizing container: &url.Error{Op:""Get"", URL:""http://kube-minion-3.c.cloud-pge.internal:10250/podInfo?podID=9acb0442"", Err:(_net.OpError)(0xc210313f80)}
> E0709 23:25:42.562650 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
> E0709 23:25:52.855564 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
> I0709 23:25:57.938119 10488 logger.go:111] GET /api/v1beta1/operations/2: (36.193us) 202
> I0709 23:25:58.128160 10488 logger.go:111] GET /api/v1beta1/operations/1: (38.593us) 202
> E0709 23:26:03.352062 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
> E0709 23:26:11.603278 10488 pod_cache.go:86] Error synchronizing container: &url.Error{Op:""Get"", URL:""http://kube-minion-3.c.cloud-pge.internal:10250/podInfo?podID=9acb0442"", Err:(_net.OpError)(0xc21025e780)}
> E0709 23:26:13.630040 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
> I0709 23:26:17.939022 10488 logger.go:111] GET /api/v1beta1/operations/2: (37.38us) 202
> I0709 23:26:18.173360 10488 logger.go:111] GET /api/v1beta1/operations/1: (37.163us) 202
> E0709 23:26:23.885397 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
> I0709 23:26:26.786686 10488 logger.go:111] POST /api/v1beta1/services?labels=: (249.584us) 202
> E0709 23:26:34.201922 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
> I0709 23:26:37.939971 10488 logger.go:111] GET /api/v1beta1/operations/2: (60.075us) 202
> E0709 23:26:41.751316 10488 pod_cache.go:86] Error synchronizing container: &url.Error{Op:""Get"", URL:""http://kube-minion-3.c.cloud-pge.internal:10250/podInfo?podID=9acb0442"", Err:(_net.OpError)(0xc2102e3780)}
> E0709 23:26:44.474538 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
> E0709 23:26:54.723643 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
> I0709 23:26:57.941113 10488 logger.go:111] GET /api/v1beta1/operations/2: (52.434us) 202
> E0709 23:27:05.002365 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
> E0709 23:27:11.938910 10488 pod_cache.go:86] Error synchronizing container: &url.Error{Op:""Get"", URL:""http://kube-minion-3.c.cloud-pge.internal:10250/podInfo?podID=9acb0442"", Err:(_net.OpError)(0xc2103ad100)}
> E0709 23:27:15.281687 10488 pod_registry.go:88] Error getting container info: &errors.errorString{s:""No cached pod info""}
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/368#issuecomment-48553228
> .
"
949	389	thockin	2014-07-10 05:18:14	MEMBER	"LGTM
"
950	389	thockin	2014-07-10 05:21:31	MEMBER	"Travis failed.  panic: runtime error: invalid memory address or nil pointer dereference
"
951	389	brendandburns	2014-07-10 05:24:57	CONTRIBUTOR	"looking...

--brendan

On Wed, Jul 9, 2014 at 10:21 PM, Tim Hockin notifications@github.com
wrote:

> Travis failed. panic: runtime error: invalid memory address or nil pointer
> dereference
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/389#issuecomment-48566379
> .
"
952	389	bgrant0607	2014-07-10 05:41:44	MEMBER	"Thanks. Would be useful to reference the issue:
https://help.github.com/articles/closing-issues-via-commit-messages

For example:
Fixes #385 
"
953	382	raphael	2014-07-10 06:12:12	CONTRIBUTOR	"Just signed it (as an individual contributor).
"
954	382	brendandburns	2014-07-10 06:14:15	CONTRIBUTOR	"Many thanks!  Merging.
"
955	374	brendandburns	2014-07-10 06:15:03	CONTRIBUTOR	"Makes sense to me, thanks for the PR!

--brendan
"
956	140	smarterclayton	2014-07-10 13:15:24	CONTRIBUTOR	"> We could override the container entrypoint with the pre-start hook command and then use ""runin"" to execute the real entrypoint, but the container's status, wait, restart, etc. would be broken. Creating a new container image that included the pre-start command, actual entrypoint, and post-termination hook might work, but we'd need to carefully propagate arguments, signals, exit status, etc.

We tried make this work, but it felt wrong on all sorts of levels and ultimately we felt there were two cases we wanted to handle:
- hooks that either need the container context (and as such executing outside the process namespace would be pointless), or if interrupted by container shutdown would not be internally inconsistent.  Pre-termination is a good example
- hooks that should be outside of a container, because they need to continue to run even if a container fails.  Deploy across multiple containers is a good example, or post-termination.

I don't think you need to subclass the entrypoint in the image - for the former, you could use ""docker exec in"" (add process to namespace) and for the latter, you can use restart-once containers (outside or inside a pod).

There's some crossover here between ""event hooks"" and ""intent/application hooks"" - reacting to changes to the infrastructure, vs reacting to user intent across multiple pods.  The ""intent hooks"" tend to be things that I think of as wanting the auth/orchestration parts of the downward-facing API (I want to wait for this label query to reach X instances, and then update another replication controller to go to Y), as well as being more like regular jobs in their own right.
"
957	345	smarterclayton	2014-07-10 15:08:37	CONTRIBUTOR	"For the purposes of this pull, I think the change here fits within the agreed direction and is merely an incremental step towards it (accordingly I marked ContainerManifest.ID as deprecated).  Disagreement?  If no could use a final rereview and merge.
"
958	390	thockin	2014-07-10 15:12:16	MEMBER	"First, the apiserver is supposed to ensure that HostPort conflicts do not
happen among pods.  We recognize that this is sort of awful, and want to
find a better answer :)

Second, I agree that the current Ports arrangement is not ideal.  I have an
idea that I want to flesh out to make it ""better"" (in my opinion).

Keep in mind that you really only need to specify a Ports entry if you want
a HostPort.  You only want a HostPort in GCE if you want an external IP to
be able to access it.  What we have done with networking is clever, but
maybe too much so.

My feeling is that this distinction is not clear, and instead everyone will
list all their ports when they do not need to.  It should be possible to
list your ports and NOT get a HostPort at all, which I think is what you
really want.

I am not against doing a random HostPort, but i want to think about it
carefully.

On Thu, Jul 10, 2014 at 12:16 AM, Yuki Yugui Sonoda <
notifications@github.com> wrote:

> IIUC, host port numbers in each pods are not important for replicated
> services in Kubernetes.
> From the perspective of orchestration, the important thing is port of
> service, but not port of pod.
> 
> Also manual assignment of host port can be troublesome because spawn of
> container just fails if the container manifest specified a port which is
> already taken by other containers or Kubernetes daemons.
> 
> So I propose the following enhancement.
> - Keep tracking taken ports in etcd
> - Allow omitting host port in container manifest
>   - If omitted, Kubelet automatically choose an available host port
>     for each exposed container port.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/390.
"
959	188	lexlapax	2014-07-10 15:17:36	NONE	"Some feedback here on the ip per pod/container model.
-IPV6 is further along in larger service providers and enterprise datacenters than we think.
-larger enterprises already use datacenter wide private ip spaces with datacenter (or row/rack/floor) wide NAT
I think rather than changing/removing the model of network addressable containers / pods, there should be other options added to allow operating in openstack or other similar clouds..
Agree with clayton, putting pods inside VMs is probably not the way to think about this.
"
960	391	thockin	2014-07-10 15:18:25	MEMBER	"This is a dangerous opening.  Can you detail why people should be able to
use this?  I'd rather pursue teaching Docker how to enable various
privileges in a more granular fashion.

If anything, we should be convincing people to run containers with LESS
privs (i.e. not root to start).

On Thu, Jul 10, 2014 at 12:28 AM, Yuki Yugui Sonoda <
notifications@github.com> wrote:

> Sometimes containers need to run with privileged mode.
> 
> https://docs.docker.com/reference/run/#runtime-privilege-and-lxc-configuration
> 
> Container manifest schema should support privileged flag so that users can
> deploy privileged container.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/391.
"
961	378	proppy	2014-07-10 16:00:56	CONTRIBUTOR	"> although I don't necessarily see the value

@brendanburns, I had a brief chat during Google I/O w/ @crawshaw from the Go team.

He gave a talk about managing dependencies for golang project.

Maybe he can expand on the rational of using godep, versus vendoring + rewriting imports versus hand crafted script.

What I remember from our discussion was that:
- using godep was useful if you wanted to distribute a stable binary of an executable (i.e: kubelet)
- using vendoring + rewriting imports was useful when `go get`ability is important: i.e, for libraries that other project might depends on.
"
962	393	brendandburns	2014-07-10 16:19:36	CONTRIBUTOR	"Thanks!
"
963	378	crawshaw	2014-07-10 16:39:51	NONE	"I haven't looked too closely at this pull request, but godep can certainly do what you need it to do. My personal preference is to write my own little program for dependency management, because I prefer code to configuration. But there are many reasonable options here, and godep is one of them.
"
964	389	brendandburns	2014-07-10 17:55:44	CONTRIBUTOR	"Comment addressed, please take another look.
"
965	389	smarterclayton	2014-07-10 18:09:05	CONTRIBUTOR	"LGTM
"
966	378	monnand	2014-07-10 18:12:07	CONTRIBUTOR	"Rebased.

Thank you, @crawshaw. I checked most available tools to do dependency management and it seems that godep requires least change to our current scripts.

The main reason I want to switch to godep is the ability to add/update dependencies automatically.
godep could tell us the dependencies by analyzing the source code, remove all unnecessary meta data (like .git). Our current approach requires us to manually add a new line to a file for each new dependency. It may not be a big deal but break the workflow.
"
967	395	brendandburns	2014-07-10 18:44:58	CONTRIBUTOR	"LGTM.  I'll wait for others, and then if there are no comments merge this afternoon.
"
968	362	brendandburns	2014-07-10 19:19:44	CONTRIBUTOR	"Cloud SDK isn't sure what could cause this, it looks like a hiccup in the console.  Closing this for now.  Please re-open if it re-occurs.

Thanks!
--brendan
"
969	396	brendandburns	2014-07-10 19:27:12	CONTRIBUTOR	"Fixes #298 
"
970	397	brendandburns	2014-07-10 19:45:34	CONTRIBUTOR	"Fixes #248 
"
971	345	thockin	2014-07-10 19:55:49	MEMBER	"LGTM - resolve conflicts or whatever is causing ""We can’t automatically merge this pull request""
"
972	378	monnand	2014-07-10 20:19:08	CONTRIBUTOR	"@brendandburns Passed CI. PTAL.
"
973	399	brendandburns	2014-07-10 21:25:57	CONTRIBUTOR	"Thanks for the PR!  Have you folks signed our CLA?  Details in [CONTRIB.md](https://github.com/GoogleCloudPlatform/kubernetes/blob/master/CONTRIB.md).

Thanks!
--brendan
"
974	399	jeffmendoza	2014-07-10 21:40:06	MEMBER	"Yes, I did about a week ago.
"
975	386	thockin	2014-07-10 23:28:01	MEMBER	"As much as I want to keep this simple, Clayton's argument of using a non
k8s native image in k8s by application of clever env vars is compelling.

But what scope?  Env vars only?  Env vars and command lines?
On Jul 9, 2014 5:08 PM, ""Clayton Coleman"" notifications@github.com wrote:

> That's true, although one strong goal in docker has been to try and offer
> via libchan/libswarm a ""Docker"" abstraction where on multiple platforms
> images can invoke consistent actions that affect the platform. That applies
> to more generic operations, whereas k8s specific actions will always
> require some downwards API. The abstraction is a long term play that
> requires active participation by a number of stakeholders and also
> implemented, demonstrated use cases, so it's not an argument for avoiding
> trying.
> 
> I don't think the downward API is bad, but some of the use cases (unique
> id for an instance, what ip do I have) might benefit from looser coupling
> or by trying to find the most lowest common denominator expression. In the
> end image authors can always ignore those settings, but you do want image
> users to have the tools to bend a recaltricant image to their will.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/386#issuecomment-48551080
> .
"
976	386	bgrant0607	2014-07-10 23:35:59	MEMBER	"I find it compelling, too. Internally, we have thousands of uses of the downward API, and I agree it introduces unnecessary coupling. OTOH, I'd like to think more about the configuration and parameterization story before introducing substitution.

As opposed to the k8s-specific mechanism I proposed, perhaps we could start by thinking about what a typical application would need to know about itself and how that information could be provided in the least entangled manner.
"
977	403	thockin	2014-07-10 23:58:55	MEMBER	"I find ""Interface"" to be a awkward given the meaning of the word in Go.  Would ""client.Client"" be better?  If someone were using this in a different project they might rename the package to ""k8s"" or something, yielding k8s.Client - which seems better than k8s.Interface
"
978	403	dsymonds	2014-07-11 00:00:26	CONTRIBUTOR	"Renaming both the package and interface sounds even better. `kubernetes.Client` reads well.
"
979	403	proppy	2014-07-11 00:01:12	CONTRIBUTOR	"There is already a `type Client` struct
https://github.com/nf/kubernetes/blob/cleanup/pkg/client/client.go#L70

So maybe `s/Client/KubeClient` for the concreate type, and `s/ClientInterface/Client` for the interface.
"
980	403	nf	2014-07-11 00:10:16	CONTRIBUTOR	"There is already a client.Client; that's the client implementation. That is fine.

`foo.Interface` is an established idiom. See `sort.Interface`, `hash.Interface` from the standard library.

If we want to rename the package we can do it in a separate PR. I am happy to share my rationale for each change, but I would like to avoid lengthy debate on each pull request as I'm likely to make hundreds of them. The faster I can move, the less likely I'll get bored.
"
981	403	thockin	2014-07-11 00:10:29	MEMBER	"Is there a standard pattern for this?  In Google C++ we would call it
Client and ClientImpl, but I don't know if all the code that uses the
existing Client uses it exclusively by the interface or not.

I.e. in C++ I would say

class Client {
  virtual int Foo() = 0;
}

class ClientImpl : public Client {
  int Foo() override;
}

Client New() {
  return new ClientImpl();
}

But I see that this code returns a Client, not ClientInterface.  If it can
return ClientInterface, it maps better to Foo and FooImpl.

On Thu, Jul 10, 2014 at 5:01 PM, Johan Euphrosine notifications@github.com
wrote:

> There is already a type Client struct
> https://github.com/nf/kubernetes/blob/cleanup/pkg/client/client.go#L70
> 
> So maybe s/Client/KubeClient for the concreate type, and
> s/ClientInterface/Client for the interface.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/pull/403#issuecomment-48680041
> .
"
982	403	nf	2014-07-11 00:11:33	CONTRIBUTOR	"This PR brings the code in line with the standard Go idiom. `client.Client` for the concrete type, `client.Interface` for the interface.
"
983	403	thockin	2014-07-11 00:11:51	MEMBER	"if pkg.Interface is a common pattern, so be it.
"
984	405	dsymonds	2014-07-11 00:39:43	CONTRIBUTOR	"Definitely not.

Please read the README at https://github.com/golang/lint carefully to understand the golint philosophy. In short, golint has false positives, where gofmt does not. The same applies to govet.

There are a number of editor plugins that individuals can use as part of their own workflow, but golint and govet both require human judgment and are not suitable for automation.
"
985	402	monnand	2014-07-11 01:10:24	CONTRIBUTOR	"@nf Actually, we discussed about this with @proppy, @brendanburns and @jbeda before this project get open source. I strongly support to make it go get able. But @jbeda a and @brendandburns have some concerns. If I remember it correctly, here are some requirement they have:
- vendoring all dependencies in a subdirectory.
- Import paths should not be changed. (this roles out tools like goven)

(I cannot find the discussion now, so there may be other requirements.)

Based on the discussion, I proposed a PR #378 to use [godep](http://github.com/tools/godep) so that we could have least changes to existing scripts.

I would like to hear your solutions.
"
986	404	nf	2014-07-11 01:27:19	CONTRIBUTOR	"LGTM
"
987	391	bgrant0607	2014-07-11 01:32:10	MEMBER	"Docker is adding capability whitelisting/blacklisting as we speak.

On Thu, Jul 10, 2014 at 8:18 AM, Tim Hockin notifications@github.com
wrote:

> This is a dangerous opening. Can you detail why people should be able to
> use this? I'd rather pursue teaching Docker how to enable various
> privileges in a more granular fashion.
> 
> If anything, we should be convincing people to run containers with LESS
> privs (i.e. not root to start).
> 
> On Thu, Jul 10, 2014 at 12:28 AM, Yuki Yugui Sonoda <
> notifications@github.com> wrote:
> 
> > Sometimes containers need to run with privileged mode.
> > 
> > https://docs.docker.com/reference/run/#runtime-privilege-and-lxc-configuration
> > 
> > Container manifest schema should support privileged flag so that users
> > can
> > deploy privileged container.
> > 
> > ## 
> > 
> > Reply to this email directly or view it on GitHub
> > https://github.com/GoogleCloudPlatform/kubernetes/issues/391.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/391#issuecomment-48618877
> .
"
988	390	bgrant0607	2014-07-11 01:46:53	MEMBER	"I don't understand the motivation for automatically assigned host ports. The motivation for requesting one is so that it can be opened in firewall rules and connected to via external clients, through frontend load balancing, etc. This is not possible with automatically assigned ports.
"
989	398	bgrant0607	2014-07-11 02:03:46	MEMBER	"For info on Docker security generally:
https://docs.docker.com/articles/security/

There are efforts underway to make it easier to use Linux security mechanisms with Docker, such as restricted capabilities, AppArmor, SELinux, non-root users, etc. 

Are you asking how to access these features through Kubernetes?

That said, our position is that Docker doesn't yet provide a sufficient security boundary:
https://developers.google.com/compute/docs/security-bulletins?_ga=1.91563352.392654364.1401901573
"
990	392	bgrant0607	2014-07-11 02:14:58	MEMBER	"You could use labels to version your containers. If you have a new version to deploy, you can create a new replicationController with a new version label, but with labels that match your service's label selector, and the new containers will start to receive traffic automatically. Once the new version has been confirmed to operate correctly, you could grow the number of replicas to the full service size and shrink the previous replicationController's count down to zero, then delete it. I agree that a bit of tooling could make this easier, but it's possible to do now. We may have demoed something similar at Dockercon.
"
991	391	yugui	2014-07-11 03:29:13	CONTRIBUTOR	"I close this issue because I discussed with thockin, and I agreed that my original motivation of adding privileged container can be covered by another approach.  On the other hand, container should be run with less priv.
So I agree that allowing privileged container is a wrong direction.
"
992	391	thockin	2014-07-11 03:32:00	MEMBER	"If we can't find a better way to do this, we can revisit this idea.

On Thu, Jul 10, 2014 at 8:29 PM, Yuki Yugui Sonoda <notifications@github.com

> wrote:
> 
> I close this issue because I discussed with thockin, and I agreed that my
> original motivation of adding privileged container can be covered by
> another approach. On the other hand, container should be run with less priv.
> So I agree that allowing privileged container is a wrong direction.
> 
> ## 
> 
> Reply to this email directly or view it on GitHub
> https://github.com/GoogleCloudPlatform/kubernetes/issues/391#issuecomment-48690921
> .
"
993	402	brendandburns	2014-07-11 03:43:52	CONTRIBUTOR	"Yeah, I think that the original discussion of this issue got eaten in the transition from our pre-release staging git repo to the released public repo.  I'll try to re-cap:

For reasons of clarity about OSS licensing (and general organization) we prefer to keep dependent libraries in a separate directory (third_party/...).  That way its very clear which LICENSE files apply to which directories, and it is also very clear what parts of the codebase our our code, and which parts are library code.

Additionally, modifying all of the OSS libraries we use causes a maintenance headache as we need to re-adjust paths every single time that we move versions of libraries to a new version.  Finally, modifying the library code to have new import paths can have non-trivial OSS license implications that we'd rather not mess around with.

I checked camlistore and it appears to not work with a vanilla go get:

``` sh
bburns$ go get github.com/bradfitz/camlistore
package github.com/bradfitz/camlistore
    imports github.com/bradfitz/camlistore
    imports github.com/bradfitz/camlistore: no buildable Go source files in /Users/bburns/git/tmp/src/github.com/bradfitz/camlistore
```

Our basic methodology for building is actually quite similar to camlistore's as documented [here](https://github.com/bradfitz/camlistore/blob/master/BUILDING)

``` sh
git clone https://github.com/GoogleCloudPlatform/kubernetes.git
./hack/build-go.sh
```

If its preferable to turn build-go.sh into a go program similar to Brad's make.go, you're welcome to send the PR for consideration. 

If there is a way to achieve go get-ability and maintain the goals that we set out above, obviously it would be great to hear about it.

Thanks!
--brendan
"
994	401	brendandburns	2014-07-11 03:55:10	CONTRIBUTOR	"Duplicate of #402 
"
995	398	brendandburns	2014-07-11 03:57:36	CONTRIBUTOR	"Closing this since I think @bgrant0607's response addresses the issue, and anything remaining is likely a broader issue for Docker/libcontainer/... rather than k8s.
"
996	392	brendandburns	2014-07-11 03:58:57	CONTRIBUTOR	"We also have the facility to do rolling updates, see the example here:

https://github.com/GoogleCloudPlatform/kubernetes/tree/master/examples/update-demo

If you combine a rolling update of a replicated service with a health checking load balancer, then you have a pretty good solution to this problem.
"
997	402	nf	2014-07-11 04:06:12	CONTRIBUTOR	"@brendanburns Thanks for the reply.

Fortunately there's precedent for all of this. Camlistore is a Google project and we already manage all this stuff easily and complying with OSS licenses.

The Camlistore approach to external dependencies:
- put them under `third_party` (with a suitable [README](https://camlistore.googlesource.com/camlistore/+/master/third_party/README)),
- rewrite import paths with a [trivial script](https://camlistore.googlesource.com/camlistore/+/master/third_party/rewrite-imports.sh),
- update them by copying over the latest version of the repo and running the above script.

In my extensive experience with OSS releasing at Google there are no non-trivial licensing issues surrounding these kinds of mechanical edits. If you believe there are, I am happy to follow this up with the Open Source team at Google on your behalf. 

Camlistore _is_ go-gettable. The error you see is because you're not specifying a go package path, just the repo base path. Try something like this:

```
$ go get camlistore.org/server/camlistored
```

(Note that `github.com/bradfitz/camlistore` is the old deprecated repo path.)

I don't object at all to special build scripts that updates the version hash and so on. This issue doesn't ask that ""go get"" be the default build mechanism, merely that the codebase should work with ""go get"" by default.

Note that installation with ""go get"" isn't the only benefit of this cleanup; having a well-organised repository permits static analysis tools like godoc and goimports to analyse your codebase without setting a special GOPATH.

I hope this puts your concerns at ease. What do you think?
"
998	401	nf	2014-07-11 04:06:36	CONTRIBUTOR	"It's not a dupe. The build error in pkg/kubelet is real, is it not?
"
999	401	brendandburns	2014-07-11 04:15:56	CONTRIBUTOR	"Pretty sure not, Travis is clean @ head:

https://travis-ci.org/GoogleCloudPlatform/kubernetes

And I just tried clean branch @ upstream/master and it builds.
"
